{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import FashionMNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/quant_he_code/weights/quant_fc_2bits_fmnist_plain_2bits_weights.pth')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/fmnist'\n",
    "weight_dir = project_path/'quant_he_code/weights/'\n",
    "\n",
    "input_bit_width = 2\n",
    "weight_bit_width = 2\n",
    "weight_file = f\"quant_fc_{input_bit_width}bits_fmnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "\n",
    "weight_file_path = weight_dir / weight_file\n",
    "assert weight_file_path.exists()\n",
    "weight_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        # transform into [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        # transform into [0, 1, 2, 3]\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    test_dataset = FashionMNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return test_dataset\n",
    "\n",
    "test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im.shape = torch.Size([784])\n",
      "Processed FashionMNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9385855f70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = test_dataset[0][0]\n",
    "print(f\"{im.shape = }\")\n",
    "print(f\"Processed FashionMNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)  # outputs has dim [batch_size, 10]\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FMNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 2 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):      \n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = FMNISTLinearQuantModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(weight_file_path, \n",
    "    map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique input values = tensor([0., 1., 2., 3.])\n",
      "unique labels values = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "nb_tests = 10000\n",
    "test_loader = DataLoader(test_dataset, nb_tests, pin_memory=True)\n",
    "for x, y in test_loader:\n",
    "    print(f\"unique input values = {x.unique()}\")\n",
    "    print(f\"unique labels values = {y.unique()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with trained 2-bit weights = 0.8108999729156494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80348/2498532057.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)  # outputs has dim [batch_size, 10]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "acc = evaluate(model, test_loader)['val_acc']\n",
    "print(f\"test accuracy with trained {weight_bit_width}-bit weights = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference on integer data and Breavitas's integer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Integer Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.unique() = tensor([-1,  0,  1], dtype=torch.int8)\n",
      "fc2.unique() = tensor([-1,  0,  1], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "fc1 = model.fc1.int_weight()\n",
    "fc2 = model.fc2.int_weight()\n",
    "print(f\"{fc1.unique() = }\")\n",
    "print(f\"{fc2.unique() = }\")\n",
    "fc1 = torch.transpose(fc1, 0, 1)\n",
    "fc2 = torch.transpose(fc2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "--x.dtype = torch.float32, \n",
      "--x.shape = torch.Size([10000, 784]), \n",
      "--x.unique() = tensor([0., 1., 2., 3.])\n",
      "labels: \n",
      "--y.dtype = torch.int64, \n",
      "--y.shape = torch.Size([10000]), \n",
      "--y.unique() = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 10000\n",
    "test_loader = DataLoader(test_dataset, test_batch_size, pin_memory=True)\n",
    "x, y = next(iter(test_loader))\n",
    "print(f\"input: \\n--{x.dtype = }, \\n--{x.shape = }, \\n--{x.unique() = }\")\n",
    "print(f\"labels: \\n--{y.dtype = }, \\n--{y.shape = }, \\n--{y.unique() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0].reshape(28,28), cmap='gray')\n",
    "print(f\"class = {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 81.02% accuracy\n"
     ]
    }
   ],
   "source": [
    "total_acc = 0\n",
    "max_value = -99999\n",
    "min_value = 99999\n",
    " \n",
    "torch_dtype = torch.int16\n",
    "fc1 = fc1.to(dtype=torch_dtype)\n",
    "fc2 = fc2.to(dtype=torch_dtype)\n",
    "\n",
    "for im, label in test_loader:\n",
    "    im = im.to(dtype=torch_dtype)\n",
    "    out = torch.matmul(im, fc1)\n",
    "    out = out * out\n",
    "    out = torch.matmul(out, fc2)\n",
    "    pred = torch.argmax(out, axis=1)\n",
    "    acc = (pred == label)\n",
    "    total_acc += acc.sum()\n",
    "    if torch.max(out) > max_value:\n",
    "        max_value = torch.max(out)\n",
    "    if torch.min(out) < min_value:\n",
    "        min_value = torch.min(out)\n",
    "\n",
    "print(f\"got {total_acc / 10000 * 100:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting final bit-width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_value = tensor(32694, dtype=torch.int16)\n",
      "min_value = tensor(-31342, dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{max_value = }\")\n",
    "print(f\"{min_value = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "final_bit_width = 16\n",
    "print(2**(final_bit_width-1) > max_value.item())\n",
    "print(-2**(final_bit_width-1) < min_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Integer Inference based on Wouter's Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual quantize function from Wouter's code. Needs more careful look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantise(element, s, b):\n",
    "    upper = 2 ** (b - 1) - 1\n",
    "    lower = -2 ** (b - 1)\n",
    "\n",
    "    value = int(round(element / s))\n",
    "\n",
    "    if value > upper:\n",
    "        return upper\n",
    "    elif value < lower:\n",
    "        return lower\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "np_quantise = np.vectorize(quantise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_fc1 = np.array(model.fc1.quant_weight().int().tolist())\n",
    "fc1_scale = model.fc1.quant_weight().scale.data.tolist()\n",
    "int_fc2 = np.array(model.fc2.quant_weight().int().tolist())\n",
    "fc2_scale = model.fc2.quant_weight().scale.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_input = 1 / 10\n",
    "fc1_ind_scale = np.max(np.abs(int_fc1)) / (2 ** (weight_bit_width - 1) - 1)\n",
    "fc1_scale = scale_factor_input * fc1_ind_scale\n",
    "# Activation\n",
    "fc1_scale_act = fc1_scale**2\n",
    "\n",
    "fc2_ind_scale = np.max(np.abs(int_fc2)) / (2 ** (weight_bit_width - 1) - 1)\n",
    "fc2_scale = fc1_scale_act * fc2_ind_scale\n",
    "\n",
    "fc1_q = np_quantise(int_fc1, fc1_ind_scale, weight_bit_width)\n",
    "fc2_q = np_quantise(int_fc2, fc2_ind_scale, weight_bit_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fc1_q == model.fc1.int_weight().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fc2_q == model.fc2.int_weight().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1: \n",
      "--type(fc1_q) = <class 'numpy.ndarray'>, \n",
      "--fc1_q.dtype = dtype('int64'), \n",
      "--fc1_q.shape = (128, 784), \n",
      "--np.unique(fc1_q) = array([-1,  0,  1])\n",
      "fc2: \n",
      "--type(fc2_q) = <class 'numpy.ndarray'>, \n",
      "--fc2_q.dtype = dtype('int64'), \n",
      "--fc2_q.shape = (10, 128), \n",
      "--np.unique(fc2_q) = array([-1,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"fc1: \\n--{type(fc1_q) = }, \\n--{fc1_q.dtype = }, \\n--{fc1_q.shape = }, \\n--{np.unique(fc1_q) = }\")\n",
    "print(f\"fc2: \\n--{type(fc2_q) = }, \\n--{fc2_q.dtype = }, \\n--{fc2_q.shape = }, \\n--{np.unique(fc2_q) = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Integer Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do manual inference on 10000 images\n",
      "got 81.08999999999999% accuracy\n"
     ]
    }
   ],
   "source": [
    "nb_tests = 10000\n",
    "test_loader = DataLoader(test_dataset, nb_tests, pin_memory=True)\n",
    "print(f\"do manual inference on {nb_tests} images\")\n",
    "\n",
    "for im, label in test_loader:\n",
    "    im = im.numpy()\n",
    "    out = np.matmul(im, np.transpose(fc1_q))\n",
    "    out = out * out\n",
    "    out = np.matmul(out, np.transpose(fc2_q))\n",
    "    pred = np.argmax(out, axis=1)\n",
    "    acc = (pred == label.numpy())\n",
    "    break  # only runs on 1 batch of data\n",
    "\n",
    "print(f\"got {acc.sum() / nb_tests * 100}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating final bit-width\n",
    "b = weight_bit_width\n",
    "\n",
    "- First fc layer: `bit_width = 2 + b`\n",
    "- First square activation: `bit_width = 2 * (2 + b) = 4 + 2b`\n",
    "- Second fc layer: `bit_width = 4 + 3b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "final_bit_width = 16\n",
    "print(2**final_bit_width)\n",
    "print(2**final_bit_width > np.max(out))\n",
    "print(-2**final_bit_width < np.min(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pockethhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
