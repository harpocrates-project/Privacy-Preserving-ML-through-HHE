{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/mnist')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bit_width = 4\n",
    "data_process_option = 2  # 2 = quantize into [0,1,2,3]\n",
    "save_weight_path = project_path/f\"quant_he_code/weights/quant_hcnn_2bits_mnist_plain_{weight_bit_width}bits_weights.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 options: \n",
    "1. Quantize into [0, 0.25, 0.5, 0.75, 1]\n",
    "2. Quantize into [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 1:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            lambda x: (x * 3).int().float()\n",
    "        ])\n",
    "    train_dataset = MNIST(root=mnist_path, download=False, transform=transform)\n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = mnist_processing(option=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "test_dataset shape: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset shape: {train_dataset.data.shape}\")\n",
    "print(f\"test_dataset shape: {test_dataset.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1ef3be8ee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_dataset[0][0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the CNN model (2 conv layers + 1 linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConvQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 conv layers + 2 square activations + 1 linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels=1, out_channels=5, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width, \n",
    "                                     return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(in_channels=5, out_channels=50, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width)\n",
    "        self.fc1 = qnn.QuantLinear(in_features=800, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = out * out  # second square\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTConvQuantModel(\n",
       "  (conv1): QuantConv2d(\n",
       "    1, 5, kernel_size=(5, 5), stride=(2, 2), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (conv2): QuantConv2d(\n",
       "    5, 50, kernel_size=(5, 5), stride=(2, 2), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=800, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(MNISTConvQuantModel(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_path, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.98\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(f\"Saved into {file_path.relative_to(project_path)}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356275/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.1411, val_acc: 0.9626\n",
      "Epoch [2], val_loss: 0.0989, val_acc: 0.9713\n",
      "Epoch [3], val_loss: 0.0860, val_acc: 0.9731\n",
      "2\t{'val_loss': 0.07891836017370224, 'val_acc': 0.9795979261398315}\n",
      "Epoch [4], val_loss: 0.0821, val_acc: 0.9761\n",
      "3\t{'val_loss': 0.06670885533094406, 'val_acc': 0.9809912443161011}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [5], val_loss: 0.0867, val_acc: 0.9763\n",
      "4\t{'val_loss': 0.08241691440343857, 'val_acc': 0.9801950454711914}\n",
      "Epoch [6], val_loss: 0.0896, val_acc: 0.9765\n",
      "5\t{'val_loss': 0.07717237621545792, 'val_acc': 0.9809912443161011}\n",
      "Epoch [7], val_loss: 0.0876, val_acc: 0.9777\n",
      "6\t{'val_loss': 0.08432413637638092, 'val_acc': 0.9790008068084717}\n",
      "Epoch [8], val_loss: 0.0897, val_acc: 0.9784\n",
      "7\t{'val_loss': 0.07536882907152176, 'val_acc': 0.9801950454711914}\n",
      "Epoch [9], val_loss: 0.0857, val_acc: 0.9800\n",
      "8\t{'val_loss': 0.066017746925354, 'val_acc': 0.9823845624923706}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [10], val_loss: 0.0803, val_acc: 0.9800\n",
      "9\t{'val_loss': 0.0706542581319809, 'val_acc': 0.9824841022491455}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [11], val_loss: 0.0860, val_acc: 0.9804\n",
      "10\t{'val_loss': 0.06951230019330978, 'val_acc': 0.9833797812461853}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [12], val_loss: 0.0755, val_acc: 0.9806\n",
      "11\t{'val_loss': 0.06637358665466309, 'val_acc': 0.984375}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [13], val_loss: 0.0900, val_acc: 0.9808\n",
      "12\t{'val_loss': 0.07347141951322556, 'val_acc': 0.984574019908905}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [14], val_loss: 0.0879, val_acc: 0.9808\n",
      "13\t{'val_loss': 0.08754027634859085, 'val_acc': 0.9827826619148254}\n",
      "Epoch [15], val_loss: 0.0922, val_acc: 0.9810\n",
      "14\t{'val_loss': 0.08337393403053284, 'val_acc': 0.9842754602432251}\n",
      "Epoch [16], val_loss: 0.0858, val_acc: 0.9812\n",
      "15\t{'val_loss': 0.06841376423835754, 'val_acc': 0.9835788011550903}\n",
      "Epoch [17], val_loss: 0.1082, val_acc: 0.9810\n",
      "16\t{'val_loss': 0.0882643610239029, 'val_acc': 0.9842754602432251}\n",
      "Epoch [18], val_loss: 0.0919, val_acc: 0.9818\n",
      "17\t{'val_loss': 0.0752510130405426, 'val_acc': 0.9842754602432251}\n",
      "Epoch [19], val_loss: 0.0813, val_acc: 0.9822\n",
      "18\t{'val_loss': 0.08296307921409607, 'val_acc': 0.9851711988449097}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [20], val_loss: 0.0905, val_acc: 0.9810\n",
      "19\t{'val_loss': 0.07940701395273209, 'val_acc': 0.9850716590881348}\n",
      "Epoch [21], val_loss: 0.1142, val_acc: 0.9782\n",
      "20\t{'val_loss': 0.08390000462532043, 'val_acc': 0.9831807613372803}\n",
      "Epoch [22], val_loss: 0.1232, val_acc: 0.9834\n",
      "21\t{'val_loss': 0.08067671954631805, 'val_acc': 0.9847730994224548}\n",
      "Epoch [23], val_loss: 0.0890, val_acc: 0.9818\n",
      "22\t{'val_loss': 0.06516159325838089, 'val_acc': 0.9839769005775452}\n",
      "Epoch [24], val_loss: 0.1238, val_acc: 0.9763\n",
      "23\t{'val_loss': 0.12930159270763397, 'val_acc': 0.9764131903648376}\n",
      "Epoch [25], val_loss: 0.1200, val_acc: 0.9798\n",
      "24\t{'val_loss': 0.08957502990961075, 'val_acc': 0.9821854829788208}\n",
      "Epoch [26], val_loss: 0.0882, val_acc: 0.9826\n",
      "25\t{'val_loss': 0.09684137254953384, 'val_acc': 0.9833797812461853}\n",
      "Epoch [27], val_loss: 0.1217, val_acc: 0.9810\n",
      "26\t{'val_loss': 0.10706063359975815, 'val_acc': 0.9801950454711914}\n",
      "Epoch [28], val_loss: 0.0875, val_acc: 0.9832\n",
      "27\t{'val_loss': 0.08069637417793274, 'val_acc': 0.9836783409118652}\n",
      "Epoch [29], val_loss: 0.1048, val_acc: 0.9816\n",
      "28\t{'val_loss': 0.09195742011070251, 'val_acc': 0.9835788011550903}\n",
      "Epoch [30], val_loss: 0.1000, val_acc: 0.9830\n",
      "29\t{'val_loss': 0.08869695663452148, 'val_acc': 0.9842754602432251}\n",
      "Epoch [31], val_loss: 0.1222, val_acc: 0.9802\n",
      "30\t{'val_loss': 0.0942312702536583, 'val_acc': 0.9840764403343201}\n",
      "Epoch [32], val_loss: 0.0884, val_acc: 0.9792\n",
      "31\t{'val_loss': 0.07626939564943314, 'val_acc': 0.9837778806686401}\n",
      "Epoch [33], val_loss: 0.1617, val_acc: 0.9808\n",
      "32\t{'val_loss': 0.1281629502773285, 'val_acc': 0.9831807613372803}\n",
      "Epoch [34], val_loss: 0.0922, val_acc: 0.9830\n",
      "33\t{'val_loss': 0.09425876289606094, 'val_acc': 0.9822850227355957}\n",
      "Epoch [35], val_loss: 0.1166, val_acc: 0.9814\n",
      "34\t{'val_loss': 0.10921376198530197, 'val_acc': 0.9829816818237305}\n",
      "Epoch [36], val_loss: 0.1129, val_acc: 0.9792\n",
      "35\t{'val_loss': 0.09274611622095108, 'val_acc': 0.980792224407196}\n",
      "Epoch [37], val_loss: 0.1350, val_acc: 0.9739\n",
      "36\t{'val_loss': 0.13227440416812897, 'val_acc': 0.9756170511245728}\n",
      "Epoch [38], val_loss: 0.1052, val_acc: 0.9836\n",
      "37\t{'val_loss': 0.10090819001197815, 'val_acc': 0.9833797812461853}\n",
      "Epoch [39], val_loss: 0.1356, val_acc: 0.9778\n",
      "38\t{'val_loss': 0.14914725720882416, 'val_acc': 0.978204607963562}\n",
      "Epoch [40], val_loss: 0.0905, val_acc: 0.9834\n",
      "39\t{'val_loss': 0.08995131403207779, 'val_acc': 0.9851711988449097}\n",
      "Epoch [41], val_loss: 0.1169, val_acc: 0.9832\n",
      "40\t{'val_loss': 0.1050974577665329, 'val_acc': 0.9831807613372803}\n",
      "Epoch [42], val_loss: 0.1219, val_acc: 0.9806\n",
      "41\t{'val_loss': 0.10410287976264954, 'val_acc': 0.9852706789970398}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [43], val_loss: 0.1304, val_acc: 0.9824\n",
      "42\t{'val_loss': 0.11417786031961441, 'val_acc': 0.9821854829788208}\n",
      "Epoch [44], val_loss: 0.1450, val_acc: 0.9814\n",
      "43\t{'val_loss': 0.14286498725414276, 'val_acc': 0.9806926846504211}\n",
      "Epoch [45], val_loss: 0.1160, val_acc: 0.9800\n",
      "44\t{'val_loss': 0.10195011645555496, 'val_acc': 0.9836783409118652}\n",
      "Epoch [46], val_loss: 0.1412, val_acc: 0.9816\n",
      "45\t{'val_loss': 0.1309039294719696, 'val_acc': 0.981090784072876}\n",
      "Epoch [47], val_loss: 0.1329, val_acc: 0.9810\n",
      "46\t{'val_loss': 0.11780480295419693, 'val_acc': 0.9839769005775452}\n",
      "Epoch [48], val_loss: 0.1227, val_acc: 0.9840\n",
      "47\t{'val_loss': 0.11097598075866699, 'val_acc': 0.9842754602432251}\n",
      "Epoch [49], val_loss: 0.1418, val_acc: 0.9836\n",
      "48\t{'val_loss': 0.14458981156349182, 'val_acc': 0.9818869233131409}\n",
      "Epoch [50], val_loss: 0.1356, val_acc: 0.9771\n",
      "49\t{'val_loss': 0.1108514592051506, 'val_acc': 0.9819864630699158}\n",
      "Epoch [51], val_loss: 0.1140, val_acc: 0.9844\n",
      "50\t{'val_loss': 0.09047439694404602, 'val_acc': 0.9858678579330444}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [52], val_loss: 0.1212, val_acc: 0.9800\n",
      "51\t{'val_loss': 0.10425619781017303, 'val_acc': 0.9829816818237305}\n",
      "Epoch [53], val_loss: 0.1147, val_acc: 0.9818\n",
      "52\t{'val_loss': 0.09321457147598267, 'val_acc': 0.9866639971733093}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [54], val_loss: 0.1062, val_acc: 0.9824\n",
      "53\t{'val_loss': 0.0912686437368393, 'val_acc': 0.9847730994224548}\n",
      "Epoch [55], val_loss: 0.1037, val_acc: 0.9828\n",
      "54\t{'val_loss': 0.0943998247385025, 'val_acc': 0.984574019908905}\n",
      "Epoch [56], val_loss: 0.1288, val_acc: 0.9802\n",
      "55\t{'val_loss': 0.12004006654024124, 'val_acc': 0.9800955653190613}\n",
      "Epoch [57], val_loss: 0.1623, val_acc: 0.9824\n",
      "56\t{'val_loss': 0.13743339478969574, 'val_acc': 0.9832802414894104}\n",
      "Epoch [58], val_loss: 0.1426, val_acc: 0.9830\n",
      "57\t{'val_loss': 0.12527751922607422, 'val_acc': 0.9834793210029602}\n",
      "Epoch [59], val_loss: 0.1423, val_acc: 0.9836\n",
      "58\t{'val_loss': 0.13451948761940002, 'val_acc': 0.9860668778419495}\n",
      "Epoch [60], val_loss: 0.0992, val_acc: 0.9836\n",
      "59\t{'val_loss': 0.0944843664765358, 'val_acc': 0.9864649772644043}\n",
      "Epoch [61], val_loss: 0.1044, val_acc: 0.9830\n",
      "60\t{'val_loss': 0.09653740376234055, 'val_acc': 0.9847730994224548}\n",
      "Epoch [62], val_loss: 0.1619, val_acc: 0.9790\n",
      "61\t{'val_loss': 0.15543165802955627, 'val_acc': 0.9805931448936462}\n",
      "Epoch [63], val_loss: 0.1255, val_acc: 0.9794\n",
      "62\t{'val_loss': 0.1282491683959961, 'val_acc': 0.9806926846504211}\n",
      "Epoch [64], val_loss: 0.1045, val_acc: 0.9800\n",
      "63\t{'val_loss': 0.0951818972826004, 'val_acc': 0.984375}\n",
      "Epoch [65], val_loss: 0.1369, val_acc: 0.9820\n",
      "64\t{'val_loss': 0.10767693817615509, 'val_acc': 0.9844745397567749}\n",
      "Epoch [66], val_loss: 0.1346, val_acc: 0.9826\n",
      "65\t{'val_loss': 0.1148744598031044, 'val_acc': 0.9834793210029602}\n",
      "Epoch [67], val_loss: 0.1369, val_acc: 0.9850\n",
      "66\t{'val_loss': 0.1235145851969719, 'val_acc': 0.9844745397567749}\n",
      "Epoch [68], val_loss: 0.1203, val_acc: 0.9832\n",
      "67\t{'val_loss': 0.09645921736955643, 'val_acc': 0.9824841022491455}\n",
      "Epoch [69], val_loss: 0.1285, val_acc: 0.9802\n",
      "68\t{'val_loss': 0.12326256185770035, 'val_acc': 0.9805931448936462}\n",
      "Epoch [70], val_loss: 0.1500, val_acc: 0.9824\n",
      "69\t{'val_loss': 0.12128126621246338, 'val_acc': 0.9833797812461853}\n",
      "Epoch [71], val_loss: 0.1465, val_acc: 0.9802\n",
      "70\t{'val_loss': 0.14353348314762115, 'val_acc': 0.9809912443161011}\n",
      "Epoch [72], val_loss: 0.1423, val_acc: 0.9802\n",
      "71\t{'val_loss': 0.12574493885040283, 'val_acc': 0.9836783409118652}\n",
      "Epoch [73], val_loss: 0.1219, val_acc: 0.9794\n",
      "72\t{'val_loss': 0.1075996682047844, 'val_acc': 0.9833797812461853}\n",
      "Epoch [74], val_loss: 0.1346, val_acc: 0.9814\n",
      "73\t{'val_loss': 0.14351454377174377, 'val_acc': 0.9819864630699158}\n",
      "Epoch [75], val_loss: 0.1319, val_acc: 0.9755\n",
      "74\t{'val_loss': 0.1283736675977707, 'val_acc': 0.9765127301216125}\n",
      "Epoch [76], val_loss: 0.1275, val_acc: 0.9786\n",
      "75\t{'val_loss': 0.1263159066438675, 'val_acc': 0.9835788011550903}\n",
      "Epoch [77], val_loss: 0.1279, val_acc: 0.9822\n",
      "76\t{'val_loss': 0.11522034555673599, 'val_acc': 0.9853702187538147}\n",
      "Epoch [78], val_loss: 0.1262, val_acc: 0.9816\n",
      "77\t{'val_loss': 0.09836684912443161, 'val_acc': 0.9838773608207703}\n",
      "Epoch [79], val_loss: 0.1467, val_acc: 0.9824\n",
      "78\t{'val_loss': 0.1233101636171341, 'val_acc': 0.9825835824012756}\n",
      "Epoch [80], val_loss: 0.1186, val_acc: 0.9828\n",
      "79\t{'val_loss': 0.10565943270921707, 'val_acc': 0.984574019908905}\n",
      "Epoch [81], val_loss: 0.1322, val_acc: 0.9812\n",
      "80\t{'val_loss': 0.11360135674476624, 'val_acc': 0.9815883636474609}\n",
      "Epoch [82], val_loss: 0.1652, val_acc: 0.9826\n",
      "81\t{'val_loss': 0.1366080939769745, 'val_acc': 0.984574019908905}\n",
      "Epoch [83], val_loss: 0.1391, val_acc: 0.9832\n",
      "82\t{'val_loss': 0.12476635724306107, 'val_acc': 0.9855692386627197}\n",
      "Epoch [84], val_loss: 0.1475, val_acc: 0.9814\n",
      "83\t{'val_loss': 0.11600209772586823, 'val_acc': 0.9837778806686401}\n",
      "Epoch [85], val_loss: 0.1411, val_acc: 0.9832\n",
      "84\t{'val_loss': 0.13532648980617523, 'val_acc': 0.9851711988449097}\n",
      "Epoch [86], val_loss: 0.1433, val_acc: 0.9812\n",
      "85\t{'val_loss': 0.15161772072315216, 'val_acc': 0.9828821420669556}\n",
      "Epoch [87], val_loss: 0.1174, val_acc: 0.9806\n",
      "86\t{'val_loss': 0.09755386412143707, 'val_acc': 0.9853702187538147}\n",
      "Epoch [88], val_loss: 0.1404, val_acc: 0.9814\n",
      "87\t{'val_loss': 0.13829568028450012, 'val_acc': 0.9818869233131409}\n",
      "Epoch [89], val_loss: 0.1394, val_acc: 0.9840\n",
      "88\t{'val_loss': 0.14061753451824188, 'val_acc': 0.984375}\n",
      "Epoch [90], val_loss: 0.1167, val_acc: 0.9838\n",
      "89\t{'val_loss': 0.12230919301509857, 'val_acc': 0.9836783409118652}\n",
      "Epoch [91], val_loss: 0.1508, val_acc: 0.9800\n",
      "90\t{'val_loss': 0.16879303753376007, 'val_acc': 0.9818869233131409}\n",
      "Epoch [92], val_loss: 0.1120, val_acc: 0.9832\n",
      "91\t{'val_loss': 0.11212751269340515, 'val_acc': 0.9853702187538147}\n",
      "Epoch [93], val_loss: 0.1120, val_acc: 0.9828\n",
      "92\t{'val_loss': 0.11633144319057465, 'val_acc': 0.9829816818237305}\n",
      "Epoch [94], val_loss: 0.1105, val_acc: 0.9830\n",
      "93\t{'val_loss': 0.10974729806184769, 'val_acc': 0.984574019908905}\n",
      "Epoch [95], val_loss: 0.1495, val_acc: 0.9780\n",
      "94\t{'val_loss': 0.13137754797935486, 'val_acc': 0.9805931448936462}\n",
      "Epoch [96], val_loss: 0.1453, val_acc: 0.9824\n",
      "95\t{'val_loss': 0.13606178760528564, 'val_acc': 0.9839769005775452}\n",
      "Epoch [97], val_loss: 0.1428, val_acc: 0.9846\n",
      "96\t{'val_loss': 0.12887366116046906, 'val_acc': 0.9855692386627197}\n",
      "Epoch [98], val_loss: 0.1514, val_acc: 0.9808\n",
      "97\t{'val_loss': 0.149994894862175, 'val_acc': 0.9823845624923706}\n",
      "Epoch [99], val_loss: 0.1695, val_acc: 0.9804\n",
      "98\t{'val_loss': 0.14017163217067719, 'val_acc': 0.984175980091095}\n",
      "Epoch [100], val_loss: 0.1546, val_acc: 0.9844\n",
      "99\t{'val_loss': 0.13253405690193176, 'val_acc': 0.9858678579330444}\n",
      "Epoch [101], val_loss: 0.1683, val_acc: 0.9788\n",
      "100\t{'val_loss': 0.14536243677139282, 'val_acc': 0.9820860028266907}\n",
      "Epoch [102], val_loss: 0.2046, val_acc: 0.9830\n",
      "101\t{'val_loss': 0.20240439474582672, 'val_acc': 0.9842754602432251}\n",
      "Epoch [103], val_loss: 0.1234, val_acc: 0.9824\n",
      "102\t{'val_loss': 0.11988138407468796, 'val_acc': 0.9852706789970398}\n",
      "Epoch [104], val_loss: 0.1849, val_acc: 0.9812\n",
      "103\t{'val_loss': 0.16612625122070312, 'val_acc': 0.9834793210029602}\n",
      "Epoch [105], val_loss: 0.1175, val_acc: 0.9763\n",
      "104\t{'val_loss': 0.09732209891080856, 'val_acc': 0.9791003465652466}\n",
      "Epoch [106], val_loss: 0.1491, val_acc: 0.9828\n",
      "105\t{'val_loss': 0.1259755641222, 'val_acc': 0.9867635369300842}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [107], val_loss: 0.4625, val_acc: 0.9697\n",
      "106\t{'val_loss': 0.4255182147026062, 'val_acc': 0.9741241931915283}\n",
      "Epoch [108], val_loss: 0.1397, val_acc: 0.9846\n",
      "107\t{'val_loss': 0.12208851426839828, 'val_acc': 0.9858678579330444}\n",
      "Epoch [109], val_loss: 0.1542, val_acc: 0.9826\n",
      "108\t{'val_loss': 0.13489258289337158, 'val_acc': 0.9848726391792297}\n",
      "Epoch [110], val_loss: 0.1818, val_acc: 0.9832\n",
      "109\t{'val_loss': 0.19480320811271667, 'val_acc': 0.9824841022491455}\n",
      "Epoch [111], val_loss: 0.1902, val_acc: 0.9822\n",
      "110\t{'val_loss': 0.16434472799301147, 'val_acc': 0.9826831221580505}\n",
      "Epoch [112], val_loss: 0.1540, val_acc: 0.9802\n",
      "111\t{'val_loss': 0.14823617041110992, 'val_acc': 0.9806926846504211}\n",
      "Epoch [113], val_loss: 0.1352, val_acc: 0.9824\n",
      "112\t{'val_loss': 0.12491754442453384, 'val_acc': 0.9849721193313599}\n",
      "Epoch [114], val_loss: 0.1616, val_acc: 0.9826\n",
      "113\t{'val_loss': 0.11455166339874268, 'val_acc': 0.9824841022491455}\n",
      "Epoch [115], val_loss: 0.2495, val_acc: 0.9784\n",
      "114\t{'val_loss': 0.2499547302722931, 'val_acc': 0.9769108295440674}\n",
      "Epoch [116], val_loss: 0.1727, val_acc: 0.9844\n",
      "115\t{'val_loss': 0.14186948537826538, 'val_acc': 0.9844745397567749}\n",
      "Epoch [117], val_loss: 0.1487, val_acc: 0.9834\n",
      "116\t{'val_loss': 0.11974215507507324, 'val_acc': 0.9857683181762695}\n",
      "Epoch [118], val_loss: 0.1524, val_acc: 0.9842\n",
      "117\t{'val_loss': 0.13543705642223358, 'val_acc': 0.9864649772644043}\n",
      "Epoch [119], val_loss: 0.1569, val_acc: 0.9828\n",
      "118\t{'val_loss': 0.12877343595027924, 'val_acc': 0.9854697585105896}\n",
      "Epoch [120], val_loss: 0.1931, val_acc: 0.9822\n",
      "119\t{'val_loss': 0.1780184805393219, 'val_acc': 0.9837778806686401}\n",
      "Epoch [121], val_loss: 0.1434, val_acc: 0.9830\n",
      "120\t{'val_loss': 0.13364244997501373, 'val_acc': 0.9833797812461853}\n",
      "Epoch [122], val_loss: 0.1841, val_acc: 0.9820\n",
      "121\t{'val_loss': 0.17275485396385193, 'val_acc': 0.9839769005775452}\n",
      "Epoch [123], val_loss: 0.1578, val_acc: 0.9816\n",
      "122\t{'val_loss': 0.12855039536952972, 'val_acc': 0.9824841022491455}\n",
      "Epoch [124], val_loss: 0.1713, val_acc: 0.9814\n",
      "123\t{'val_loss': 0.15762631595134735, 'val_acc': 0.9831807613372803}\n",
      "Epoch [125], val_loss: 0.1784, val_acc: 0.9836\n",
      "124\t{'val_loss': 0.14928840100765228, 'val_acc': 0.9835788011550903}\n",
      "Epoch [126], val_loss: 0.1617, val_acc: 0.9844\n",
      "125\t{'val_loss': 0.1352074295282364, 'val_acc': 0.9869625568389893}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [127], val_loss: 0.1634, val_acc: 0.9842\n",
      "126\t{'val_loss': 0.1368819773197174, 'val_acc': 0.9846735596656799}\n",
      "Epoch [128], val_loss: 0.1514, val_acc: 0.9828\n",
      "127\t{'val_loss': 0.11857408285140991, 'val_acc': 0.9859673380851746}\n",
      "Epoch [129], val_loss: 0.1653, val_acc: 0.9834\n",
      "128\t{'val_loss': 0.1433587521314621, 'val_acc': 0.984375}\n",
      "Epoch [130], val_loss: 0.1282, val_acc: 0.9828\n",
      "129\t{'val_loss': 0.1252075433731079, 'val_acc': 0.9853702187538147}\n",
      "Epoch [131], val_loss: 0.1780, val_acc: 0.9699\n",
      "130\t{'val_loss': 0.16671352088451385, 'val_acc': 0.9736266136169434}\n",
      "Epoch [132], val_loss: 0.1372, val_acc: 0.9848\n",
      "131\t{'val_loss': 0.1445104032754898, 'val_acc': 0.9847730994224548}\n",
      "Epoch [133], val_loss: 0.1349, val_acc: 0.9820\n",
      "132\t{'val_loss': 0.12755988538265228, 'val_acc': 0.9838773608207703}\n",
      "Epoch [134], val_loss: 0.1538, val_acc: 0.9828\n",
      "133\t{'val_loss': 0.14843718707561493, 'val_acc': 0.9846735596656799}\n",
      "Epoch [135], val_loss: 0.1749, val_acc: 0.9838\n",
      "134\t{'val_loss': 0.17213855683803558, 'val_acc': 0.9832802414894104}\n",
      "Epoch [136], val_loss: 0.1612, val_acc: 0.9838\n",
      "135\t{'val_loss': 0.1517198234796524, 'val_acc': 0.9871616363525391}\n",
      "Saved into quant_he_code/weights/quant_hcnn_4bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [137], val_loss: 0.1625, val_acc: 0.9850\n",
      "136\t{'val_loss': 0.16165287792682648, 'val_acc': 0.9848726391792297}\n",
      "Epoch [138], val_loss: 0.2055, val_acc: 0.9751\n",
      "137\t{'val_loss': 0.18041576445102692, 'val_acc': 0.9763137102127075}\n",
      "Epoch [139], val_loss: 0.1935, val_acc: 0.9818\n",
      "138\t{'val_loss': 0.20896483957767487, 'val_acc': 0.9824841022491455}\n",
      "Epoch [140], val_loss: 0.1363, val_acc: 0.9850\n",
      "139\t{'val_loss': 0.1415199190378189, 'val_acc': 0.9850716590881348}\n",
      "Epoch [141], val_loss: 0.1890, val_acc: 0.9828\n",
      "140\t{'val_loss': 0.16329723596572876, 'val_acc': 0.9836783409118652}\n",
      "Epoch [142], val_loss: 0.1336, val_acc: 0.9822\n",
      "141\t{'val_loss': 0.11272048950195312, 'val_acc': 0.9851711988449097}\n",
      "Epoch [143], val_loss: 0.1663, val_acc: 0.9838\n",
      "142\t{'val_loss': 0.14742961525917053, 'val_acc': 0.9846735596656799}\n",
      "Epoch [144], val_loss: 0.2144, val_acc: 0.9790\n",
      "143\t{'val_loss': 0.19321279227733612, 'val_acc': 0.9798964858055115}\n",
      "Epoch [145], val_loss: 0.2066, val_acc: 0.9836\n",
      "144\t{'val_loss': 0.18927933275699615, 'val_acc': 0.9839769005775452}\n",
      "Epoch [146], val_loss: 0.1586, val_acc: 0.9832\n",
      "145\t{'val_loss': 0.1312314122915268, 'val_acc': 0.9862658977508545}\n",
      "Epoch [147], val_loss: 0.2083, val_acc: 0.9844\n",
      "146\t{'val_loss': 0.15683193504810333, 'val_acc': 0.9849721193313599}\n",
      "Epoch [148], val_loss: 0.1656, val_acc: 0.9842\n",
      "147\t{'val_loss': 0.15215080976486206, 'val_acc': 0.9861664175987244}\n",
      "Epoch [149], val_loss: 0.2002, val_acc: 0.9836\n",
      "148\t{'val_loss': 0.1767331063747406, 'val_acc': 0.9851711988449097}\n",
      "Epoch [150], val_loss: 0.1643, val_acc: 0.9844\n",
      "149\t{'val_loss': 0.15086650848388672, 'val_acc': 0.9844745397567749}\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=150, lr=0.001, model=model, \n",
    "               train_loader=train_loader, \n",
    "               val_loader=val_loader, \n",
    "               test_loader=test_loader, \n",
    "               file_path=save_weight_path, \n",
    "               opt_func=torch.optim.Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356275/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 0.15086650848388672, 'val_acc': 0.9844745397567749}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
