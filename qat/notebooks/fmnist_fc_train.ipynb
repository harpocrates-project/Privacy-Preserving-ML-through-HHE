{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import FashionMNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/fmnist')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/fmnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/quant_he_code/weights/quant_fc_2bits_fmnist_plain_2bits_weights.pth')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_bit_width = 2\n",
    "input_bit_width = 2  # quantize into [0,1,2,3]\n",
    "save_weight_path = project_path/f\"quant_he_code/weights/quant_fc_{input_bit_width}bits_fmnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "save_weight_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:  \n",
    "\n",
    "0. Quantize into [0, 0.25, 0.5, 0.75, 1]\n",
    "1. Quantize into [0, 1]\n",
    "2. Quantize into [0, 1, 2, 3]\n",
    "3. Quantize into [0, 1, 2, 3, ..., 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    train_dataset = FashionMNIST(root=mnist_path, download=False, transform=transform)\n",
    "    test_dataset = FashionMNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "torch.Size([784])\n",
      "Processed FashionMNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5ed404e3d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"train_dataset shape: {train_dataset.data.shape}\")\n",
    "print(f\"test_dataset shape: {test_dataset.data.shape}\")\n",
    "\n",
    "im = train_dataset[0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed FashionMNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "tensor([0., 1., 2., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the FC Model (2 linear layers, 1 square activation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT 2-FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FMNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 1 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        # out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = to_device(FMNISTLinearQuantModel(), device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_path, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.8\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(f\"Saved into {file_path.relative_to(project_path)}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75676/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.7908, val_acc: 0.7896\n",
      "Epoch [2], val_loss: 0.5789, val_acc: 0.8269\n",
      "Epoch [3], val_loss: 0.7421, val_acc: 0.8010\n",
      "2\t{'val_loss': 0.7779514193534851, 'val_acc': 0.7892118096351624}\n",
      "Epoch [4], val_loss: 0.7447, val_acc: 0.7856\n",
      "3\t{'val_loss': 0.8091359734535217, 'val_acc': 0.7754777073860168}\n",
      "Epoch [5], val_loss: 0.7393, val_acc: 0.7478\n",
      "4\t{'val_loss': 0.7603796720504761, 'val_acc': 0.7455214858055115}\n",
      "Epoch [6], val_loss: 0.6507, val_acc: 0.8034\n",
      "5\t{'val_loss': 0.6598157286643982, 'val_acc': 0.7963773608207703}\n",
      "Epoch [7], val_loss: 0.6522, val_acc: 0.8008\n",
      "6\t{'val_loss': 0.6863119602203369, 'val_acc': 0.7923964858055115}\n",
      "Epoch [8], val_loss: 0.8066, val_acc: 0.7773\n",
      "7\t{'val_loss': 0.8002647161483765, 'val_acc': 0.7805533409118652}\n",
      "Epoch [9], val_loss: 0.6802, val_acc: 0.7939\n",
      "8\t{'val_loss': 0.7033435702323914, 'val_acc': 0.7893112897872925}\n",
      "Epoch [10], val_loss: 0.7715, val_acc: 0.7668\n",
      "9\t{'val_loss': 0.765008807182312, 'val_acc': 0.7698049545288086}\n",
      "Epoch [11], val_loss: 0.8158, val_acc: 0.7836\n",
      "10\t{'val_loss': 0.8420885801315308, 'val_acc': 0.7797571420669556}\n",
      "Epoch [12], val_loss: 0.7090, val_acc: 0.8101\n",
      "11\t{'val_loss': 0.7185146808624268, 'val_acc': 0.8109076619148254}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_fmnist_plain_2bits_weights.pth\n",
      "Epoch [13], val_loss: 0.8021, val_acc: 0.7666\n",
      "12\t{'val_loss': 0.8269937038421631, 'val_acc': 0.7661226391792297}\n",
      "Epoch [14], val_loss: 0.7185, val_acc: 0.7975\n",
      "13\t{'val_loss': 0.7652752995491028, 'val_acc': 0.7919983863830566}\n",
      "Epoch [15], val_loss: 0.8427, val_acc: 0.8010\n",
      "14\t{'val_loss': 0.8690915703773499, 'val_acc': 0.796875}\n",
      "Epoch [16], val_loss: 0.8421, val_acc: 0.7992\n",
      "15\t{'val_loss': 0.8201066851615906, 'val_acc': 0.7993630766868591}\n",
      "Epoch [17], val_loss: 0.7727, val_acc: 0.7943\n",
      "16\t{'val_loss': 0.8083584904670715, 'val_acc': 0.7850318551063538}\n",
      "Epoch [18], val_loss: 0.9806, val_acc: 0.7840\n",
      "17\t{'val_loss': 0.9909672141075134, 'val_acc': 0.7758758068084717}\n",
      "Epoch [19], val_loss: 1.0357, val_acc: 0.7415\n",
      "18\t{'val_loss': 1.0811607837677002, 'val_acc': 0.7463176846504211}\n",
      "Epoch [20], val_loss: 0.9025, val_acc: 0.7909\n",
      "19\t{'val_loss': 0.8835017681121826, 'val_acc': 0.7900079488754272}\n",
      "Epoch [21], val_loss: 0.7071, val_acc: 0.7943\n",
      "20\t{'val_loss': 0.7370642423629761, 'val_acc': 0.7898089289665222}\n",
      "Epoch [22], val_loss: 0.7183, val_acc: 0.8097\n",
      "21\t{'val_loss': 0.7697775363922119, 'val_acc': 0.8017516136169434}\n",
      "Epoch [23], val_loss: 0.7183, val_acc: 0.7898\n",
      "22\t{'val_loss': 0.7407103180885315, 'val_acc': 0.7909036874771118}\n",
      "Epoch [24], val_loss: 0.7122, val_acc: 0.8022\n",
      "23\t{'val_loss': 0.7405896186828613, 'val_acc': 0.7950835824012756}\n",
      "Epoch [25], val_loss: 0.9060, val_acc: 0.7917\n",
      "24\t{'val_loss': 0.9482740759849548, 'val_acc': 0.7836385369300842}\n",
      "Epoch [26], val_loss: 0.9379, val_acc: 0.7981\n",
      "25\t{'val_loss': 0.944663941860199, 'val_acc': 0.7909036874771118}\n",
      "Epoch [27], val_loss: 0.8218, val_acc: 0.8062\n",
      "26\t{'val_loss': 0.8500641584396362, 'val_acc': 0.8009554147720337}\n",
      "Epoch [28], val_loss: 0.8768, val_acc: 0.8091\n",
      "27\t{'val_loss': 0.9138903021812439, 'val_acc': 0.7944864630699158}\n",
      "Epoch [29], val_loss: 0.8386, val_acc: 0.8032\n",
      "28\t{'val_loss': 0.8946192264556885, 'val_acc': 0.7926950454711914}\n",
      "Epoch [30], val_loss: 0.8583, val_acc: 0.7767\n",
      "29\t{'val_loss': 0.8644618391990662, 'val_acc': 0.7680135369300842}\n",
      "Epoch [31], val_loss: 0.8628, val_acc: 0.8036\n",
      "30\t{'val_loss': 0.8476147651672363, 'val_acc': 0.8020501732826233}\n",
      "Epoch [32], val_loss: 0.7344, val_acc: 0.8038\n",
      "31\t{'val_loss': 0.7690116167068481, 'val_acc': 0.7934912443161011}\n",
      "Epoch [33], val_loss: 1.0024, val_acc: 0.7591\n",
      "32\t{'val_loss': 0.9998626112937927, 'val_acc': 0.7570660710334778}\n",
      "Epoch [34], val_loss: 0.9295, val_acc: 0.7884\n",
      "33\t{'val_loss': 0.9344002604484558, 'val_acc': 0.7826433181762695}\n",
      "Epoch [35], val_loss: 1.0073, val_acc: 0.7860\n",
      "34\t{'val_loss': 0.9977441430091858, 'val_acc': 0.7936902642250061}\n",
      "Epoch [36], val_loss: 0.6329, val_acc: 0.8196\n",
      "35\t{'val_loss': 0.6840075850486755, 'val_acc': 0.8070262670516968}\n",
      "Epoch [37], val_loss: 1.0681, val_acc: 0.7381\n",
      "36\t{'val_loss': 1.0826032161712646, 'val_acc': 0.7383558750152588}\n",
      "Epoch [38], val_loss: 2.7052, val_acc: 0.7344\n",
      "37\t{'val_loss': 2.6088650226593018, 'val_acc': 0.7271098494529724}\n",
      "Epoch [39], val_loss: 0.7011, val_acc: 0.8159\n",
      "38\t{'val_loss': 0.7150346636772156, 'val_acc': 0.8106091022491455}\n",
      "Epoch [40], val_loss: 1.0290, val_acc: 0.7811\n",
      "39\t{'val_loss': 1.080509066581726, 'val_acc': 0.7772691249847412}\n",
      "Epoch [41], val_loss: 0.7144, val_acc: 0.8169\n",
      "40\t{'val_loss': 0.7776710391044617, 'val_acc': 0.8070262670516968}\n",
      "Epoch [42], val_loss: 0.7253, val_acc: 0.8180\n",
      "41\t{'val_loss': 0.7684456706047058, 'val_acc': 0.8065286874771118}\n",
      "Epoch [43], val_loss: 0.7247, val_acc: 0.8161\n",
      "42\t{'val_loss': 0.794441282749176, 'val_acc': 0.8019506335258484}\n",
      "Epoch [44], val_loss: 0.9448, val_acc: 0.7896\n",
      "43\t{'val_loss': 0.9937999844551086, 'val_acc': 0.7761743664741516}\n",
      "Epoch [45], val_loss: 0.7720, val_acc: 0.8034\n",
      "44\t{'val_loss': 0.8254713416099548, 'val_acc': 0.7914012670516968}\n",
      "Epoch [46], val_loss: 0.7606, val_acc: 0.8060\n",
      "45\t{'val_loss': 0.7825355529785156, 'val_acc': 0.8046377301216125}\n",
      "Epoch [47], val_loss: 0.7873, val_acc: 0.8064\n",
      "46\t{'val_loss': 0.8200128674507141, 'val_acc': 0.7971735596656799}\n",
      "Epoch [48], val_loss: 0.7126, val_acc: 0.8131\n",
      "47\t{'val_loss': 0.7750887870788574, 'val_acc': 0.800457775592804}\n",
      "Epoch [49], val_loss: 1.1662, val_acc: 0.7801\n",
      "48\t{'val_loss': 1.2002657651901245, 'val_acc': 0.7711982727050781}\n",
      "Epoch [50], val_loss: 0.9948, val_acc: 0.7789\n",
      "49\t{'val_loss': 1.0524183511734009, 'val_acc': 0.7760748267173767}\n",
      "Epoch [51], val_loss: 0.8797, val_acc: 0.8024\n",
      "50\t{'val_loss': 0.9937093257904053, 'val_acc': 0.7893112897872925}\n",
      "Epoch [52], val_loss: 0.7734, val_acc: 0.8040\n",
      "51\t{'val_loss': 0.8593717217445374, 'val_acc': 0.7956807613372803}\n",
      "Epoch [53], val_loss: 0.9179, val_acc: 0.7830\n",
      "52\t{'val_loss': 0.9672324657440186, 'val_acc': 0.777965784072876}\n",
      "Epoch [54], val_loss: 0.9702, val_acc: 0.7824\n",
      "53\t{'val_loss': 1.011603832244873, 'val_acc': 0.7713972926139832}\n",
      "Epoch [55], val_loss: 0.9155, val_acc: 0.7915\n",
      "54\t{'val_loss': 0.9519963264465332, 'val_acc': 0.7800557613372803}\n",
      "Epoch [56], val_loss: 1.1227, val_acc: 0.7534\n",
      "55\t{'val_loss': 1.210311770439148, 'val_acc': 0.7461186051368713}\n",
      "Epoch [57], val_loss: 1.5011, val_acc: 0.7257\n",
      "56\t{'val_loss': 1.6229280233383179, 'val_acc': 0.7202428579330444}\n",
      "Epoch [58], val_loss: 1.3287, val_acc: 0.7431\n",
      "57\t{'val_loss': 1.346761703491211, 'val_acc': 0.7369625568389893}\n",
      "Epoch [59], val_loss: 1.1215, val_acc: 0.7844\n",
      "58\t{'val_loss': 1.1389679908752441, 'val_acc': 0.781050980091095}\n",
      "Epoch [60], val_loss: 1.1904, val_acc: 0.7621\n",
      "59\t{'val_loss': 1.1803263425827026, 'val_acc': 0.7617436051368713}\n",
      "Epoch [61], val_loss: 1.4452, val_acc: 0.7526\n",
      "60\t{'val_loss': 1.455496072769165, 'val_acc': 0.7547770738601685}\n",
      "Epoch [62], val_loss: 0.8875, val_acc: 0.7917\n",
      "61\t{'val_loss': 0.8985525965690613, 'val_acc': 0.7887141704559326}\n",
      "Epoch [63], val_loss: 0.9538, val_acc: 0.7935\n",
      "62\t{'val_loss': 0.9351381063461304, 'val_acc': 0.7897093892097473}\n",
      "Epoch [64], val_loss: 0.7087, val_acc: 0.8198\n",
      "63\t{'val_loss': 0.7626990079879761, 'val_acc': 0.8108081221580505}\n",
      "Epoch [65], val_loss: 0.8511, val_acc: 0.8172\n",
      "64\t{'val_loss': 0.8978027701377869, 'val_acc': 0.8060310482978821}\n",
      "Epoch [66], val_loss: 1.1918, val_acc: 0.7690\n",
      "65\t{'val_loss': 1.188292384147644, 'val_acc': 0.7667197585105896}\n",
      "Epoch [67], val_loss: 1.2520, val_acc: 0.7696\n",
      "66\t{'val_loss': 1.378771424293518, 'val_acc': 0.7536823153495789}\n",
      "Epoch [68], val_loss: 0.7325, val_acc: 0.8224\n",
      "67\t{'val_loss': 0.7669838070869446, 'val_acc': 0.805832028388977}\n",
      "Epoch [69], val_loss: 0.9050, val_acc: 0.8048\n",
      "68\t{'val_loss': 0.9591565132141113, 'val_acc': 0.7941879034042358}\n",
      "Epoch [70], val_loss: 1.3012, val_acc: 0.7688\n",
      "69\t{'val_loss': 1.2974573373794556, 'val_acc': 0.7630374431610107}\n",
      "Epoch [71], val_loss: 1.5521, val_acc: 0.7516\n",
      "70\t{'val_loss': 1.5375332832336426, 'val_acc': 0.7509952187538147}\n",
      "Epoch [72], val_loss: 1.2299, val_acc: 0.7617\n",
      "71\t{'val_loss': 1.174409031867981, 'val_acc': 0.7628383636474609}\n",
      "Epoch [73], val_loss: 0.8059, val_acc: 0.7981\n",
      "72\t{'val_loss': 0.8114714026451111, 'val_acc': 0.7955812215805054}\n",
      "Epoch [74], val_loss: 1.9365, val_acc: 0.7263\n",
      "73\t{'val_loss': 2.0056755542755127, 'val_acc': 0.7141719460487366}\n",
      "Epoch [75], val_loss: 1.6663, val_acc: 0.7397\n",
      "74\t{'val_loss': 1.6412935256958008, 'val_acc': 0.7423368096351624}\n",
      "Epoch [76], val_loss: 1.0160, val_acc: 0.7894\n",
      "75\t{'val_loss': 1.0701797008514404, 'val_acc': 0.7794585824012756}\n",
      "Epoch [77], val_loss: 1.0162, val_acc: 0.7816\n",
      "76\t{'val_loss': 1.077644944190979, 'val_acc': 0.7743829488754272}\n",
      "Epoch [78], val_loss: 0.8444, val_acc: 0.7894\n",
      "77\t{'val_loss': 0.8653162121772766, 'val_acc': 0.7821456789970398}\n",
      "Epoch [79], val_loss: 1.2029, val_acc: 0.7603\n",
      "78\t{'val_loss': 1.1794445514678955, 'val_acc': 0.7560708522796631}\n",
      "Epoch [80], val_loss: 1.6459, val_acc: 0.7221\n",
      "79\t{'val_loss': 1.5899361371994019, 'val_acc': 0.7182523608207703}\n",
      "Epoch [81], val_loss: 1.6708, val_acc: 0.7571\n",
      "80\t{'val_loss': 1.891499400138855, 'val_acc': 0.7458200454711914}\n",
      "Epoch [82], val_loss: 1.2618, val_acc: 0.7633\n",
      "81\t{'val_loss': 1.3303978443145752, 'val_acc': 0.7520899772644043}\n",
      "Epoch [83], val_loss: 0.9960, val_acc: 0.7710\n",
      "82\t{'val_loss': 1.0457334518432617, 'val_acc': 0.7635350227355957}\n",
      "Epoch [84], val_loss: 1.0158, val_acc: 0.7929\n",
      "83\t{'val_loss': 0.9897289872169495, 'val_acc': 0.7899084687232971}\n",
      "Epoch [85], val_loss: 0.9720, val_acc: 0.8048\n",
      "84\t{'val_loss': 0.9971984028816223, 'val_acc': 0.7967754602432251}\n",
      "Epoch [86], val_loss: 1.0256, val_acc: 0.7832\n",
      "85\t{'val_loss': 1.058402419090271, 'val_acc': 0.7833399772644043}\n",
      "Epoch [87], val_loss: 1.0168, val_acc: 0.7935\n",
      "86\t{'val_loss': 1.0464223623275757, 'val_acc': 0.781449019908905}\n",
      "Epoch [88], val_loss: 1.0544, val_acc: 0.8014\n",
      "87\t{'val_loss': 1.0644145011901855, 'val_acc': 0.7887141704559326}\n",
      "Epoch [89], val_loss: 1.3252, val_acc: 0.7595\n",
      "88\t{'val_loss': 1.3048416376113892, 'val_acc': 0.7617436051368713}\n",
      "Epoch [90], val_loss: 1.1072, val_acc: 0.7935\n",
      "89\t{'val_loss': 1.0880309343338013, 'val_acc': 0.790704607963562}\n",
      "Epoch [91], val_loss: 0.9519, val_acc: 0.7917\n",
      "90\t{'val_loss': 1.020148754119873, 'val_acc': 0.7819466590881348}\n",
      "Epoch [92], val_loss: 0.9617, val_acc: 0.7996\n",
      "91\t{'val_loss': 0.906697690486908, 'val_acc': 0.7973726391792297}\n",
      "Epoch [93], val_loss: 1.0738, val_acc: 0.7646\n",
      "92\t{'val_loss': 1.0789661407470703, 'val_acc': 0.7653264403343201}\n",
      "Epoch [94], val_loss: 1.0511, val_acc: 0.7989\n",
      "93\t{'val_loss': 1.0469709634780884, 'val_acc': 0.7892118096351624}\n",
      "Epoch [95], val_loss: 1.2117, val_acc: 0.7563\n",
      "94\t{'val_loss': 1.1509904861450195, 'val_acc': 0.7580612897872925}\n",
      "Epoch [96], val_loss: 1.4634, val_acc: 0.7700\n",
      "95\t{'val_loss': 1.6702332496643066, 'val_acc': 0.7575637102127075}\n",
      "Epoch [97], val_loss: 1.3519, val_acc: 0.7720\n",
      "96\t{'val_loss': 1.377918004989624, 'val_acc': 0.7653264403343201}\n",
      "Epoch [98], val_loss: 1.5344, val_acc: 0.7458\n",
      "97\t{'val_loss': 1.4997831583023071, 'val_acc': 0.7510947585105896}\n",
      "Epoch [99], val_loss: 1.4739, val_acc: 0.7455\n",
      "98\t{'val_loss': 1.4819438457489014, 'val_acc': 0.7396496534347534}\n",
      "Epoch [100], val_loss: 0.9987, val_acc: 0.7725\n",
      "99\t{'val_loss': 0.9912408590316772, 'val_acc': 0.7703025341033936}\n",
      "Epoch [101], val_loss: 0.8030, val_acc: 0.8048\n",
      "100\t{'val_loss': 0.8940408229827881, 'val_acc': 0.7859275341033936}\n",
      "Epoch [102], val_loss: 1.7973, val_acc: 0.7739\n",
      "101\t{'val_loss': 1.915055274963379, 'val_acc': 0.7545780539512634}\n",
      "Epoch [103], val_loss: 0.8722, val_acc: 0.7725\n",
      "102\t{'val_loss': 0.9226479530334473, 'val_acc': 0.7701035141944885}\n",
      "Epoch [104], val_loss: 1.2118, val_acc: 0.7937\n",
      "103\t{'val_loss': 1.2036430835723877, 'val_acc': 0.7884156107902527}\n",
      "Epoch [105], val_loss: 1.0683, val_acc: 0.7763\n",
      "104\t{'val_loss': 1.116749882698059, 'val_acc': 0.7737858295440674}\n",
      "Epoch [106], val_loss: 1.0283, val_acc: 0.7793\n",
      "105\t{'val_loss': 1.0218194723129272, 'val_acc': 0.7753781676292419}\n",
      "Epoch [107], val_loss: 0.7717, val_acc: 0.8165\n",
      "106\t{'val_loss': 0.8352847695350647, 'val_acc': 0.8017516136169434}\n",
      "Epoch [108], val_loss: 1.6221, val_acc: 0.7051\n",
      "107\t{'val_loss': 1.703701138496399, 'val_acc': 0.7028264403343201}\n",
      "Epoch [109], val_loss: 0.8758, val_acc: 0.7947\n",
      "108\t{'val_loss': 0.9623517990112305, 'val_acc': 0.7890127301216125}\n",
      "Epoch [110], val_loss: 1.0769, val_acc: 0.8131\n",
      "109\t{'val_loss': 1.1608153581619263, 'val_acc': 0.8049362897872925}\n",
      "Epoch [111], val_loss: 2.3836, val_acc: 0.6764\n",
      "110\t{'val_loss': 2.4572694301605225, 'val_acc': 0.6781449317932129}\n",
      "Epoch [112], val_loss: 1.5830, val_acc: 0.7852\n",
      "111\t{'val_loss': 1.6975020170211792, 'val_acc': 0.7740843892097473}\n",
      "Epoch [113], val_loss: 1.2258, val_acc: 0.7767\n",
      "112\t{'val_loss': 1.309618592262268, 'val_acc': 0.7619426846504211}\n",
      "Epoch [114], val_loss: 1.4431, val_acc: 0.7702\n",
      "113\t{'val_loss': 1.4116308689117432, 'val_acc': 0.7677149772644043}\n",
      "Epoch [115], val_loss: 1.2850, val_acc: 0.7696\n",
      "114\t{'val_loss': 1.372389316558838, 'val_acc': 0.7612460255622864}\n",
      "Epoch [116], val_loss: 1.8784, val_acc: 0.7316\n",
      "115\t{'val_loss': 2.0035817623138428, 'val_acc': 0.7234275341033936}\n",
      "Epoch [117], val_loss: 1.3327, val_acc: 0.7601\n",
      "116\t{'val_loss': 1.3692007064819336, 'val_acc': 0.7503980994224548}\n",
      "Epoch [118], val_loss: 1.1791, val_acc: 0.7682\n",
      "117\t{'val_loss': 1.2588897943496704, 'val_acc': 0.7551751732826233}\n",
      "Epoch [119], val_loss: 3.3728, val_acc: 0.6774\n",
      "118\t{'val_loss': 3.429438829421997, 'val_acc': 0.6787420511245728}\n",
      "Epoch [120], val_loss: 1.3003, val_acc: 0.7852\n",
      "119\t{'val_loss': 1.2779772281646729, 'val_acc': 0.7817476391792297}\n",
      "Epoch [121], val_loss: 1.7235, val_acc: 0.7282\n",
      "120\t{'val_loss': 1.7826253175735474, 'val_acc': 0.7311902642250061}\n",
      "Epoch [122], val_loss: 1.1228, val_acc: 0.7654\n",
      "121\t{'val_loss': 1.180343747138977, 'val_acc': 0.7573646306991577}\n",
      "Epoch [123], val_loss: 1.3049, val_acc: 0.7820\n",
      "122\t{'val_loss': 1.354404091835022, 'val_acc': 0.7764729261398315}\n",
      "Epoch [124], val_loss: 0.7700, val_acc: 0.8222\n",
      "123\t{'val_loss': 0.8283085823059082, 'val_acc': 0.8070262670516968}\n",
      "Epoch [125], val_loss: 1.9488, val_acc: 0.7674\n",
      "124\t{'val_loss': 2.004486083984375, 'val_acc': 0.7562699317932129}\n",
      "Epoch [126], val_loss: 0.8802, val_acc: 0.7771\n",
      "125\t{'val_loss': 0.916610836982727, 'val_acc': 0.7711982727050781}\n",
      "Epoch [127], val_loss: 1.5104, val_acc: 0.7583\n",
      "126\t{'val_loss': 1.5845506191253662, 'val_acc': 0.7515923380851746}\n",
      "Epoch [128], val_loss: 1.2680, val_acc: 0.7830\n",
      "127\t{'val_loss': 1.2942684888839722, 'val_acc': 0.7662221193313599}\n",
      "Epoch [129], val_loss: 0.9020, val_acc: 0.7959\n",
      "128\t{'val_loss': 0.9167696237564087, 'val_acc': 0.7942874431610107}\n",
      "Epoch [130], val_loss: 1.1488, val_acc: 0.7678\n",
      "129\t{'val_loss': 1.1847522258758545, 'val_acc': 0.7616441249847412}\n",
      "Epoch [131], val_loss: 1.4323, val_acc: 0.7666\n",
      "130\t{'val_loss': 1.3989001512527466, 'val_acc': 0.7749800682067871}\n",
      "Epoch [132], val_loss: 1.1124, val_acc: 0.7694\n",
      "131\t{'val_loss': 1.1839057207107544, 'val_acc': 0.7596536874771118}\n",
      "Epoch [133], val_loss: 1.6380, val_acc: 0.7579\n",
      "132\t{'val_loss': 1.6798052787780762, 'val_acc': 0.7510947585105896}\n",
      "Epoch [134], val_loss: 1.0227, val_acc: 0.8139\n",
      "133\t{'val_loss': 1.057674527168274, 'val_acc': 0.8024482727050781}\n",
      "Epoch [135], val_loss: 1.4427, val_acc: 0.7739\n",
      "134\t{'val_loss': 1.3869563341140747, 'val_acc': 0.7659235596656799}\n",
      "Epoch [136], val_loss: 1.8245, val_acc: 0.7294\n",
      "135\t{'val_loss': 1.8607192039489746, 'val_acc': 0.7291998267173767}\n",
      "Epoch [137], val_loss: 1.5187, val_acc: 0.7769\n",
      "136\t{'val_loss': 1.715625524520874, 'val_acc': 0.7569665312767029}\n",
      "Epoch [138], val_loss: 1.2878, val_acc: 0.7949\n",
      "137\t{'val_loss': 1.4334975481033325, 'val_acc': 0.7835389971733093}\n",
      "Epoch [139], val_loss: 0.9937, val_acc: 0.7941\n",
      "138\t{'val_loss': 1.0822433233261108, 'val_acc': 0.7818471193313599}\n",
      "Epoch [140], val_loss: 1.3393, val_acc: 0.7931\n",
      "139\t{'val_loss': 1.4392822980880737, 'val_acc': 0.7746815085411072}\n",
      "Epoch [141], val_loss: 0.9300, val_acc: 0.8016\n",
      "140\t{'val_loss': 0.9644388556480408, 'val_acc': 0.7883160710334778}\n",
      "Epoch [142], val_loss: 1.1363, val_acc: 0.7850\n",
      "141\t{'val_loss': 1.165525197982788, 'val_acc': 0.7792595624923706}\n",
      "Epoch [143], val_loss: 2.5876, val_acc: 0.7773\n",
      "142\t{'val_loss': 2.5867390632629395, 'val_acc': 0.7709991931915283}\n",
      "Epoch [144], val_loss: 1.8363, val_acc: 0.7146\n",
      "143\t{'val_loss': 1.879317045211792, 'val_acc': 0.7072054147720337}\n",
      "Epoch [145], val_loss: 1.1496, val_acc: 0.7998\n",
      "144\t{'val_loss': 1.1911416053771973, 'val_acc': 0.7913017272949219}\n",
      "Epoch [146], val_loss: 1.1860, val_acc: 0.7646\n",
      "145\t{'val_loss': 1.2751575708389282, 'val_acc': 0.7541799545288086}\n",
      "Epoch [147], val_loss: 0.9646, val_acc: 0.7965\n",
      "146\t{'val_loss': 1.0279765129089355, 'val_acc': 0.7743829488754272}\n",
      "Epoch [148], val_loss: 1.1814, val_acc: 0.7545\n",
      "147\t{'val_loss': 1.2074936628341675, 'val_acc': 0.7450239062309265}\n",
      "Epoch [149], val_loss: 1.0919, val_acc: 0.7834\n",
      "148\t{'val_loss': 1.1718947887420654, 'val_acc': 0.774582028388977}\n",
      "Epoch [150], val_loss: 2.0064, val_acc: 0.7350\n",
      "149\t{'val_loss': 2.0952341556549072, 'val_acc': 0.7266122698783875}\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=150, lr=0.001, model=model, \n",
    "               train_loader=train_loader, \n",
    "               val_loader=val_loader, \n",
    "               test_loader=test_loader, \n",
    "               file_path=save_weight_path, \n",
    "               opt_func=torch.optim.Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75676/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 2.0952341556549072, 'val_acc': 0.7266122698783875}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1]\n",
      "[-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "int_fc1 = np.array(model.fc1.quant_weight().int().tolist())\n",
    "int_fc2 = np.array(model.fc2.quant_weight().int().tolist())\n",
    "\n",
    "print(np.unique(int_fc1))\n",
    "print(np.unique(int_fc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
