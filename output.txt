
+----------------------------------------------------------------------------------------------------------------+
|         PocketNN: Training on hypnogram data using direct feedback alignment with a 1-layer FC network         |
+----------------------------------------------------------------------------------------------------------------+
----- Loading hypnogram data ----- 
Loading data/hypnogram/hypnogram_input_train.csv
(36865, 300)
Loading data/hypnogram/hypnogram_output_train.csv
(36865, 1)
Loading data/hypnogram/hypnogram_input_test.csv
(9217, 300)
Loading data/hypnogram/hypnogram_output_test.csv
(9217, 1)
----- Defining the neural net ----- 
Neural network with 1 fc layer and pocket_sigmoid activation
----- Initial stats before training ----- 
Initial train correct predictions: 25682 (out of 36865 examples)
Initial train accuracy: 69.665%
----- Initial stats before testing ----- 
Initial test correct predictions: 6424 (out of 9217 examples)
Initial test accuracy: 69.6973%
----- Start training -----
Learning Rate Inverse = 50, numTrainSamples = 36865, miniBatchSize = 4, numEpochs = 100, weight lower bound = -2047, weight upper bound = 2048
Epoch | SumLoss | NumCorrect | Accuracy
1 | 95436422 | 24982 | 0.677662
2 | 104658756 | 23817 | 0.64606
3 | 111334024 | 22981 | 0.623383
4 | 107520050 | 23450 | 0.636105
5 | 116589348 | 22310 | 0.605181
6 | 115124036 | 22489 | 0.610037
7 | 111868182 | 22890 | 0.620914
8 | 114594746 | 22547 | 0.61161
9 | 111305272 | 22948 | 0.622487
10 | 119739380 | 21879 | 0.59349
11 | 100345864 | 24263 | 0.658158
12 | 106247914 | 23524 | 0.638112
13 | 115430486 | 22417 | 0.608084
14 | 93357702 | 25185 | 0.683168
15 | 101013584 | 24187 | 0.656097
16 | 95260432 | 24883 | 0.674976
17 | 102086768 | 24064 | 0.65276
18 | 87519890 | 25876 | 0.701912
19 | 96681214 | 24714 | 0.670392
20 | 78494056 | 26920 | 0.730232
21 | 78433006 | 26887 | 0.729337
22 | 77843978 | 26994 | 0.732239
23 | 78112588 | 26962 | 0.731371
24 | 77541114 | 27034 | 0.733324
25 | 77456192 | 27028 | 0.733162
26 | 80131416 | 26700 | 0.724264
27 | 77235744 | 27074 | 0.734409
28 | 77627776 | 27041 | 0.733514
29 | 76916382 | 27097 | 0.735033
30 | 75656810 | 27221 | 0.738397
31 | 75463800 | 27268 | 0.739672
32 | 75834818 | 27210 | 0.738098
33 | 75437402 | 27267 | 0.739645
34 | 75458456 | 27252 | 0.739238
35 | 75479290 | 27270 | 0.739726
36 | 75505716 | 27252 | 0.739238
37 | 75357398 | 27236 | 0.738804
38 | 75108332 | 27326 | 0.741245
39 | 75215402 | 27296 | 0.740431
40 | 74015234 | 27419 | 0.743768
41 | 74841312 | 27359 | 0.74214
42 | 74281418 | 27399 | 0.743225
43 | 74252482 | 27357 | 0.742086
44 | 74181026 | 27392 | 0.743035
45 | 74078508 | 27390 | 0.742981
46 | 74092024 | 27377 | 0.742629
47 | 74175250 | 27391 | 0.743008
48 | 73904450 | 27441 | 0.744365
49 | 73981790 | 27400 | 0.743252
50 | 73544228 | 27425 | 0.743931
51 | 73736646 | 27431 | 0.744093
52 | 73596152 | 27463 | 0.744961
53 | 73761340 | 27452 | 0.744663
54 | 73516370 | 27435 | 0.744202
55 | 73648672 | 27479 | 0.745395
56 | 73693066 | 27458 | 0.744826
57 | 73619228 | 27440 | 0.744337
58 | 73921668 | 27449 | 0.744582
59 | 73459100 | 27443 | 0.744419
60 | 73207094 | 27499 | 0.745938
61 | 73226762 | 27510 | 0.746236
62 | 73423812 | 27509 | 0.746209
63 | 73540542 | 27488 | 0.74564
64 | 73443320 | 27514 | 0.746345
65 | 73368172 | 27486 | 0.745585
66 | 73530726 | 27497 | 0.745884
67 | 73260688 | 27502 | 0.746019
68 | 73193220 | 27490 | 0.745694
69 | 73304292 | 27517 | 0.746426
70 | 73558284 | 27499 | 0.745938
71 | 73558284 | 27499 | 0.745938
72 | 73558284 | 27499 | 0.745938
73 | 73558284 | 27499 | 0.745938
74 | 73558284 | 27499 | 0.745938
75 | 73558284 | 27499 | 0.745938
76 | 73558284 | 27499 | 0.745938
77 | 73558284 | 27499 | 0.745938
78 | 73558284 | 27499 | 0.745938
79 | 73558284 | 27499 | 0.745938
80 | 73550220 | 27500 | 0.745965
81 | 73558284 | 27499 | 0.745938
82 | 73558284 | 27499 | 0.745938
83 | 73558284 | 27499 | 0.745938
84 | 73558284 | 27499 | 0.745938
85 | 73558284 | 27499 | 0.745938
86 | 73550220 | 27500 | 0.745965
87 | 73558284 | 27499 | 0.745938
88 | 73558284 | 27499 | 0.745938
89 | 73558284 | 27499 | 0.745938
90 | 73558284 | 27499 | 0.745938
91 | 73550220 | 27500 | 0.745965
92 | 73558284 | 27499 | 0.745938
93 | 73558284 | 27499 | 0.745938
94 | 73550220 | 27500 | 0.745965
95 | 73550220 | 27500 | 0.745965
96 | 73558284 | 27499 | 0.745938
97 | 73550220 | 27500 | 0.745965
98 | 73550220 | 27500 | 0.745965
99 | 73558284 | 27499 | 0.745938
100 | 73558284 | 27499 | 0.745938
Epoch | NumCorrect | TestAccuracy 
1 | 6421 | 0.696648
found best test accuracy = 0.696648 at epoch 1. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
2 | 6427 | 0.697298
found best test accuracy = 0.697298 at epoch 2. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
3 | 6423 | 0.696864
4 | 6422 | 0.696756
5 | 2653 | 0.287838
6 | 6503 | 0.705544
found best test accuracy = 0.705544 at epoch 6. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
7 | 6435 | 0.698166
8 | 2741 | 0.297385
9 | 2579 | 0.279809
10 | 6574 | 0.713247
found best test accuracy = 0.713247 at epoch 10. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
11 | 6734 | 0.730606
found best test accuracy = 0.730606 at epoch 11. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
12 | 6493 | 0.704459
13 | 6680 | 0.724748
14 | 6582 | 0.714115
15 | 6694 | 0.726267
16 | 6595 | 0.715526
17 | 2628 | 0.285125
18 | 6798 | 0.737550
found best test accuracy = 0.737550 at epoch 18. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
19 | 6707 | 0.727677
20 | 6775 | 0.735055
21 | 6797 | 0.737442
22 | 6734 | 0.730606
23 | 6797 | 0.737442
24 | 6787 | 0.736357
25 | 6734 | 0.730606
26 | 6713 | 0.728328
27 | 6753 | 0.732668
28 | 6782 | 0.735814
29 | 6817 | 0.739612
found best test accuracy = 0.739612 at epoch 29. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
30 | 6821 | 0.740046
found best test accuracy = 0.740046 at epoch 30. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
31 | 6810 | 0.738852
32 | 6752 | 0.732559
33 | 6755 | 0.732885
34 | 6803 | 0.738093
35 | 6758 | 0.733210
36 | 6818 | 0.739720
37 | 6819 | 0.739829
38 | 6775 | 0.735055
39 | 6829 | 0.740914
found best test accuracy = 0.740914 at epoch 39. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
40 | 6779 | 0.735489
41 | 6833 | 0.741347
found best test accuracy = 0.741347 at epoch 41. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
42 | 6820 | 0.739937
43 | 6762 | 0.733644
44 | 6835 | 0.741565
found best test accuracy = 0.741565 at epoch 44. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
45 | 6802 | 0.737984
46 | 6826 | 0.740588
47 | 6841 | 0.742215
found best test accuracy = 0.742215 at epoch 47. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
48 | 6842 | 0.742324
found best test accuracy = 0.742324 at epoch 48. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp2048.csv
49 | 6822 | 0.740154
50 | 6837 | 0.741781
51 | 6784 | 0.736031
52 | 6839 | 0.741998
53 | 6837 | 0.741781
54 | 6777 | 0.735272
55 | 6813 | 0.739178
56 | 6836 | 0.741673
57 | 6827 | 0.740697
58 | 6829 | 0.740914
59 | 6826 | 0.740588
60 | 6827 | 0.740697
61 | 6833 | 0.741347
62 | 6830 | 0.741022
63 | 6829 | 0.740914
64 | 6842 | 0.742324
65 | 6830 | 0.741022
66 | 6825 | 0.740480
67 | 6839 | 0.741998
68 | 6842 | 0.742324
69 | 6828 | 0.740805
70 | 6828 | 0.740805
71 | 6828 | 0.740805
72 | 6828 | 0.740805
73 | 6828 | 0.740805
74 | 6828 | 0.740805
75 | 6828 | 0.740805
76 | 6828 | 0.740805
77 | 6828 | 0.740805
78 | 6828 | 0.740805
79 | 6828 | 0.740805
80 | 6828 | 0.740805
81 | 6828 | 0.740805
82 | 6828 | 0.740805
83 | 6828 | 0.740805
84 | 6828 | 0.740805
85 | 6828 | 0.740805
86 | 6828 | 0.740805
87 | 6828 | 0.740805
88 | 6828 | 0.740805
89 | 6828 | 0.740805
90 | 6828 | 0.740805
91 | 6828 | 0.740805
92 | 6828 | 0.740805
93 | 6828 | 0.740805
94 | 6828 | 0.740805
95 | 6828 | 0.740805
96 | 6828 | 0.740805
97 | 6828 | 0.740805
98 | 6828 | 0.740805
99 | 6828 | 0.740805
100 | 6828 | 0.740805
----- Results -----
best train accuracy = 0.746426 at epoch 69
best test accuracy = 0.742324 at epoch 48
trained weight shape: (300, 1)
trained weight average = -17; trained weight max value = 2043; trained weight min value = -2040
trained bias shape: (1, 1)
trained bias = -105 
