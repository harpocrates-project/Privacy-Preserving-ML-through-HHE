+----------------------------------------------------------------------------------------------------------------+
|         PocketNN: Training on hypnogram data using direct feedback alignment with a 1-layer FC network         |
+----------------------------------------------------------------------------------------------------------------+
----- Loading hypnogram data ----- 
Loading data/hypnogram/hypnogram_input_train.csv
(36865, 300)
Loading data/hypnogram/hypnogram_output_train.csv
(36865, 1)
Loading data/hypnogram/hypnogram_input_test.csv
(9217, 300)
Loading data/hypnogram/hypnogram_output_test.csv
(9217, 1)
----- Defining the neural net ----- 
Neural network with 1 fc layer and pocket_sigmoid activation
----- Initial stats before training ----- 
Initial train correct predictions: 25682 (out of 36865 examples)
Initial train accuracy: 69.665%
----- Initial stats before testing ----- 
Initial test correct predictions: 6424 (out of 9217 examples)
Initial test accuracy: 69.6973%
----- Start training -----
Learning Rate Inverse = 50, numTrainSamples = 36865, miniBatchSize = 4, numEpochs = 100, weight lower bound = -1023, weight upper bound = 1024
Epoch | SumLoss | NumCorrect | Accuracy
1 | 95436422 | 24982 | 0.677662
2 | 104658756 | 23817 | 0.64606
3 | 111334024 | 22981 | 0.623383
4 | 107520050 | 23450 | 0.636105
5 | 116589348 | 22310 | 0.605181
6 | 116239868 | 22347 | 0.606185
7 | 109274834 | 23234 | 0.630246
8 | 114282662 | 22578 | 0.612451
9 | 110238854 | 23079 | 0.626041
10 | 121182040 | 21736 | 0.589611
11 | 117909808 | 22102 | 0.599539
12 | 122629678 | 21551 | 0.584592
13 | 116910434 | 22215 | 0.602604
14 | 122467794 | 21552 | 0.58462
15 | 95056310 | 24939 | 0.676495
16 | 108660126 | 23283 | 0.631575
17 | 107432336 | 23367 | 0.633853
18 | 105541316 | 23645 | 0.641394
19 | 128727356 | 20770 | 0.563407
20 | 207726426 | 11091 | 0.300854
21 | 207383302 | 11136 | 0.302075
22 | 207436632 | 11133 | 0.301994
23 | 207329454 | 11145 | 0.302319
24 | 207306486 | 11147 | 0.302374
25 | 207244214 | 11153 | 0.302536
26 | 207036648 | 11182 | 0.303323
27 | 206888618 | 11202 | 0.303865
28 | 206837530 | 11210 | 0.304082
29 | 206833744 | 11207 | 0.304001
30 | 206807412 | 11212 | 0.304137
31 | 206799744 | 11214 | 0.304191
32 | 206794200 | 11215 | 0.304218
33 | 206791188 | 11215 | 0.304218
34 | 206791430 | 11216 | 0.304245
35 | 206780914 | 11218 | 0.304299
36 | 206776364 | 11219 | 0.304327
37 | 206780202 | 11218 | 0.304299
38 | 206773420 | 11219 | 0.304327
39 | 206769160 | 11219 | 0.304327
40 | 206774618 | 11218 | 0.304299
41 | 206773978 | 11218 | 0.304299
42 | 206767250 | 11218 | 0.304299
43 | 206765982 | 11219 | 0.304327
44 | 206763096 | 11221 | 0.304381
45 | 206772774 | 11218 | 0.304299
46 | 206764236 | 11218 | 0.304299
47 | 206757012 | 11222 | 0.304408
48 | 206768756 | 11219 | 0.304327
49 | 206758468 | 11221 | 0.304381
50 | 206767812 | 11220 | 0.304354
51 | 206759692 | 11221 | 0.304381
52 | 206759412 | 11221 | 0.304381
53 | 206759148 | 11221 | 0.304381
54 | 206758986 | 11222 | 0.304408
55 | 206758768 | 11222 | 0.304408
56 | 206766904 | 11221 | 0.304381
57 | 206767038 | 11221 | 0.304381
58 | 206766940 | 11221 | 0.304381
59 | 206758994 | 11222 | 0.304408
60 | 206766914 | 11221 | 0.304381
61 | 206766914 | 11221 | 0.304381
62 | 206766914 | 11221 | 0.304381
63 | 206758850 | 11222 | 0.304408
64 | 206766914 | 11221 | 0.304381
65 | 206758850 | 11222 | 0.304408
66 | 206758850 | 11222 | 0.304408
67 | 206766914 | 11221 | 0.304381
68 | 206758850 | 11222 | 0.304408
69 | 206758850 | 11222 | 0.304408
70 | 206758850 | 11222 | 0.304408
71 | 206758850 | 11222 | 0.304408
72 | 206758850 | 11222 | 0.304408
73 | 206758850 | 11222 | 0.304408
74 | 206758850 | 11222 | 0.304408
75 | 206758850 | 11222 | 0.304408
76 | 206758850 | 11222 | 0.304408
77 | 206758850 | 11222 | 0.304408
78 | 206758850 | 11222 | 0.304408
79 | 206758850 | 11222 | 0.304408
80 | 206766914 | 11221 | 0.304381
81 | 206766914 | 11221 | 0.304381
82 | 206758850 | 11222 | 0.304408
83 | 206758850 | 11222 | 0.304408
84 | 206758850 | 11222 | 0.304408
85 | 206758850 | 11222 | 0.304408
86 | 206766914 | 11221 | 0.304381
87 | 206758850 | 11222 | 0.304408
88 | 206758850 | 11222 | 0.304408
89 | 206758850 | 11222 | 0.304408
90 | 206758850 | 11222 | 0.304408
91 | 206766914 | 11221 | 0.304381
92 | 206758850 | 11222 | 0.304408
93 | 206758850 | 11222 | 0.304408
94 | 206766914 | 11221 | 0.304381
95 | 206766914 | 11221 | 0.304381
96 | 206758850 | 11222 | 0.304408
97 | 206766914 | 11221 | 0.304381
98 | 206766914 | 11221 | 0.304381
99 | 206758850 | 11222 | 0.304408
100 | 206758850 | 11222 | 0.304408
Epoch | NumCorrect | TestAccuracy 
1 | 6421 | 0.696648
found best test accuracy = 0.696648 at epoch 1. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
2 | 6427 | 0.697298
found best test accuracy = 0.697298 at epoch 2. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
3 | 6423 | 0.696864
4 | 6422 | 0.696756
5 | 2653 | 0.287838
6 | 2665 | 0.289140
7 | 6457 | 0.700553
found best test accuracy = 0.700553 at epoch 7. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
8 | 2686 | 0.291418
9 | 6521 | 0.707497
found best test accuracy = 0.707497 at epoch 9. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
10 | 6479 | 0.702940
11 | 2657 | 0.288272
12 | 6546 | 0.710209
found best test accuracy = 0.710209 at epoch 12. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
13 | 6507 | 0.705978
14 | 6490 | 0.704134
15 | 2669 | 0.289574
16 | 6563 | 0.712054
found best test accuracy = 0.712054 at epoch 16. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
17 | 2562 | 0.277965
18 | 6666 | 0.723229
found best test accuracy = 0.723229 at epoch 18. save weights to weights/hypnogram/fc1_weight_100epochs_bz4_clamp1024.csv
19 | 2676 | 0.290333
20 | 2778 | 0.301400
21 | 2778 | 0.301400
22 | 2780 | 0.301617
23 | 2782 | 0.301834
24 | 2780 | 0.301617
25 | 2785 | 0.302159
26 | 2793 | 0.303027
27 | 2794 | 0.303136
28 | 2795 | 0.303244
29 | 2795 | 0.303244
30 | 2795 | 0.303244
31 | 2795 | 0.303244
32 | 2795 | 0.303244
33 | 2795 | 0.303244
34 | 2795 | 0.303244
35 | 2795 | 0.303244
36 | 2795 | 0.303244
37 | 2795 | 0.303244
38 | 2795 | 0.303244
39 | 2795 | 0.303244
40 | 2795 | 0.303244
41 | 2795 | 0.303244
42 | 2795 | 0.303244
43 | 2795 | 0.303244
44 | 2795 | 0.303244
45 | 2795 | 0.303244
46 | 2795 | 0.303244
47 | 2795 | 0.303244
48 | 2795 | 0.303244
49 | 2795 | 0.303244
50 | 2795 | 0.303244
51 | 2795 | 0.303244
52 | 2795 | 0.303244
53 | 2795 | 0.303244
54 | 2795 | 0.303244
55 | 2795 | 0.303244
56 | 2795 | 0.303244
57 | 2795 | 0.303244
58 | 2795 | 0.303244
59 | 2795 | 0.303244
60 | 2795 | 0.303244
61 | 2795 | 0.303244
62 | 2795 | 0.303244
63 | 2795 | 0.303244
64 | 2795 | 0.303244
65 | 2795 | 0.303244
66 | 2795 | 0.303244
67 | 2795 | 0.303244
68 | 2795 | 0.303244
69 | 2795 | 0.303244
70 | 2795 | 0.303244
71 | 2795 | 0.303244
72 | 2795 | 0.303244
73 | 2795 | 0.303244
74 | 2795 | 0.303244
75 | 2795 | 0.303244
76 | 2795 | 0.303244
77 | 2795 | 0.303244
78 | 2795 | 0.303244
79 | 2795 | 0.303244
80 | 2795 | 0.303244
81 | 2795 | 0.303244
82 | 2795 | 0.303244
83 | 2795 | 0.303244
84 | 2795 | 0.303244
85 | 2795 | 0.303244
86 | 2795 | 0.303244
87 | 2795 | 0.303244
88 | 2795 | 0.303244
89 | 2795 | 0.303244
90 | 2795 | 0.303244
91 | 2795 | 0.303244
92 | 2795 | 0.303244
93 | 2795 | 0.303244
94 | 2795 | 0.303244
95 | 2795 | 0.303244
96 | 2795 | 0.303244
97 | 2795 | 0.303244
98 | 2795 | 0.303244
99 | 2795 | 0.303244
100 | 2795 | 0.303244
----- Results -----
best train accuracy = 0.677662 at epoch 1
best test accuracy = 0.723229 at epoch 18
trained weight shape: (300, 1)
trained weight average = 180; trained weight max value = 1024; trained weight min value = -840
trained bias shape: (1, 1)
trained bias = -105