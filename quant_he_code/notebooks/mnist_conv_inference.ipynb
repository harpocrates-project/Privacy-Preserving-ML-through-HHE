{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/quant_he_code/weights/quant_hcnn_2bits_mnist_plain_4bits_weights.pth')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "weight_dir = project_path/'quant_he_code/weights/'\n",
    "\n",
    "input_bit_width = 2\n",
    "weight_bit_width = 4\n",
    "weight_file = f\"quant_hcnn_{input_bit_width}bits_mnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "weight_file_path = weight_dir / weight_file\n",
    "weight_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return test_dataset\n",
    "\n",
    "test_dataset = mnist_processing(option=input_bit_width)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa6dfb19970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXlElEQVR4nO3df2hV9/3H8dfV6q2VmwvBJvfemYZQlI1GhKpTg9UoGLww+dpsxrYwkn+kXaMQ0uLm/MOwP0wnKPsjq2NluMp01YB1gjKbERNbXEYqKRVXJMU4M8wlM7T3xtTdYP18/whedk0aTXJv3vfH8wEHvOee63l7evDZ4733xOOccwIAwMAc6wEAAPmLCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNPWQ/wqAcPHuj27dvy+XzyeDzW4wAApsg5p+HhYYVCIc2ZM/m1TsZF6Pbt2yopKbEeAwAwQ/39/Vq8ePGk22TcP8f5fD7rEQAAKfAkf5+nLULvvvuuysrK9PTTT2vFihX6+OOPn+h1/BMcAOSGJ/n7PC0ROnnypBoaGrRv3z719PTopZdeUjgc1q1bt9KxOwBAlvKk4y7aq1ev1osvvqgjR44k1v3gBz/Qtm3b1NzcPOlrY7GY/H5/qkcCAMyyaDSqgoKCSbdJ+ZXQ6Oiorly5oqqqqqT1VVVVunz58rjt4/G4YrFY0gIAyA8pj9CdO3f07bffqri4OGl9cXGxIpHIuO2bm5vl9/sTC5+MA4D8kbYPJjz6hpRzbsI3qfbu3atoNJpY+vv70zUSACDDpPx7QosWLdLcuXPHXfUMDg6OuzqSJK/XK6/Xm+oxAABZIOVXQvPnz9eKFSvU1taWtL6trU0VFRWp3h0AIIul5Y4JjY2N+ulPf6qVK1dq7dq1+v3vf69bt27pjTfeSMfuAABZKi0R2rFjh4aGhvSrX/1KAwMDKi8v1/nz51VaWpqO3QEAslRavic0E3xPCAByg8n3hAAAeFJECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMBMyiPU1NQkj8eTtAQCgVTvBgCQA55Kx2/6wgsv6G9/+1vi8dy5c9OxGwBAlktLhJ566imufgAAj5WW94R6e3sVCoVUVlamV155RTdu3PjObePxuGKxWNICAMgPKY/Q6tWrdezYMV24cEHvvfeeIpGIKioqNDQ0NOH2zc3N8vv9iaWkpCTVIwEAMpTHOefSuYORkRE9//zz2rNnjxobG8c9H4/HFY/HE49jsRghAoAcEI1GVVBQMOk2aXlP6H8tXLhQy5YtU29v74TPe71eeb3edI8BAMhAaf+eUDwe1xdffKFgMJjuXQEAskzKI/T222+rs7NTfX19+sc//qGf/OQnisViqq2tTfWuAABZLuX/HPfvf/9br776qu7cuaNnn31Wa9asUVdXl0pLS1O9KwBAlkv7BxOmKhaLye/3W48BAJihJ/lgAveOAwCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMpP2H2iHznTp1alqva21tTfEktvsBMPu4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZj3POWQ/xv2KxmPx+v/UYWWu6d8QGcllNTY31CHkpGo2qoKBg0m24EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADU2j79u3Tel1ra+us7StT94PsMJ1zdSavwxhuYAoAyGhECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBluYArAzKlTp2ZlP9zA1AY3MAUAZDQiBAAwM+UIXbp0SVu3blUoFJLH49GZM2eSnnfOqampSaFQSAsWLFBlZaWuXbuWqnkBADlkyhEaGRnR8uXL1dLSMuHzBw8e1OHDh9XS0qLu7m4FAgFt3rxZw8PDMx4WAJBbnprqC8LhsMLh8ITPOef0m9/8Rvv27VN1dbUk6f3331dxcbFOnDih119/fWbTAgBySkrfE+rr61MkElFVVVVindfr1YYNG3T58uUJXxOPxxWLxZIWAEB+SGmEIpGIJKm4uDhpfXFxceK5RzU3N8vv9yeWkpKSVI4EAMhgafl0nMfjSXrsnBu37qG9e/cqGo0mlv7+/nSMBADIQFN+T2gygUBA0tgVUTAYTKwfHBwcd3X0kNfrldfrTeUYAIAskdIrobKyMgUCAbW1tSXWjY6OqrOzUxUVFancFQAgB0z5Suju3bv68ssvE4/7+vr02WefqbCwUM8995waGhp04MABLVmyREuWLNGBAwf0zDPP6LXXXkvp4ACA7DflCH366afauHFj4nFjY6Mkqba2Vn/84x+1Z88e3bt3T2+++aa++uorrV69Wh999JF8Pl/qpgYA5ARuYArAzGzdwLSmpmZW9oNk3MAUAJDRiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYCalP1kVQP6azh2xt2/fPuXXeDyeKb8GmYsrIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADDcwBTDOdG4sOh2tra2zsh9kLq6EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAUwDizdQPTmpqaWdkPMhdXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5gCuSw2boRKTBdXAkBAMwQIQCAmSlH6NKlS9q6datCoZA8Ho/OnDmT9HxdXZ08Hk/SsmbNmlTNCwDIIVOO0MjIiJYvX66Wlpbv3GbLli0aGBhILOfPn5/RkACA3DTlDyaEw2GFw+FJt/F6vQoEAtMeCgCQH9LynlBHR4eKioq0dOlS7dy5U4ODg9+5bTweVywWS1oAAPkh5REKh8M6fvy42tvbdejQIXV3d2vTpk2Kx+MTbt/c3Cy/359YSkpKUj0SACBDpfx7Qjt27Ej8ury8XCtXrlRpaanOnTun6urqcdvv3btXjY2NicexWIwQAUCeSPuXVYPBoEpLS9Xb2zvh816vV16vN91jAAAyUNq/JzQ0NKT+/n4Fg8F07woAkGWmfCV09+5dffnll4nHfX19+uyzz1RYWKjCwkI1NTXpxz/+sYLBoG7evKlf/vKXWrRokV5++eWUDg4AyH5TjtCnn36qjRs3Jh4/fD+ntrZWR44c0dWrV3Xs2DF9/fXXCgaD2rhxo06ePCmfz5e6qQEAOcHjnHPWQ/yvWCwmv99vPQaQE06dOjVr+6qpqZm1fSE7RKNRFRQUTLoN944DAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmbT/ZFUAqbF9+3brEYCU40oIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDDDUyBLDGbNzCtqamZtX0hv3ElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamgIFTp07Nyn5aW1tnZT/AdHElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamwAzN1s1Ip4MbmCLTcSUEADBDhAAAZqYUoebmZq1atUo+n09FRUXatm2brl+/nrSNc05NTU0KhUJasGCBKisrde3atZQODQDIDVOKUGdnp+rr69XV1aW2tjbdv39fVVVVGhkZSWxz8OBBHT58WC0tLeru7lYgENDmzZs1PDyc8uEBANnN45xz033xf/7zHxUVFamzs1Pr16+Xc06hUEgNDQ36+c9/LkmKx+MqLi7Wr3/9a73++uuP/T1jsZj8fv90RwJmXSZ/MKGmpsZ6BOSxaDSqgoKCSbeZ0XtC0WhUklRYWChJ6uvrUyQSUVVVVWIbr9erDRs26PLlyxP+HvF4XLFYLGkBAOSHaUfIOafGxkatW7dO5eXlkqRIJCJJKi4uTtq2uLg48dyjmpub5ff7E0tJScl0RwIAZJlpR2jXrl36/PPP9ec//3nccx6PJ+mxc27cuof27t2raDSaWPr7+6c7EgAgy0zry6q7d+/W2bNndenSJS1evDixPhAISBq7IgoGg4n1g4OD466OHvJ6vfJ6vdMZAwCQ5aZ0JeSc065du3T69Gm1t7errKws6fmysjIFAgG1tbUl1o2Ojqqzs1MVFRWpmRgAkDOmdCVUX1+vEydO6C9/+Yt8Pl/ifR6/368FCxbI4/GooaFBBw4c0JIlS7RkyRIdOHBAzzzzjF577bW0/AEAANlrShE6cuSIJKmysjJp/dGjR1VXVydJ2rNnj+7du6c333xTX331lVavXq2PPvpIPp8vJQMDAHLHjL4nlA58TwjZZra+J8R3fpBt0v49IQAAZoIIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmpvWTVYFcNVt3xG5tbZ2V/QCZjishAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzBFTtq+fbv1CJPiBqbAGK6EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAUOSnTb2AKYAxXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5giozHzUiB3MWVEADADBECAJiZUoSam5u1atUq+Xw+FRUVadu2bbp+/XrSNnV1dfJ4PEnLmjVrUjo0ACA3TClCnZ2dqq+vV1dXl9ra2nT//n1VVVVpZGQkabstW7ZoYGAgsZw/fz6lQwMAcsOUPpjw17/+Nenx0aNHVVRUpCtXrmj9+vWJ9V6vV4FAIDUTAgBy1ozeE4pGo5KkwsLCpPUdHR0qKirS0qVLtXPnTg0ODn7n7xGPxxWLxZIWAEB+mHaEnHNqbGzUunXrVF5enlgfDod1/Phxtbe369ChQ+ru7tamTZsUj8cn/H2am5vl9/sTS0lJyXRHAgBkGY9zzk3nhfX19Tp37pw++eQTLV68+Du3GxgYUGlpqT744ANVV1ePez4ejycFKhaLESIkmc73hDL9u0U1NTXWIwBpF41GVVBQMOk20/qy6u7du3X27FldunRp0gBJUjAYVGlpqXp7eyd83uv1yuv1TmcMAECWm1KEnHPavXu3PvzwQ3V0dKisrOyxrxkaGlJ/f7+CweC0hwQA5KYpvSdUX1+vP/3pTzpx4oR8Pp8ikYgikYju3bsnSbp7967efvtt/f3vf9fNmzfV0dGhrVu3atGiRXr55ZfT8gcAAGSvKV0JHTlyRJJUWVmZtP7o0aOqq6vT3LlzdfXqVR07dkxff/21gsGgNm7cqJMnT8rn86VsaABAbpjyP8dNZsGCBbpw4cKMBgIA5A/uog3MEJ90A6aPG5gCAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGam/eO90yUWi8nv91uPAQCYoSf58d5cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADCTcRHKsFvZAQCm6Un+Ps+4CA0PD1uPAABIgSf5+zzj7qL94MED3b59Wz6fTx6PJ+m5WCymkpIS9ff3P/bOrLmM4zCG4zCG4zCG4zAmE46Dc07Dw8MKhUKaM2fya52nZmmmJzZnzhwtXrx40m0KCgry+iR7iOMwhuMwhuMwhuMwxvo4POmP5Mm4f44DAOQPIgQAMJNVEfJ6vdq/f7+8Xq/1KKY4DmM4DmM4DmM4DmOy7Thk3AcTAAD5I6uuhAAAuYUIAQDMECEAgBkiBAAwk1URevfdd1VWVqann35aK1as0Mcff2w90qxqamqSx+NJWgKBgPVYaXfp0iVt3bpVoVBIHo9HZ86cSXreOaempiaFQiEtWLBAlZWVunbtms2wafS441BXVzfu/FizZo3NsGnS3NysVatWyefzqaioSNu2bdP169eTtsmH8+FJjkO2nA9ZE6GTJ0+qoaFB+/btU09Pj1566SWFw2HdunXLerRZ9cILL2hgYCCxXL161XqktBsZGdHy5cvV0tIy4fMHDx7U4cOH1dLSou7ubgUCAW3evDnn7kP4uOMgSVu2bEk6P86fPz+LE6ZfZ2en6uvr1dXVpba2Nt2/f19VVVUaGRlJbJMP58OTHAcpS84HlyV++MMfujfeeCNp3fe//333i1/8wmii2bd//363fPly6zFMSXIffvhh4vGDBw9cIBBw77zzTmLdf//7X+f3+93vfvc7gwlnx6PHwTnnamtr3f/93/+ZzGNlcHDQSXKdnZ3Oufw9Hx49Ds5lz/mQFVdCo6OjunLliqqqqpLWV1VV6fLly0ZT2ejt7VUoFFJZWZleeeUV3bhxw3okU319fYpEIknnhtfr1YYNG/Lu3JCkjo4OFRUVaenSpdq5c6cGBwetR0qraDQqSSosLJSUv+fDo8fhoWw4H7IiQnfu3NG3336r4uLipPXFxcWKRCJGU82+1atX69ixY7pw4YLee+89RSIRVVRUaGhoyHo0Mw//++f7uSFJ4XBYx48fV3t7uw4dOqTu7m5t2rRJ8XjcerS0cM6psbFR69atU3l5uaT8PB8mOg5S9pwPGXcX7ck8+qMdnHPj1uWycDic+PWyZcu0du1aPf/883r//ffV2NhoOJm9fD83JGnHjh2JX5eXl2vlypUqLS3VuXPnVF1dbThZeuzatUuff/65Pvnkk3HP5dP58F3HIVvOh6y4Elq0aJHmzp077v9kBgcHx/0fTz5ZuHChli1bpt7eXutRzDz8dCDnxnjBYFClpaU5eX7s3r1bZ8+e1cWLF5N+9Eu+nQ/fdRwmkqnnQ1ZEaP78+VqxYoXa2tqS1re1tamiosJoKnvxeFxffPGFgsGg9ShmysrKFAgEks6N0dFRdXZ25vW5IUlDQ0Pq7+/PqfPDOaddu3bp9OnTam9vV1lZWdLz+XI+PO44TCRjzwfDD0VMyQcffODmzZvn/vCHP7h//vOfrqGhwS1cuNDdvHnTerRZ89Zbb7mOjg5348YN19XV5X70ox85n8+X88dgeHjY9fT0uJ6eHifJHT582PX09Lh//etfzjnn3nnnHef3+93p06fd1atX3auvvuqCwaCLxWLGk6fWZMdheHjYvfXWW+7y5cuur6/PXbx40a1du9Z973vfy6nj8LOf/cz5/X7X0dHhBgYGEss333yT2CYfzofHHYdsOh+yJkLOOffb3/7WlZaWuvnz57sXX3wx6eOI+WDHjh0uGAy6efPmuVAo5Kqrq921a9esx0q7ixcvOknjltraWufc2Mdy9+/f7wKBgPN6vW79+vXu6tWrtkOnwWTH4ZtvvnFVVVXu2WefdfPmzXPPPfecq62tdbdu3bIeO6Um+vNLckePHk1skw/nw+OOQzadD/woBwCAmax4TwgAkJuIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADP/D6UqaCLqVwAsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale_factor_input = 1 / 3\n",
    "im = test_dataset[0][0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConvQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 conv layers + 2 square activations + 1 linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels=1, out_channels=5, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width, \n",
    "                                     return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(in_channels=5, out_channels=50, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width)\n",
    "        self.fc1 = qnn.QuantLinear(in_features=800, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = out * out  # second square\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTConvQuantModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(weight_file_path, \n",
    "    map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_loader:\n",
    "    print(x.unique())\n",
    "    print(y.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_367082/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with trained 4-bit weights = 0.9871616363525391\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "acc = evaluate(model, test_loader)['val_acc']\n",
    "print(f\"test accuracy with trained {weight_bit_width}-bit weights = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1,  2,  6,  1, -5],\n",
       "          [ 1,  0, -3,  4, -1],\n",
       "          [ 0,  2, -1,  5,  7],\n",
       "          [-1,  7,  4,  0,  2],\n",
       "          [ 4,  3, -3, -5, -4]]],\n",
       "\n",
       "\n",
       "        [[[-1,  1, -3, -1,  0],\n",
       "          [-3,  5,  4,  5,  3],\n",
       "          [-5,  4,  4,  3, -4],\n",
       "          [-6, -7, -7, -1,  1],\n",
       "          [-2,  2, -3, -5,  1]]],\n",
       "\n",
       "\n",
       "        [[[-1, -7, -6, -2,  2],\n",
       "          [ 4,  0, -5, -7,  2],\n",
       "          [ 6,  6,  2, -6, -2],\n",
       "          [ 2,  6,  4, -3, -2],\n",
       "          [-3,  2,  3, -2, -1]]],\n",
       "\n",
       "\n",
       "        [[[-2,  0, -3,  4,  3],\n",
       "          [-6, -2, -1,  7, -2],\n",
       "          [-6,  0,  4,  6, -4],\n",
       "          [-6, -1,  3,  6, -2],\n",
       "          [-5,  0, -1,  3,  2]]],\n",
       "\n",
       "\n",
       "        [[[-1,  1,  2,  1,  0],\n",
       "          [ 2,  2, -4, -3,  2],\n",
       "          [-5, -6, -5,  0,  7],\n",
       "          [-7, -3,  3,  7,  1],\n",
       "          [ 4,  6,  7,  2, -3]]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.int_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-1.,  2.,  6.,  1., -5.],\n",
       "          [ 1., -0., -3.,  4., -1.],\n",
       "          [ 0.,  2., -1.,  5.,  7.],\n",
       "          [-1.,  7.,  4., -0.,  2.],\n",
       "          [ 4.,  3., -3., -5., -4.]]],\n",
       "\n",
       "\n",
       "        [[[-1.,  1., -3., -1.,  0.],\n",
       "          [-3.,  5.,  4.,  5.,  3.],\n",
       "          [-5.,  4.,  4.,  3., -4.],\n",
       "          [-6., -7., -7., -1.,  1.],\n",
       "          [-2.,  2., -3., -5.,  1.]]],\n",
       "\n",
       "\n",
       "        [[[-1., -7., -6., -2.,  2.],\n",
       "          [ 4., -0., -5., -7.,  2.],\n",
       "          [ 6.,  6.,  2., -6., -2.],\n",
       "          [ 2.,  6.,  4., -3., -2.],\n",
       "          [-3.,  2.,  3., -2., -1.]]],\n",
       "\n",
       "\n",
       "        [[[-2.,  0., -3.,  4.,  3.],\n",
       "          [-6., -2., -1.,  7., -2.],\n",
       "          [-6.,  0.,  4.,  6., -4.],\n",
       "          [-6., -1.,  3.,  6., -2.],\n",
       "          [-5.,  0., -1.,  3.,  2.]]],\n",
       "\n",
       "\n",
       "        [[[-1.,  1.,  2.,  1., -0.],\n",
       "          [ 2.,  2., -4., -3.,  2.],\n",
       "          [-5., -6., -5., -0.,  7.],\n",
       "          [-7., -3.,  3.,  7.,  1.],\n",
       "          [ 4.,  6.,  7.,  2., -3.]]]], grad_fn=<DivBackward0>), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.quant_weight() / model.conv1.quant_weight().scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pockethhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
