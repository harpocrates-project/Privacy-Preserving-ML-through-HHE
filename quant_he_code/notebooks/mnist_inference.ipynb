{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "\n",
    "weight_bit_width = 8\n",
    "weight_path = project_path/f'quant_he_code/weights/quant_hcnn_mnist_plain_{weight_bit_width}bits.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(\n",
    "    root=mnist_path, \n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: (x*4).int(),\n",
    "        lambda x: x.float()/4,\n",
    "]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "Processed MNIST data unique values = tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec9a14aee0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale_factor_input = 1 / 3\n",
    "im = test_dataset[0][0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConvQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 conv layers + 2 square activations + 1 linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels=1, out_channels=5, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width, \n",
    "                                     return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(in_channels=5, out_channels=50, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width)\n",
    "        self.fc1 = qnn.QuantLinear(in_features=800, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = out * out  # second square\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTConvQuantModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(weight_path, \n",
    "    map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104420/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with trained 8-bit weights = 0.9861664175987244\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "acc = evaluate(model, test_loader)['val_acc']\n",
    "print(f\"test accuracy with trained {weight_bit_width}-bit weights = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 9.5367e-07, -9.5367e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -9.5367e-07,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -1.9073e-06,  0.0000e+00,  1.9073e-06,  0.0000e+00],\n",
       "          [ 0.0000e+00,  9.5367e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-1.9073e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 9.5367e-07,  1.9073e-06,  0.0000e+00,  0.0000e+00, -9.5367e-07],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6294e-06,  0.0000e+00],\n",
       "          [ 0.0000e+00, -1.9073e-06,  0.0000e+00,  0.0000e+00,  9.5367e-07],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  3.8147e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -1.9073e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 9.5367e-07,  0.0000e+00,  0.0000e+00, -9.5367e-07, -9.5367e-07],\n",
       "          [ 0.0000e+00,  0.0000e+00,  9.5367e-07,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]],\n",
       "       grad_fn=<AddBackward0>), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.quant_weight() / model.conv1.quant_weight().scale - model.conv1.int_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  11,    9,   41,   21,  -17],\n",
       "          [  -6,    4,    4,   11,    2],\n",
       "          [ -68,   63,   13,    7,  -20],\n",
       "          [ -16,  127,    6,  -80,   -3],\n",
       "          [  41,   77,  -43,  -94,   11]]],\n",
       "\n",
       "\n",
       "        [[[  32,  -70,  -14,   -3,   12],\n",
       "          [   5,  -22,   75,   22,   -8],\n",
       "          [ -43,  -13,  -10,  -85,  -82],\n",
       "          [ -30,  -58,  -78,   24,   25],\n",
       "          [  -3,   47,  -12,   15,   68]]],\n",
       "\n",
       "\n",
       "        [[[  -9,  -18,  -21,    0,  -11],\n",
       "          [ -12,  -46,  -66,  -88,   53],\n",
       "          [  38,   18,    1, -108,  -13],\n",
       "          [  43,   96,   69,  -31,  -35],\n",
       "          [  -3,   10,   24,    0,   -9]]],\n",
       "\n",
       "\n",
       "        [[[ -16,   49,  -27,   32,  -31],\n",
       "          [ -95,   20,   59,   47,   32],\n",
       "          [ -46,    7,   57,   17,   29],\n",
       "          [ -85,  -52,  -12,  -19,  -20],\n",
       "          [ -33,  -22,  -22,  -24,  -14]]],\n",
       "\n",
       "\n",
       "        [[[  11,   33,    8,  -15,  -15],\n",
       "          [   4,  -46,   -9,   -7,    5],\n",
       "          [ -61,  -89,   -6,   27,   46],\n",
       "          [ -38,   10,   92,   64,   -6],\n",
       "          [  25,   80,   53,  -25,  -27]]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.int_weight()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pockethhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
