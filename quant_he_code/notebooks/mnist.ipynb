{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/mnist')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bit_width = 4\n",
    "save_weight_path = project_path/f\"quant_he_code/weights/quant_hcnn_4bits_mnist_plain_{weight_bit_width}bits.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize into [0, 0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(\n",
    "    root=mnist_path, \n",
    "    download=False, \n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: (x*4).int(),\n",
    "        lambda x: x.float()/4,\n",
    "]))\n",
    "test_dataset = MNIST(\n",
    "    root=mnist_path, \n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: (x*4).int(),\n",
    "        lambda x: x.float()/4,\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize into [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(\n",
    "    root=mnist_path, \n",
    "    download=False, \n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: (x * 3).int().float()\n",
    "]))\n",
    "test_dataset = MNIST(\n",
    "    root=mnist_path, \n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: (x * 3).int().float()\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f50446de040>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_dataset[0][0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n",
      "tensor([0., 1., 2., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the CNN model (2 conv layers + 1 linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConvQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 conv layers + 2 square activations + 1 linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels=1, out_channels=5, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width, \n",
    "                                     return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(in_channels=5, out_channels=50, kernel_size=5, \n",
    "                                     stride=(2, 2), padding=0, bias=False, \n",
    "                                     weight_bit_width=weight_bit_width)\n",
    "        self.fc1 = qnn.QuantLinear(in_features=800, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = out * out  # second square\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT 2-FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 2 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        # out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTConvQuantModel(\n",
       "  (conv1): QuantConv2d(\n",
       "    1, 5, kernel_size=(5, 5), stride=(2, 2), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (conv2): QuantConv2d(\n",
       "    5, 50, kernel_size=(5, 5), stride=(2, 2), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=800, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(MNISTConvQuantModel(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_name, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.98\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_name)\n",
    "                print(\"Saved\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108533/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.1562, val_acc: 0.9585\n",
      "Epoch [2], val_loss: 0.1007, val_acc: 0.9717\n",
      "Epoch [3], val_loss: 0.0980, val_acc: 0.9719\n",
      "2\t{'val_loss': 0.09899615496397018, 'val_acc': 0.9766122698783875}\n",
      "Epoch [4], val_loss: 0.0801, val_acc: 0.9777\n",
      "3\t{'val_loss': 0.07495930045843124, 'val_acc': 0.9809912443161011}\n",
      "Saved\n",
      "Epoch [5], val_loss: 0.0799, val_acc: 0.9784\n",
      "4\t{'val_loss': 0.07751865684986115, 'val_acc': 0.9795979261398315}\n",
      "Epoch [6], val_loss: 0.0828, val_acc: 0.9788\n",
      "5\t{'val_loss': 0.07836957275867462, 'val_acc': 0.9811902642250061}\n",
      "Saved\n",
      "Epoch [7], val_loss: 0.1264, val_acc: 0.9689\n",
      "6\t{'val_loss': 0.13192443549633026, 'val_acc': 0.9696456789970398}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m [evaluate(model, val_loader)]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m fit(epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                train_loader\u001b[39m=\u001b[39;49mtrain_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                val_loader\u001b[39m=\u001b[39;49mval_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                test_loader\u001b[39m=\u001b[39;49mtest_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                file_name\u001b[39m=\u001b[39;49msave_weight_path, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                opt_func\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam)\n",
      "\u001b[1;32m/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m high_acc \u001b[39m=\u001b[39m \u001b[39m0.98\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Training Phase\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining_step(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dk/Desktop/projects/PocketHHE/quant_he_code/notebooks/mnist.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39myield\u001b[39;00m to_device(b, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/utils/data/dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 679\u001b[0m     data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mpin_memory\u001b[39m.\u001b[39;49mpin_memory(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pin_memory_device)\n\u001b[1;32m    680\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:70\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m     69\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data])  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m     69\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data])  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/miniconda3/envs/pockethhe/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:55\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpin_memory\u001b[39m(data, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 55\u001b[0m         \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mpin_memory(device)\n\u001b[1;32m     56\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[1;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=150, lr=0.001, model=model, \n",
    "               train_loader=train_loader, \n",
    "               val_loader=val_loader, \n",
    "               test_loader=test_loader, \n",
    "               file_name=save_weight_path, \n",
    "               opt_func=torch.optim.Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93415/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 0.2583501935005188, 'val_acc': 0.9856687784194946}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pockethhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
