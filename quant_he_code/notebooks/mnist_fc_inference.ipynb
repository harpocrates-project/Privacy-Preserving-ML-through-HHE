{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "weight_dir = project_path/'quant_he_code/weights/'\n",
    "\n",
    "input_bit_width = 2\n",
    "weight_bit_width = 4\n",
    "weight_file = f\"quant_fc_{input_bit_width}bits_mnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "weight_file_path = weight_dir / weight_file\n",
    "assert weight_file_path.exists()\n",
    "weight_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return test_dataset\n",
    "\n",
    "test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im.shape = torch.Size([784])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f46dc35d400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale_factor_input = 1 / 3\n",
    "im = test_dataset[0][0]\n",
    "print(f\"{im.shape = }\")\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)  # outputs has dim [batch_size, 10]\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 2 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):      \n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = MNISTLinearQuantModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(weight_file_path, \n",
    "    map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=10000, pin_memory=True)\n",
    "for x, y in test_loader:\n",
    "    print(x.unique())\n",
    "    print(y.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with trained 4-bit weights = 0.9776999950408936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_370622/2498532057.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)  # outputs has dim [batch_size, 10]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "acc = evaluate(model, test_loader)['val_acc']\n",
    "print(f\"test accuracy with trained {weight_bit_width}-bit weights = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
       "       dtype=torch.int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.int_weight().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[0., 0., -0.,  ..., -0., 0., 0.],\n",
       "        [-0., -0., -0.,  ..., -0., -0., 0.],\n",
       "        [-0., 0., 0.,  ..., 0., -0., -0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., -0., 0., -0.],\n",
       "        [0., -0., -0.,  ..., 0., 0., 0.],\n",
       "        [-0., 0., 0.,  ..., -0., 0., -0.]], grad_fn=<DivBackward0>), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.quant_weight() / model.fc1.quant_weight().scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference on integer data and integer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "--x.dtype = torch.int8, \n",
      "--x.shape = torch.Size([10000, 784]), \n",
      "--x.unique() = tensor([0, 1, 2, 3], dtype=torch.int8)\n",
      "labels: \n",
      "--y.dtype = torch.int64, \n",
      "--y.shape = torch.Size([10000]), \n",
      "--y.unique() = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x = x.type(torch.int8)\n",
    "print(f\"input: \\n--{x.dtype = }, \\n--{x.shape = }, \\n--{x.unique() = }\")\n",
    "print(f\"labels: \\n--{y.dtype = }, \\n--{y.shape = }, \\n--{y.unique() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0].reshape(28,28), cmap='gray')\n",
    "print(f\"class = {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual quantize function from Wouter's code. Needs more careful look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_input = 1 / 10\n",
    "\n",
    "def quantise(element, s, b):\n",
    "    upper = 2 ** (b - 1) - 1\n",
    "    lower = -2 ** (b - 1)\n",
    "\n",
    "    value = int(round(element / s))\n",
    "\n",
    "    if value > upper:\n",
    "        return upper\n",
    "    elif value < lower:\n",
    "        return lower\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "np_quantise = np.vectorize(quantise)\n",
    "\n",
    "float_fc1 = np.array(model.fc1.weight.data.tolist())\n",
    "float_fc2 = np.array(model.fc2.weight.data.tolist())\n",
    "fc1_ind_scale = model.fc1.quant_weight().scale.data.tolist()\n",
    "fc2_ind_scale = model.fc2.quant_weight().scale.data.tolist()\n",
    "\n",
    "int_fc1 = np_quantise(float_fc1, fc1_ind_scale, weight_bit_width)\n",
    "int_fc2 = np_quantise(float_fc2, fc2_ind_scale, weight_bit_width)\n",
    "\n",
    "assert torch.all(torch.tensor(int_fc2) == model.fc2.int_weight())\n",
    "assert torch.all(torch.tensor(int_fc1) == model.fc1.int_weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_fc1 = model.fc1.int_weight()\n",
    "int_fc2 = model.fc2.int_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1: \n",
      "--int_fc1.dtype = torch.int8, \n",
      "--int_fc1.shape = torch.Size([128, 784]), \n",
      "--np.unique(int_fc1) = array([-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
      "      dtype=int8)\n",
      "fc2: \n",
      "--int_fc2.dtype = torch.int8, \n",
      "--int_fc2.shape = torch.Size([10, 128]), \n",
      "--np.unique(int_fc2) = array([-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
      "      dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"fc1: \\n--{int_fc1.dtype = }, \\n--{int_fc1.shape = }, \\n--{np.unique(int_fc1) = }\")\n",
    "print(f\"fc2: \\n--{int_fc2.dtype = }, \\n--{int_fc2.shape = }, \\n--{np.unique(int_fc2) = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Integer Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 89.91%\n"
     ]
    }
   ],
   "source": [
    "corrects = 0\n",
    "for im, label in zip(x, y):\n",
    "    out = torch.matmul(model.fc1.int_weight(), im)\n",
    "    out = out.type(torch.int32)\n",
    "    out = out * out\n",
    "    # print(model.fc2.int_weight())\n",
    "    out = torch.matmul(model.fc2.int_weight().type(torch.int32), out)\n",
    "    pred = torch.argmax(out)\n",
    "    if pred == label:\n",
    "        corrects += 1\n",
    "    # break\n",
    "\n",
    "print(f\"acc = {corrects / x.shape[0] * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pockethhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
