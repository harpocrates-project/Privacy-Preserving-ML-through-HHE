{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/mnist')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_bit_width = 2\n",
    "input_bit_width = 2  # quantize into [0,1,2,3]\n",
    "save_weight_path = project_path/f\"quant_he_code/weights/quant_fc_{input_bit_width}bits_mnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "save_weight_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:  \n",
    "\n",
    "0. Quantize into [0, 0.25, 0.5, 0.75, 1]\n",
    "1. Quantize into [0, 1]\n",
    "2. Quantize into [0, 1, 2, 3]\n",
    "3. Quantize into [0, 1, 2, 3, ..., 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    train_dataset = MNIST(root=mnist_path, download=False, transform=transform)\n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "torch.Size([784])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33041784c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"train_dataset shape: {train_dataset.data.shape}\")\n",
    "print(f\"test_dataset shape: {test_dataset.data.shape}\")\n",
    "\n",
    "im = train_dataset[0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, torch.device('cpu'))\n",
    "val_loader = DeviceDataLoader(val_loader, torch.device('cpu'))\n",
    "test_loader = DeviceDataLoader(test_loader, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the FC Model (2 linear layers, 1 square activation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT 2-FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 1 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        # out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = to_device(MNISTLinearQuantModel(), torch.device('cpu'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_path, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.95\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(f\"Saved into {file_path.relative_to(project_path)}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224648/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.3852, val_acc: 0.9490\n",
      "Epoch [2], val_loss: 0.3839, val_acc: 0.9498\n",
      "Epoch [3], val_loss: 0.4938, val_acc: 0.9341\n",
      "2\t{'val_loss': 0.4407382309436798, 'val_acc': 0.9413813948631287}\n",
      "Epoch [4], val_loss: 0.5156, val_acc: 0.9424\n",
      "3\t{'val_loss': 0.44915372133255005, 'val_acc': 0.9514331221580505}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [5], val_loss: 0.5198, val_acc: 0.9355\n",
      "4\t{'val_loss': 0.46876439452171326, 'val_acc': 0.941082775592804}\n",
      "Epoch [6], val_loss: 0.5358, val_acc: 0.9426\n",
      "5\t{'val_loss': 0.5639872550964355, 'val_acc': 0.9413813948631287}\n",
      "Epoch [7], val_loss: 0.7932, val_acc: 0.9258\n",
      "6\t{'val_loss': 0.7326686382293701, 'val_acc': 0.9317277073860168}\n",
      "Epoch [8], val_loss: 0.4566, val_acc: 0.9519\n",
      "7\t{'val_loss': 1.1381418704986572, 'val_acc': 0.9445660710334778}\n",
      "Epoch [9], val_loss: 0.5982, val_acc: 0.9456\n",
      "8\t{'val_loss': 0.6856125593185425, 'val_acc': 0.9426751732826233}\n",
      "Epoch [10], val_loss: 0.6385, val_acc: 0.9304\n",
      "9\t{'val_loss': 0.5777278542518616, 'val_acc': 0.9386942386627197}\n",
      "Epoch [11], val_loss: 0.5579, val_acc: 0.9571\n",
      "10\t{'val_loss': 0.523302435874939, 'val_acc': 0.9587977528572083}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [12], val_loss: 0.5441, val_acc: 0.9600\n",
      "11\t{'val_loss': 0.5554792284965515, 'val_acc': 0.9601910710334778}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [13], val_loss: 0.7312, val_acc: 0.9511\n",
      "12\t{'val_loss': 0.7485927939414978, 'val_acc': 0.9521297812461853}\n",
      "Epoch [14], val_loss: 2.9598, val_acc: 0.9331\n",
      "13\t{'val_loss': 0.6134487390518188, 'val_acc': 0.9558120965957642}\n",
      "Epoch [15], val_loss: 0.6186, val_acc: 0.9529\n",
      "14\t{'val_loss': 0.6852606534957886, 'val_acc': 0.9527269005775452}\n",
      "Epoch [16], val_loss: 0.8064, val_acc: 0.9490\n",
      "15\t{'val_loss': 0.7528340220451355, 'val_acc': 0.9518312215805054}\n",
      "Epoch [17], val_loss: 0.7748, val_acc: 0.9593\n",
      "16\t{'val_loss': 0.6996086835861206, 'val_acc': 0.9607881903648376}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [18], val_loss: 1.2007, val_acc: 0.9341\n",
      "17\t{'val_loss': 1.0438334941864014, 'val_acc': 0.9369028806686401}\n",
      "Epoch [19], val_loss: 1.1150, val_acc: 0.9389\n",
      "18\t{'val_loss': 1.0309802293777466, 'val_acc': 0.9429737329483032}\n",
      "Epoch [20], val_loss: 1.0890, val_acc: 0.9464\n",
      "19\t{'val_loss': 0.9970120191574097, 'val_acc': 0.9528264403343201}\n",
      "Epoch [21], val_loss: 0.9584, val_acc: 0.9569\n",
      "20\t{'val_loss': 0.9844459295272827, 'val_acc': 0.9575039744377136}\n",
      "Epoch [22], val_loss: 0.9562, val_acc: 0.9606\n",
      "21\t{'val_loss': 0.8920127749443054, 'val_acc': 0.9612858295440674}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [23], val_loss: 1.1045, val_acc: 0.9521\n",
      "22\t{'val_loss': 1.0551402568817139, 'val_acc': 0.956011176109314}\n",
      "Epoch [24], val_loss: 1.0103, val_acc: 0.9569\n",
      "23\t{'val_loss': 0.9409986138343811, 'val_acc': 0.9584991931915283}\n",
      "Epoch [25], val_loss: 1.5667, val_acc: 0.9403\n",
      "24\t{'val_loss': 1.5508928298950195, 'val_acc': 0.9411823153495789}\n",
      "Epoch [26], val_loss: 1.5240, val_acc: 0.9509\n",
      "25\t{'val_loss': 1.329878330230713, 'val_acc': 0.9545183181762695}\n",
      "Epoch [27], val_loss: 1.4843, val_acc: 0.9511\n",
      "26\t{'val_loss': 1.5907503366470337, 'val_acc': 0.9481489062309265}\n",
      "Epoch [28], val_loss: 1.4638, val_acc: 0.9575\n",
      "27\t{'val_loss': 1.3238738775253296, 'val_acc': 0.9606887102127075}\n",
      "Epoch [29], val_loss: 2.1221, val_acc: 0.9438\n",
      "28\t{'val_loss': 2.1917872428894043, 'val_acc': 0.944167971611023}\n",
      "Epoch [30], val_loss: 1.6336, val_acc: 0.9515\n",
      "29\t{'val_loss': 1.7407265901565552, 'val_acc': 0.9517316818237305}\n",
      "Epoch [31], val_loss: 1.8785, val_acc: 0.9541\n",
      "30\t{'val_loss': 1.5551371574401855, 'val_acc': 0.959494411945343}\n",
      "Epoch [32], val_loss: 2.0310, val_acc: 0.9492\n",
      "31\t{'val_loss': 1.7481257915496826, 'val_acc': 0.9555135369300842}\n",
      "Epoch [33], val_loss: 1.8823, val_acc: 0.9595\n",
      "32\t{'val_loss': 1.5439037084579468, 'val_acc': 0.9606887102127075}\n",
      "Epoch [34], val_loss: 2.1642, val_acc: 0.9494\n",
      "33\t{'val_loss': 2.0394155979156494, 'val_acc': 0.9512341022491455}\n",
      "Epoch [35], val_loss: 1.8892, val_acc: 0.9577\n",
      "34\t{'val_loss': 2.0602011680603027, 'val_acc': 0.9579020738601685}\n",
      "Epoch [36], val_loss: 2.1719, val_acc: 0.9561\n",
      "35\t{'val_loss': 2.200953483581543, 'val_acc': 0.9568073153495789}\n",
      "Epoch [37], val_loss: 2.2451, val_acc: 0.9589\n",
      "36\t{'val_loss': 2.0002527236938477, 'val_acc': 0.9600915312767029}\n",
      "Epoch [38], val_loss: 2.4633, val_acc: 0.9608\n",
      "37\t{'val_loss': 2.4843218326568604, 'val_acc': 0.9583996534347534}\n",
      "Epoch [39], val_loss: 2.1479, val_acc: 0.9604\n",
      "38\t{'val_loss': 2.037051200866699, 'val_acc': 0.9584991931915283}\n",
      "Epoch [40], val_loss: 2.4497, val_acc: 0.9521\n",
      "39\t{'val_loss': 2.2788522243499756, 'val_acc': 0.9577030539512634}\n",
      "Epoch [41], val_loss: 3.0890, val_acc: 0.9446\n",
      "40\t{'val_loss': 3.1428544521331787, 'val_acc': 0.9466560482978821}\n",
      "Epoch [42], val_loss: 2.4716, val_acc: 0.9620\n",
      "41\t{'val_loss': 2.261659622192383, 'val_acc': 0.9612858295440674}\n",
      "Epoch [43], val_loss: 2.8101, val_acc: 0.9555\n",
      "42\t{'val_loss': 2.7635104656219482, 'val_acc': 0.956707775592804}\n",
      "Epoch [44], val_loss: 3.3222, val_acc: 0.9415\n",
      "43\t{'val_loss': 3.097454786300659, 'val_acc': 0.9450637102127075}\n",
      "Epoch [45], val_loss: 3.4598, val_acc: 0.9549\n",
      "44\t{'val_loss': 3.503552198410034, 'val_acc': 0.9575039744377136}\n",
      "Epoch [46], val_loss: 3.4761, val_acc: 0.9508\n",
      "45\t{'val_loss': 3.2364649772644043, 'val_acc': 0.9519307613372803}\n",
      "Epoch [47], val_loss: 3.6828, val_acc: 0.9509\n",
      "46\t{'val_loss': 3.0922741889953613, 'val_acc': 0.9568073153495789}\n",
      "Epoch [48], val_loss: 3.7043, val_acc: 0.9563\n",
      "47\t{'val_loss': 3.2649664878845215, 'val_acc': 0.9615843892097473}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [49], val_loss: 3.0656, val_acc: 0.9595\n",
      "48\t{'val_loss': 3.0790326595306396, 'val_acc': 0.9630772471427917}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [50], val_loss: 4.5949, val_acc: 0.9409\n",
      "49\t{'val_loss': 4.513370037078857, 'val_acc': 0.9481489062309265}\n",
      "Epoch [51], val_loss: 3.5051, val_acc: 0.9608\n",
      "50\t{'val_loss': 3.3666622638702393, 'val_acc': 0.9634753465652466}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [52], val_loss: 3.2006, val_acc: 0.9630\n",
      "51\t{'val_loss': 3.613415002822876, 'val_acc': 0.9602906107902527}\n",
      "Epoch [53], val_loss: 3.4681, val_acc: 0.9573\n",
      "52\t{'val_loss': 3.6055266857147217, 'val_acc': 0.9579020738601685}\n",
      "Epoch [54], val_loss: 3.7910, val_acc: 0.9597\n",
      "53\t{'val_loss': 4.03801155090332, 'val_acc': 0.9570063948631287}\n",
      "Epoch [55], val_loss: 3.9896, val_acc: 0.9587\n",
      "54\t{'val_loss': 3.9038054943084717, 'val_acc': 0.9552149772644043}\n",
      "Epoch [56], val_loss: 4.3394, val_acc: 0.9587\n",
      "55\t{'val_loss': 4.2656025886535645, 'val_acc': 0.9603901505470276}\n",
      "Epoch [57], val_loss: 5.5762, val_acc: 0.9464\n",
      "56\t{'val_loss': 6.258067607879639, 'val_acc': 0.9447651505470276}\n",
      "Epoch [58], val_loss: 4.3343, val_acc: 0.9654\n",
      "57\t{'val_loss': 4.182075023651123, 'val_acc': 0.959792971611023}\n",
      "Epoch [59], val_loss: 4.2053, val_acc: 0.9620\n",
      "58\t{'val_loss': 4.079603672027588, 'val_acc': 0.9636743664741516}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [60], val_loss: 5.3909, val_acc: 0.9561\n",
      "59\t{'val_loss': 5.565883636474609, 'val_acc': 0.956210196018219}\n",
      "Epoch [61], val_loss: 3.6730, val_acc: 0.9642\n",
      "60\t{'val_loss': 3.937288522720337, 'val_acc': 0.9624800682067871}\n",
      "Epoch [62], val_loss: 5.1372, val_acc: 0.9612\n",
      "61\t{'val_loss': 4.570697784423828, 'val_acc': 0.9635748267173767}\n",
      "Epoch [63], val_loss: 4.5791, val_acc: 0.9589\n",
      "62\t{'val_loss': 4.853306293487549, 'val_acc': 0.9609872698783875}\n",
      "Epoch [64], val_loss: 5.8422, val_acc: 0.9591\n",
      "63\t{'val_loss': 5.24924373626709, 'val_acc': 0.9611862897872925}\n",
      "Epoch [65], val_loss: 4.8612, val_acc: 0.9614\n",
      "64\t{'val_loss': 4.901157855987549, 'val_acc': 0.9600915312767029}\n",
      "Epoch [66], val_loss: 5.9527, val_acc: 0.9551\n",
      "65\t{'val_loss': 6.531888961791992, 'val_acc': 0.9548168778419495}\n",
      "Epoch [67], val_loss: 5.1030, val_acc: 0.9638\n",
      "66\t{'val_loss': 5.452843189239502, 'val_acc': 0.9612858295440674}\n",
      "Epoch [68], val_loss: 5.9352, val_acc: 0.9583\n",
      "67\t{'val_loss': 5.575258731842041, 'val_acc': 0.962380588054657}\n",
      "Epoch [69], val_loss: 4.9048, val_acc: 0.9624\n",
      "68\t{'val_loss': 5.3480706214904785, 'val_acc': 0.9633758068084717}\n",
      "Epoch [70], val_loss: 6.2329, val_acc: 0.9591\n",
      "69\t{'val_loss': 6.292614936828613, 'val_acc': 0.9583001732826233}\n",
      "Epoch [71], val_loss: 6.7752, val_acc: 0.9569\n",
      "70\t{'val_loss': 6.456630706787109, 'val_acc': 0.9566082954406738}\n",
      "Epoch [72], val_loss: 6.9721, val_acc: 0.9589\n",
      "71\t{'val_loss': 5.661270618438721, 'val_acc': 0.9640724658966064}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [73], val_loss: 6.2701, val_acc: 0.9575\n",
      "72\t{'val_loss': 6.277126789093018, 'val_acc': 0.9615843892097473}\n",
      "Epoch [74], val_loss: 7.2362, val_acc: 0.9563\n",
      "73\t{'val_loss': 6.030657768249512, 'val_acc': 0.9634753465652466}\n",
      "Epoch [75], val_loss: 7.6831, val_acc: 0.9595\n",
      "74\t{'val_loss': 7.7263665199279785, 'val_acc': 0.959295392036438}\n",
      "Epoch [76], val_loss: 7.2093, val_acc: 0.9527\n",
      "75\t{'val_loss': 6.411994934082031, 'val_acc': 0.9584991931915283}\n",
      "Epoch [77], val_loss: 6.9068, val_acc: 0.9606\n",
      "76\t{'val_loss': 7.1185102462768555, 'val_acc': 0.9605891704559326}\n",
      "Epoch [78], val_loss: 7.7344, val_acc: 0.9589\n",
      "77\t{'val_loss': 7.996432781219482, 'val_acc': 0.9589968323707581}\n",
      "Epoch [79], val_loss: 6.3104, val_acc: 0.9656\n",
      "78\t{'val_loss': 6.112354278564453, 'val_acc': 0.9682523608207703}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [80], val_loss: 8.4920, val_acc: 0.9573\n",
      "79\t{'val_loss': 7.653403282165527, 'val_acc': 0.9588972926139832}\n",
      "Epoch [81], val_loss: 7.0496, val_acc: 0.9608\n",
      "80\t{'val_loss': 6.867026329040527, 'val_acc': 0.9645700454711914}\n",
      "Epoch [82], val_loss: 8.5888, val_acc: 0.9579\n",
      "81\t{'val_loss': 8.649785995483398, 'val_acc': 0.9595939517021179}\n",
      "Epoch [83], val_loss: 8.8979, val_acc: 0.9549\n",
      "82\t{'val_loss': 7.7476115226745605, 'val_acc': 0.962579607963562}\n",
      "Epoch [84], val_loss: 11.2458, val_acc: 0.9521\n",
      "83\t{'val_loss': 9.932367324829102, 'val_acc': 0.9580016136169434}\n",
      "Epoch [85], val_loss: 10.1668, val_acc: 0.9573\n",
      "84\t{'val_loss': 9.283138275146484, 'val_acc': 0.9585987329483032}\n",
      "Epoch [86], val_loss: 8.4244, val_acc: 0.9616\n",
      "85\t{'val_loss': 7.787932872772217, 'val_acc': 0.962380588054657}\n",
      "Epoch [87], val_loss: 9.5416, val_acc: 0.9646\n",
      "86\t{'val_loss': 9.618184089660645, 'val_acc': 0.9618829488754272}\n",
      "Epoch [88], val_loss: 11.8181, val_acc: 0.9561\n",
      "87\t{'val_loss': 10.855942726135254, 'val_acc': 0.956210196018219}\n",
      "Epoch [89], val_loss: 10.7890, val_acc: 0.9591\n",
      "88\t{'val_loss': 9.716132164001465, 'val_acc': 0.9606887102127075}\n",
      "Epoch [90], val_loss: 9.1141, val_acc: 0.9608\n",
      "89\t{'val_loss': 8.952967643737793, 'val_acc': 0.9634753465652466}\n",
      "Epoch [91], val_loss: 10.7307, val_acc: 0.9579\n",
      "90\t{'val_loss': 9.540980339050293, 'val_acc': 0.9607881903648376}\n",
      "Epoch [92], val_loss: 9.6582, val_acc: 0.9634\n",
      "91\t{'val_loss': 10.909400939941406, 'val_acc': 0.9629777073860168}\n",
      "Epoch [93], val_loss: 9.8681, val_acc: 0.9591\n",
      "92\t{'val_loss': 10.861540794372559, 'val_acc': 0.9599920511245728}\n",
      "Epoch [94], val_loss: 9.4756, val_acc: 0.9624\n",
      "93\t{'val_loss': 10.743721961975098, 'val_acc': 0.9642714858055115}\n",
      "Epoch [95], val_loss: 10.9534, val_acc: 0.9579\n",
      "94\t{'val_loss': 11.948690414428711, 'val_acc': 0.9583001732826233}\n",
      "Epoch [96], val_loss: 11.8423, val_acc: 0.9630\n",
      "95\t{'val_loss': 11.137497901916504, 'val_acc': 0.9630772471427917}\n",
      "Epoch [97], val_loss: 11.7099, val_acc: 0.9608\n",
      "96\t{'val_loss': 11.885991096496582, 'val_acc': 0.9604896306991577}\n",
      "Epoch [98], val_loss: 11.4892, val_acc: 0.9591\n",
      "97\t{'val_loss': 12.130149841308594, 'val_acc': 0.9589968323707581}\n",
      "Epoch [99], val_loss: 12.6782, val_acc: 0.9593\n",
      "98\t{'val_loss': 11.653162002563477, 'val_acc': 0.9604896306991577}\n",
      "Epoch [100], val_loss: 11.4162, val_acc: 0.9660\n",
      "99\t{'val_loss': 10.455716133117676, 'val_acc': 0.9636743664741516}\n",
      "Epoch [101], val_loss: 11.9004, val_acc: 0.9587\n",
      "100\t{'val_loss': 10.78735637664795, 'val_acc': 0.9633758068084717}\n",
      "Epoch [102], val_loss: 11.8630, val_acc: 0.9612\n",
      "101\t{'val_loss': 13.412006378173828, 'val_acc': 0.9583996534347534}\n",
      "Epoch [103], val_loss: 12.8948, val_acc: 0.9583\n",
      "102\t{'val_loss': 12.071861267089844, 'val_acc': 0.9619824886322021}\n",
      "Epoch [104], val_loss: 12.9000, val_acc: 0.9587\n",
      "103\t{'val_loss': 11.480138778686523, 'val_acc': 0.9609872698783875}\n",
      "Epoch [105], val_loss: 12.5545, val_acc: 0.9600\n",
      "104\t{'val_loss': 11.186235427856445, 'val_acc': 0.9640724658966064}\n",
      "Epoch [106], val_loss: 11.8734, val_acc: 0.9648\n",
      "105\t{'val_loss': 13.264556884765625, 'val_acc': 0.9618829488754272}\n",
      "Epoch [107], val_loss: 12.6627, val_acc: 0.9644\n",
      "106\t{'val_loss': 14.242712020874023, 'val_acc': 0.9611862897872925}\n",
      "Epoch [108], val_loss: 14.6326, val_acc: 0.9612\n",
      "107\t{'val_loss': 15.203003883361816, 'val_acc': 0.9577030539512634}\n",
      "Epoch [109], val_loss: 13.0059, val_acc: 0.9656\n",
      "108\t{'val_loss': 13.376850128173828, 'val_acc': 0.9660629034042358}\n",
      "Epoch [110], val_loss: 14.5425, val_acc: 0.9610\n",
      "109\t{'val_loss': 17.36244773864746, 'val_acc': 0.9573049545288086}\n",
      "Epoch [111], val_loss: 19.6341, val_acc: 0.9543\n",
      "110\t{'val_loss': 18.547086715698242, 'val_acc': 0.956707775592804}\n",
      "Epoch [112], val_loss: 17.9493, val_acc: 0.9604\n",
      "111\t{'val_loss': 17.411523818969727, 'val_acc': 0.9609872698783875}\n",
      "Epoch [113], val_loss: 14.9533, val_acc: 0.9656\n",
      "112\t{'val_loss': 15.977036476135254, 'val_acc': 0.9635748267173767}\n",
      "Epoch [114], val_loss: 17.2397, val_acc: 0.9561\n",
      "113\t{'val_loss': 14.680493354797363, 'val_acc': 0.962579607963562}\n",
      "Epoch [115], val_loss: 18.1115, val_acc: 0.9551\n",
      "114\t{'val_loss': 17.45359230041504, 'val_acc': 0.9590963125228882}\n",
      "Epoch [116], val_loss: 14.8163, val_acc: 0.9624\n",
      "115\t{'val_loss': 13.899660110473633, 'val_acc': 0.9668591022491455}\n",
      "Epoch [117], val_loss: 17.9166, val_acc: 0.9559\n",
      "116\t{'val_loss': 15.200055122375488, 'val_acc': 0.9617834687232971}\n",
      "Epoch [118], val_loss: 15.8603, val_acc: 0.9638\n",
      "117\t{'val_loss': 13.778364181518555, 'val_acc': 0.9662619233131409}\n",
      "Epoch [119], val_loss: 21.0688, val_acc: 0.9551\n",
      "118\t{'val_loss': 17.917375564575195, 'val_acc': 0.9588972926139832}\n",
      "Epoch [120], val_loss: 18.1574, val_acc: 0.9581\n",
      "119\t{'val_loss': 16.681922912597656, 'val_acc': 0.9583001732826233}\n",
      "Epoch [121], val_loss: 16.3287, val_acc: 0.9644\n",
      "120\t{'val_loss': 17.924654006958008, 'val_acc': 0.9601910710334778}\n",
      "Epoch [122], val_loss: 21.6090, val_acc: 0.9563\n",
      "121\t{'val_loss': 20.24728775024414, 'val_acc': 0.9602906107902527}\n",
      "Epoch [123], val_loss: 18.6456, val_acc: 0.9600\n",
      "122\t{'val_loss': 18.00542640686035, 'val_acc': 0.9608877301216125}\n",
      "Epoch [124], val_loss: 22.1698, val_acc: 0.9604\n",
      "123\t{'val_loss': 19.22650718688965, 'val_acc': 0.9649681448936462}\n",
      "Epoch [125], val_loss: 20.0034, val_acc: 0.9616\n",
      "124\t{'val_loss': 18.69479751586914, 'val_acc': 0.9608877301216125}\n",
      "Epoch [126], val_loss: 22.3980, val_acc: 0.9581\n",
      "125\t{'val_loss': 19.805208206176758, 'val_acc': 0.9644705653190613}\n",
      "Epoch [127], val_loss: 18.7482, val_acc: 0.9620\n",
      "126\t{'val_loss': 20.072275161743164, 'val_acc': 0.9639729261398315}\n",
      "Epoch [128], val_loss: 16.4660, val_acc: 0.9682\n",
      "127\t{'val_loss': 17.1572208404541, 'val_acc': 0.9670581221580505}\n",
      "Epoch [129], val_loss: 19.4212, val_acc: 0.9628\n",
      "128\t{'val_loss': 18.404502868652344, 'val_acc': 0.965465784072876}\n",
      "Epoch [130], val_loss: 20.6782, val_acc: 0.9575\n",
      "129\t{'val_loss': 20.959789276123047, 'val_acc': 0.9639729261398315}\n",
      "Epoch [131], val_loss: 20.7208, val_acc: 0.9612\n",
      "130\t{'val_loss': 20.297283172607422, 'val_acc': 0.9630772471427917}\n",
      "Epoch [132], val_loss: 18.9150, val_acc: 0.9668\n",
      "131\t{'val_loss': 20.680103302001953, 'val_acc': 0.9646695852279663}\n",
      "Epoch [133], val_loss: 25.3921, val_acc: 0.9604\n",
      "132\t{'val_loss': 22.623672485351562, 'val_acc': 0.9615843892097473}\n",
      "Epoch [134], val_loss: 18.9124, val_acc: 0.9636\n",
      "133\t{'val_loss': 23.031417846679688, 'val_acc': 0.9596934914588928}\n",
      "Epoch [135], val_loss: 19.9719, val_acc: 0.9650\n",
      "134\t{'val_loss': 21.628963470458984, 'val_acc': 0.9648686051368713}\n",
      "Epoch [136], val_loss: 21.2243, val_acc: 0.9620\n",
      "135\t{'val_loss': 20.25843048095703, 'val_acc': 0.9643710255622864}\n",
      "Epoch [137], val_loss: 19.9472, val_acc: 0.9608\n",
      "136\t{'val_loss': 21.372493743896484, 'val_acc': 0.9643710255622864}\n",
      "Epoch [138], val_loss: 21.7572, val_acc: 0.9640\n",
      "137\t{'val_loss': 19.69138526916504, 'val_acc': 0.968949019908905}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [139], val_loss: 24.3156, val_acc: 0.9614\n",
      "138\t{'val_loss': 23.693675994873047, 'val_acc': 0.9644705653190613}\n",
      "Epoch [140], val_loss: 20.4510, val_acc: 0.9626\n",
      "139\t{'val_loss': 22.91682243347168, 'val_acc': 0.9637739062309265}\n",
      "Epoch [141], val_loss: 37.2592, val_acc: 0.9482\n",
      "140\t{'val_loss': 39.512062072753906, 'val_acc': 0.9462579488754272}\n",
      "Epoch [142], val_loss: 22.0008, val_acc: 0.9630\n",
      "141\t{'val_loss': 21.527278900146484, 'val_acc': 0.965863823890686}\n",
      "Epoch [143], val_loss: 22.3238, val_acc: 0.9652\n",
      "142\t{'val_loss': 17.63097381591797, 'val_acc': 0.9713375568389893}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_2bits_weights.pth\n",
      "Epoch [144], val_loss: 25.5268, val_acc: 0.9600\n",
      "143\t{'val_loss': 23.71394157409668, 'val_acc': 0.9617834687232971}\n",
      "Epoch [145], val_loss: 25.5122, val_acc: 0.9624\n",
      "144\t{'val_loss': 29.130399703979492, 'val_acc': 0.9626791477203369}\n",
      "Epoch [146], val_loss: 35.3891, val_acc: 0.9527\n",
      "145\t{'val_loss': 32.460147857666016, 'val_acc': 0.956409215927124}\n",
      "Epoch [147], val_loss: 32.1502, val_acc: 0.9543\n",
      "146\t{'val_loss': 30.618436813354492, 'val_acc': 0.9589968323707581}\n",
      "Epoch [148], val_loss: 25.1273, val_acc: 0.9610\n",
      "147\t{'val_loss': 25.41750144958496, 'val_acc': 0.9607881903648376}\n",
      "Epoch [149], val_loss: 30.8284, val_acc: 0.9563\n",
      "148\t{'val_loss': 29.073625564575195, 'val_acc': 0.9595939517021179}\n",
      "Epoch [150], val_loss: 25.4589, val_acc: 0.9644\n",
      "149\t{'val_loss': 24.46444320678711, 'val_acc': 0.9633758068084717}\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=150, lr=0.001, model=model, \n",
    "               train_loader=train_loader, \n",
    "               val_loader=val_loader, \n",
    "               test_loader=test_loader, \n",
    "               file_path=save_weight_path, \n",
    "               opt_func=torch.optim.Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224648/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 24.46444320678711, 'val_acc': 0.9633758068084717}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1]\n",
      "[-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "int_fc1 = np.array(model.fc1.quant_weight().int().tolist())\n",
    "int_fc2 = np.array(model.fc2.quant_weight().int().tolist())\n",
    "\n",
    "print(np.unique(int_fc1))\n",
    "print(np.unique(int_fc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
