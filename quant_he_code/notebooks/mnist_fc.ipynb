{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/mnist')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bit_width = 4\n",
    "input_bit_width = 2  # quantize into [0,1,2,3]\n",
    "save_weight_path = project_path/f\"quant_he_code/weights/quant_fc_{input_bit_width}bits_mnist_plain_{weight_bit_width}bits_weights.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:  \n",
    "\n",
    "0. Quantize into [0, 0.25, 0.5, 0.75, 1]\n",
    "1. Quantize into [0, 1]\n",
    "2. Quantize into [0, 1, 2, 3]\n",
    "3. Quantize into [0, 1, 2, 3, ..., 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    train_dataset = MNIST(root=mnist_path, download=False, transform=transform)\n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "torch.Size([784])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f670519fbb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"train_dataset shape: {train_dataset.data.shape}\")\n",
    "print(f\"test_dataset shape: {test_dataset.data.shape}\")\n",
    "\n",
    "im = train_dataset[0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the FC Model (2 linear layers, 1 square activation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT 2-FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    2 linear layers + 1 square activations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width,\n",
    "                                   return_quant_tensor=True)\n",
    "        \n",
    "        self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                   weight_bit_width=weight_bit_width, \n",
    "                                   return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        out = self.fc1(xb)\n",
    "        out = out * out  # first square\n",
    "        # out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = to_device(MNISTLinearQuantModel(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_path, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.96\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(f\"Saved into {file_path.relative_to(project_path)}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_365837/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.3887, val_acc: 0.9626\n",
      "Epoch [2], val_loss: 0.3547, val_acc: 0.9660\n",
      "Epoch [3], val_loss: 0.3950, val_acc: 0.9693\n",
      "2\t{'val_loss': 0.40848469734191895, 'val_acc': 0.9674562215805054}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [4], val_loss: 0.4185, val_acc: 0.9717\n",
      "3\t{'val_loss': 0.44978851079940796, 'val_acc': 0.9706408977508545}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [5], val_loss: 0.5843, val_acc: 0.9595\n",
      "4\t{'val_loss': 0.5809839367866516, 'val_acc': 0.9619824886322021}\n",
      "Epoch [6], val_loss: 0.4829, val_acc: 0.9680\n",
      "5\t{'val_loss': 0.5866458415985107, 'val_acc': 0.9639729261398315}\n",
      "Epoch [7], val_loss: 0.5554, val_acc: 0.9662\n",
      "6\t{'val_loss': 0.647015392780304, 'val_acc': 0.9674562215805054}\n",
      "Epoch [8], val_loss: 0.5506, val_acc: 0.9684\n",
      "7\t{'val_loss': 0.6475145816802979, 'val_acc': 0.9693471193313599}\n",
      "Epoch [9], val_loss: 0.5552, val_acc: 0.9701\n",
      "8\t{'val_loss': 0.6736658215522766, 'val_acc': 0.968949019908905}\n",
      "Epoch [10], val_loss: 0.6311, val_acc: 0.9693\n",
      "9\t{'val_loss': 0.7135081887245178, 'val_acc': 0.9693471193313599}\n",
      "Epoch [11], val_loss: 0.7166, val_acc: 0.9693\n",
      "10\t{'val_loss': 0.8854777216911316, 'val_acc': 0.9673566818237305}\n",
      "Epoch [12], val_loss: 0.7669, val_acc: 0.9670\n",
      "11\t{'val_loss': 0.8199714422225952, 'val_acc': 0.9676552414894104}\n",
      "Epoch [13], val_loss: 0.6691, val_acc: 0.9727\n",
      "12\t{'val_loss': 0.7683092951774597, 'val_acc': 0.9696456789970398}\n",
      "Epoch [14], val_loss: 0.7823, val_acc: 0.9678\n",
      "13\t{'val_loss': 1.0790287256240845, 'val_acc': 0.9649681448936462}\n",
      "Epoch [15], val_loss: 0.9685, val_acc: 0.9672\n",
      "14\t{'val_loss': 0.9801402688026428, 'val_acc': 0.9697452187538147}\n",
      "Epoch [16], val_loss: 1.5373, val_acc: 0.9632\n",
      "15\t{'val_loss': 1.5124104022979736, 'val_acc': 0.9666600227355957}\n",
      "Epoch [17], val_loss: 0.9002, val_acc: 0.9729\n",
      "16\t{'val_loss': 0.999160647392273, 'val_acc': 0.972332775592804}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [18], val_loss: 1.0947, val_acc: 0.9662\n",
      "17\t{'val_loss': 1.4041705131530762, 'val_acc': 0.9645700454711914}\n",
      "Epoch [19], val_loss: 1.1754, val_acc: 0.9701\n",
      "18\t{'val_loss': 1.2615257501602173, 'val_acc': 0.9698447585105896}\n",
      "Epoch [20], val_loss: 1.3513, val_acc: 0.9674\n",
      "19\t{'val_loss': 1.3020799160003662, 'val_acc': 0.9706408977508545}\n",
      "Epoch [21], val_loss: 1.4144, val_acc: 0.9666\n",
      "20\t{'val_loss': 1.3813326358795166, 'val_acc': 0.9696456789970398}\n",
      "Epoch [22], val_loss: 1.5291, val_acc: 0.9662\n",
      "21\t{'val_loss': 1.5199382305145264, 'val_acc': 0.9682523608207703}\n",
      "Epoch [23], val_loss: 1.5339, val_acc: 0.9721\n",
      "22\t{'val_loss': 1.7373225688934326, 'val_acc': 0.9707404375076294}\n",
      "Epoch [24], val_loss: 1.4993, val_acc: 0.9745\n",
      "23\t{'val_loss': 1.6569710969924927, 'val_acc': 0.9738256335258484}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [25], val_loss: 1.9755, val_acc: 0.9658\n",
      "24\t{'val_loss': 2.001300096511841, 'val_acc': 0.9671576619148254}\n",
      "Epoch [26], val_loss: 1.6347, val_acc: 0.9735\n",
      "25\t{'val_loss': 1.5535727739334106, 'val_acc': 0.9729299545288086}\n",
      "Epoch [27], val_loss: 1.7910, val_acc: 0.9703\n",
      "26\t{'val_loss': 1.728105902671814, 'val_acc': 0.971835196018219}\n",
      "Epoch [28], val_loss: 1.8358, val_acc: 0.9713\n",
      "27\t{'val_loss': 1.7536333799362183, 'val_acc': 0.9691480994224548}\n",
      "Epoch [29], val_loss: 2.1019, val_acc: 0.9648\n",
      "28\t{'val_loss': 2.5317602157592773, 'val_acc': 0.9622810482978821}\n",
      "Epoch [30], val_loss: 1.7947, val_acc: 0.9745\n",
      "29\t{'val_loss': 1.69761061668396, 'val_acc': 0.975417971611023}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [31], val_loss: 1.8431, val_acc: 0.9735\n",
      "30\t{'val_loss': 1.7608722448349, 'val_acc': 0.9763137102127075}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [32], val_loss: 2.0199, val_acc: 0.9711\n",
      "31\t{'val_loss': 2.0850584506988525, 'val_acc': 0.9721337556838989}\n",
      "Epoch [33], val_loss: 2.7005, val_acc: 0.9630\n",
      "32\t{'val_loss': 2.6122233867645264, 'val_acc': 0.965863823890686}\n",
      "Epoch [34], val_loss: 2.2450, val_acc: 0.9717\n",
      "33\t{'val_loss': 2.471442937850952, 'val_acc': 0.9700437784194946}\n",
      "Epoch [35], val_loss: 2.1670, val_acc: 0.9737\n",
      "34\t{'val_loss': 2.284418821334839, 'val_acc': 0.9736266136169434}\n",
      "Epoch [36], val_loss: 2.1524, val_acc: 0.9735\n",
      "35\t{'val_loss': 2.2390873432159424, 'val_acc': 0.9764131903648376}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [37], val_loss: 2.6708, val_acc: 0.9703\n",
      "36\t{'val_loss': 2.5152740478515625, 'val_acc': 0.9738256335258484}\n",
      "Epoch [38], val_loss: 2.7901, val_acc: 0.9688\n",
      "37\t{'val_loss': 2.4591429233551025, 'val_acc': 0.9721337556838989}\n",
      "Epoch [39], val_loss: 2.5785, val_acc: 0.9701\n",
      "38\t{'val_loss': 2.666816234588623, 'val_acc': 0.9722332954406738}\n",
      "Epoch [40], val_loss: 2.6735, val_acc: 0.9707\n",
      "39\t{'val_loss': 3.3112356662750244, 'val_acc': 0.9680533409118652}\n",
      "Epoch [41], val_loss: 2.8004, val_acc: 0.9697\n",
      "40\t{'val_loss': 3.033480644226074, 'val_acc': 0.972034215927124}\n",
      "Epoch [42], val_loss: 2.7228, val_acc: 0.9735\n",
      "41\t{'val_loss': 3.17722749710083, 'val_acc': 0.9711385369300842}\n",
      "Epoch [43], val_loss: 3.3773, val_acc: 0.9678\n",
      "42\t{'val_loss': 3.4707863330841064, 'val_acc': 0.9691480994224548}\n",
      "Epoch [44], val_loss: 3.1884, val_acc: 0.9727\n",
      "43\t{'val_loss': 3.4656240940093994, 'val_acc': 0.971636176109314}\n",
      "Epoch [45], val_loss: 3.5603, val_acc: 0.9725\n",
      "44\t{'val_loss': 3.6499040126800537, 'val_acc': 0.9733280539512634}\n",
      "Epoch [46], val_loss: 3.1674, val_acc: 0.9739\n",
      "45\t{'val_loss': 2.985144853591919, 'val_acc': 0.975119411945343}\n",
      "Epoch [47], val_loss: 3.8146, val_acc: 0.9717\n",
      "46\t{'val_loss': 3.3785719871520996, 'val_acc': 0.9740246534347534}\n",
      "Epoch [48], val_loss: 3.7012, val_acc: 0.9711\n",
      "47\t{'val_loss': 3.796865224838257, 'val_acc': 0.9726313948631287}\n",
      "Epoch [49], val_loss: 5.1668, val_acc: 0.9624\n",
      "48\t{'val_loss': 5.4776225090026855, 'val_acc': 0.9659633636474609}\n",
      "Epoch [50], val_loss: 3.2809, val_acc: 0.9735\n",
      "49\t{'val_loss': 3.7270777225494385, 'val_acc': 0.9746218323707581}\n",
      "Epoch [51], val_loss: 4.4999, val_acc: 0.9693\n",
      "50\t{'val_loss': 4.443172454833984, 'val_acc': 0.9694466590881348}\n",
      "Epoch [52], val_loss: 4.4516, val_acc: 0.9707\n",
      "51\t{'val_loss': 4.4407501220703125, 'val_acc': 0.972034215927124}\n",
      "Epoch [53], val_loss: 4.1947, val_acc: 0.9688\n",
      "52\t{'val_loss': 4.225818157196045, 'val_acc': 0.9717356562614441}\n",
      "Epoch [54], val_loss: 4.3915, val_acc: 0.9691\n",
      "53\t{'val_loss': 4.450457572937012, 'val_acc': 0.9698447585105896}\n",
      "Epoch [55], val_loss: 4.3304, val_acc: 0.9727\n",
      "54\t{'val_loss': 5.031691074371338, 'val_acc': 0.9704418778419495}\n",
      "Epoch [56], val_loss: 4.4023, val_acc: 0.9731\n",
      "55\t{'val_loss': 5.105350017547607, 'val_acc': 0.9700437784194946}\n",
      "Epoch [57], val_loss: 4.0980, val_acc: 0.9747\n",
      "56\t{'val_loss': 4.345388412475586, 'val_acc': 0.9734275341033936}\n",
      "Epoch [58], val_loss: 3.9445, val_acc: 0.9749\n",
      "57\t{'val_loss': 3.8218705654144287, 'val_acc': 0.9750199317932129}\n",
      "Epoch [59], val_loss: 4.9237, val_acc: 0.9725\n",
      "58\t{'val_loss': 4.991652011871338, 'val_acc': 0.9714370965957642}\n",
      "Epoch [60], val_loss: 5.0328, val_acc: 0.9701\n",
      "59\t{'val_loss': 5.0262770652771, 'val_acc': 0.9717356562614441}\n",
      "Epoch [61], val_loss: 4.3537, val_acc: 0.9747\n",
      "60\t{'val_loss': 4.832997798919678, 'val_acc': 0.9743232727050781}\n",
      "Epoch [62], val_loss: 4.7398, val_acc: 0.9737\n",
      "61\t{'val_loss': 5.019684314727783, 'val_acc': 0.9734275341033936}\n",
      "Epoch [63], val_loss: 5.8291, val_acc: 0.9680\n",
      "62\t{'val_loss': 5.518272876739502, 'val_acc': 0.9711385369300842}\n",
      "Epoch [64], val_loss: 5.6593, val_acc: 0.9695\n",
      "63\t{'val_loss': 5.256733417510986, 'val_acc': 0.9721337556838989}\n",
      "Epoch [65], val_loss: 5.1440, val_acc: 0.9735\n",
      "64\t{'val_loss': 5.408555507659912, 'val_acc': 0.9744227528572083}\n",
      "Epoch [66], val_loss: 5.6961, val_acc: 0.9711\n",
      "65\t{'val_loss': 6.915326118469238, 'val_acc': 0.9692476391792297}\n",
      "Epoch [67], val_loss: 5.2025, val_acc: 0.9729\n",
      "66\t{'val_loss': 5.514327526092529, 'val_acc': 0.9739251732826233}\n",
      "Epoch [68], val_loss: 5.3702, val_acc: 0.9729\n",
      "67\t{'val_loss': 5.733294486999512, 'val_acc': 0.9736266136169434}\n",
      "Epoch [69], val_loss: 6.0249, val_acc: 0.9729\n",
      "68\t{'val_loss': 6.729809284210205, 'val_acc': 0.9719347357749939}\n",
      "Epoch [70], val_loss: 6.0007, val_acc: 0.9684\n",
      "69\t{'val_loss': 6.075690746307373, 'val_acc': 0.9730294346809387}\n",
      "Epoch [71], val_loss: 5.4814, val_acc: 0.9713\n",
      "70\t{'val_loss': 7.181191444396973, 'val_acc': 0.9724323153495789}\n",
      "Epoch [72], val_loss: 5.2457, val_acc: 0.9753\n",
      "71\t{'val_loss': 6.491272449493408, 'val_acc': 0.9719347357749939}\n",
      "Epoch [73], val_loss: 7.2805, val_acc: 0.9666\n",
      "72\t{'val_loss': 8.129740715026855, 'val_acc': 0.9676552414894104}\n",
      "Epoch [74], val_loss: 6.4960, val_acc: 0.9697\n",
      "73\t{'val_loss': 7.449890613555908, 'val_acc': 0.9735270738601685}\n",
      "Epoch [75], val_loss: 7.3671, val_acc: 0.9701\n",
      "74\t{'val_loss': 8.020654678344727, 'val_acc': 0.971636176109314}\n",
      "Epoch [76], val_loss: 6.2590, val_acc: 0.9721\n",
      "75\t{'val_loss': 6.676243305206299, 'val_acc': 0.9750199317932129}\n",
      "Epoch [77], val_loss: 6.4291, val_acc: 0.9727\n",
      "76\t{'val_loss': 7.399644374847412, 'val_acc': 0.9729299545288086}\n",
      "Epoch [78], val_loss: 6.8867, val_acc: 0.9727\n",
      "77\t{'val_loss': 6.781338691711426, 'val_acc': 0.9759156107902527}\n",
      "Epoch [79], val_loss: 6.1238, val_acc: 0.9741\n",
      "78\t{'val_loss': 7.725697040557861, 'val_acc': 0.9738256335258484}\n",
      "Epoch [80], val_loss: 6.5036, val_acc: 0.9735\n",
      "79\t{'val_loss': 7.612921237945557, 'val_acc': 0.9735270738601685}\n",
      "Epoch [81], val_loss: 6.6960, val_acc: 0.9737\n",
      "80\t{'val_loss': 7.692291259765625, 'val_acc': 0.9734275341033936}\n",
      "Epoch [82], val_loss: 6.9761, val_acc: 0.9739\n",
      "81\t{'val_loss': 8.2322416305542, 'val_acc': 0.9738256335258484}\n",
      "Epoch [83], val_loss: 6.4803, val_acc: 0.9743\n",
      "82\t{'val_loss': 7.935451984405518, 'val_acc': 0.9739251732826233}\n",
      "Epoch [84], val_loss: 7.5580, val_acc: 0.9731\n",
      "83\t{'val_loss': 8.925345420837402, 'val_acc': 0.9713375568389893}\n",
      "Epoch [85], val_loss: 7.2633, val_acc: 0.9733\n",
      "84\t{'val_loss': 7.288003444671631, 'val_acc': 0.9748208522796631}\n",
      "Epoch [86], val_loss: 6.9277, val_acc: 0.9737\n",
      "85\t{'val_loss': 9.257293701171875, 'val_acc': 0.971636176109314}\n",
      "Epoch [87], val_loss: 7.4908, val_acc: 0.9733\n",
      "86\t{'val_loss': 9.535006523132324, 'val_acc': 0.9734275341033936}\n",
      "Epoch [88], val_loss: 8.3416, val_acc: 0.9693\n",
      "87\t{'val_loss': 9.394769668579102, 'val_acc': 0.9715366363525391}\n",
      "Epoch [89], val_loss: 7.8749, val_acc: 0.9719\n",
      "88\t{'val_loss': 9.44518756866455, 'val_acc': 0.9702428579330444}\n",
      "Epoch [90], val_loss: 7.2365, val_acc: 0.9739\n",
      "89\t{'val_loss': 8.441130638122559, 'val_acc': 0.9748208522796631}\n",
      "Epoch [91], val_loss: 8.2833, val_acc: 0.9691\n",
      "90\t{'val_loss': 9.473653793334961, 'val_acc': 0.9713375568389893}\n",
      "Epoch [92], val_loss: 9.1814, val_acc: 0.9747\n",
      "91\t{'val_loss': 9.005294799804688, 'val_acc': 0.9738256335258484}\n",
      "Epoch [93], val_loss: 7.8365, val_acc: 0.9739\n",
      "92\t{'val_loss': 8.739456176757812, 'val_acc': 0.9735270738601685}\n",
      "Epoch [94], val_loss: 9.9287, val_acc: 0.9707\n",
      "93\t{'val_loss': 10.445647239685059, 'val_acc': 0.9717356562614441}\n",
      "Epoch [95], val_loss: 10.1496, val_acc: 0.9731\n",
      "94\t{'val_loss': 9.21389389038086, 'val_acc': 0.9756170511245728}\n",
      "Epoch [96], val_loss: 9.0147, val_acc: 0.9723\n",
      "95\t{'val_loss': 9.402727127075195, 'val_acc': 0.9732285141944885}\n",
      "Epoch [97], val_loss: 9.0612, val_acc: 0.9725\n",
      "96\t{'val_loss': 9.876259803771973, 'val_acc': 0.9739251732826233}\n",
      "Epoch [98], val_loss: 8.1293, val_acc: 0.9739\n",
      "97\t{'val_loss': 8.854690551757812, 'val_acc': 0.9748208522796631}\n",
      "Epoch [99], val_loss: 8.6671, val_acc: 0.9739\n",
      "98\t{'val_loss': 9.42066478729248, 'val_acc': 0.9756170511245728}\n",
      "Epoch [100], val_loss: 9.7215, val_acc: 0.9731\n",
      "99\t{'val_loss': 9.524386405944824, 'val_acc': 0.9747213125228882}\n",
      "Epoch [101], val_loss: 8.6555, val_acc: 0.9765\n",
      "100\t{'val_loss': 9.88702392578125, 'val_acc': 0.9752189517021179}\n",
      "Epoch [102], val_loss: 10.2825, val_acc: 0.9721\n",
      "101\t{'val_loss': 10.140083312988281, 'val_acc': 0.9741241931915283}\n",
      "Epoch [103], val_loss: 9.9478, val_acc: 0.9727\n",
      "102\t{'val_loss': 10.24181079864502, 'val_acc': 0.9750199317932129}\n",
      "Epoch [104], val_loss: 10.5067, val_acc: 0.9735\n",
      "103\t{'val_loss': 10.727508544921875, 'val_acc': 0.9732285141944885}\n",
      "Epoch [105], val_loss: 9.9085, val_acc: 0.9717\n",
      "104\t{'val_loss': 10.302902221679688, 'val_acc': 0.9737260937690735}\n",
      "Epoch [106], val_loss: 9.9962, val_acc: 0.9705\n",
      "105\t{'val_loss': 11.051985740661621, 'val_acc': 0.9737260937690735}\n",
      "Epoch [107], val_loss: 11.0211, val_acc: 0.9727\n",
      "106\t{'val_loss': 13.427862167358398, 'val_acc': 0.9684514403343201}\n",
      "Epoch [108], val_loss: 8.2991, val_acc: 0.9767\n",
      "107\t{'val_loss': 9.69902515411377, 'val_acc': 0.9768112897872925}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [109], val_loss: 10.0968, val_acc: 0.9751\n",
      "108\t{'val_loss': 11.895515441894531, 'val_acc': 0.9735270738601685}\n",
      "Epoch [110], val_loss: 10.5739, val_acc: 0.9721\n",
      "109\t{'val_loss': 11.828972816467285, 'val_acc': 0.9744227528572083}\n",
      "Epoch [111], val_loss: 9.8786, val_acc: 0.9745\n",
      "110\t{'val_loss': 11.245440483093262, 'val_acc': 0.9752189517021179}\n",
      "Epoch [112], val_loss: 10.6230, val_acc: 0.9715\n",
      "111\t{'val_loss': 11.11502742767334, 'val_acc': 0.974920392036438}\n",
      "Epoch [113], val_loss: 10.5754, val_acc: 0.9749\n",
      "112\t{'val_loss': 12.564476013183594, 'val_acc': 0.9767118096351624}\n",
      "Epoch [114], val_loss: 11.8037, val_acc: 0.9753\n",
      "113\t{'val_loss': 12.60534381866455, 'val_acc': 0.9724323153495789}\n",
      "Epoch [115], val_loss: 11.3335, val_acc: 0.9721\n",
      "114\t{'val_loss': 13.66214370727539, 'val_acc': 0.972332775592804}\n",
      "Epoch [116], val_loss: 11.3740, val_acc: 0.9725\n",
      "115\t{'val_loss': 12.474120140075684, 'val_acc': 0.9735270738601685}\n",
      "Epoch [117], val_loss: 13.1100, val_acc: 0.9733\n",
      "116\t{'val_loss': 13.127552032470703, 'val_acc': 0.9724323153495789}\n",
      "Epoch [118], val_loss: 12.0652, val_acc: 0.9741\n",
      "117\t{'val_loss': 12.46446418762207, 'val_acc': 0.9728304147720337}\n",
      "Epoch [119], val_loss: 12.4139, val_acc: 0.9739\n",
      "118\t{'val_loss': 13.504712104797363, 'val_acc': 0.9738256335258484}\n",
      "Epoch [120], val_loss: 12.2171, val_acc: 0.9699\n",
      "119\t{'val_loss': 13.906343460083008, 'val_acc': 0.9712380766868591}\n",
      "Epoch [121], val_loss: 12.4491, val_acc: 0.9739\n",
      "120\t{'val_loss': 13.940343856811523, 'val_acc': 0.9748208522796631}\n",
      "Epoch [122], val_loss: 12.8759, val_acc: 0.9737\n",
      "121\t{'val_loss': 14.971877098083496, 'val_acc': 0.9727308750152588}\n",
      "Epoch [123], val_loss: 12.3514, val_acc: 0.9739\n",
      "122\t{'val_loss': 13.865509986877441, 'val_acc': 0.9742237329483032}\n",
      "Epoch [124], val_loss: 12.9811, val_acc: 0.9721\n",
      "123\t{'val_loss': 15.033745765686035, 'val_acc': 0.9734275341033936}\n",
      "Epoch [125], val_loss: 13.2601, val_acc: 0.9755\n",
      "124\t{'val_loss': 11.695151329040527, 'val_acc': 0.9778065085411072}\n",
      "Saved into quant_he_code/weights/quant_fc_2bits_mnist_plain_4bits_weights.pth\n",
      "Epoch [126], val_loss: 14.1031, val_acc: 0.9711\n",
      "125\t{'val_loss': 16.160083770751953, 'val_acc': 0.9713375568389893}\n",
      "Epoch [127], val_loss: 12.2219, val_acc: 0.9751\n",
      "126\t{'val_loss': 13.658196449279785, 'val_acc': 0.9756170511245728}\n",
      "Epoch [128], val_loss: 12.8735, val_acc: 0.9747\n",
      "127\t{'val_loss': 15.395164489746094, 'val_acc': 0.9734275341033936}\n",
      "Epoch [129], val_loss: 12.3468, val_acc: 0.9741\n",
      "128\t{'val_loss': 13.782946586608887, 'val_acc': 0.9737260937690735}\n",
      "Epoch [130], val_loss: 13.4178, val_acc: 0.9751\n",
      "129\t{'val_loss': 14.354681968688965, 'val_acc': 0.9764131903648376}\n",
      "Epoch [131], val_loss: 13.6998, val_acc: 0.9778\n",
      "130\t{'val_loss': 14.191934585571289, 'val_acc': 0.9761146306991577}\n",
      "Epoch [132], val_loss: 13.2168, val_acc: 0.9759\n",
      "131\t{'val_loss': 14.601548194885254, 'val_acc': 0.975417971611023}\n",
      "Epoch [133], val_loss: 15.6539, val_acc: 0.9727\n",
      "132\t{'val_loss': 17.371030807495117, 'val_acc': 0.9715366363525391}\n",
      "Epoch [134], val_loss: 14.9351, val_acc: 0.9695\n",
      "133\t{'val_loss': 16.304105758666992, 'val_acc': 0.9726313948631287}\n",
      "Epoch [135], val_loss: 11.9666, val_acc: 0.9757\n",
      "134\t{'val_loss': 14.590911865234375, 'val_acc': 0.9759156107902527}\n",
      "Epoch [136], val_loss: 13.3358, val_acc: 0.9775\n",
      "135\t{'val_loss': 15.371672630310059, 'val_acc': 0.9761146306991577}\n",
      "Epoch [137], val_loss: 14.7993, val_acc: 0.9709\n",
      "136\t{'val_loss': 17.45729637145996, 'val_acc': 0.9737260937690735}\n",
      "Epoch [138], val_loss: 13.6952, val_acc: 0.9735\n",
      "137\t{'val_loss': 16.24440574645996, 'val_acc': 0.9753184914588928}\n",
      "Epoch [139], val_loss: 14.9983, val_acc: 0.9731\n",
      "138\t{'val_loss': 16.391277313232422, 'val_acc': 0.9745222926139832}\n",
      "Epoch [140], val_loss: 15.1048, val_acc: 0.9749\n",
      "139\t{'val_loss': 15.8728666305542, 'val_acc': 0.9750199317932129}\n",
      "Epoch [141], val_loss: 15.1489, val_acc: 0.9747\n",
      "140\t{'val_loss': 17.524621963500977, 'val_acc': 0.9734275341033936}\n",
      "Epoch [142], val_loss: 20.2607, val_acc: 0.9701\n",
      "141\t{'val_loss': 19.392030715942383, 'val_acc': 0.9721337556838989}\n",
      "Epoch [143], val_loss: 16.6505, val_acc: 0.9751\n",
      "142\t{'val_loss': 18.40362548828125, 'val_acc': 0.9737260937690735}\n",
      "Epoch [144], val_loss: 14.3117, val_acc: 0.9757\n",
      "143\t{'val_loss': 15.98145580291748, 'val_acc': 0.9727308750152588}\n",
      "Epoch [145], val_loss: 13.5007, val_acc: 0.9759\n",
      "144\t{'val_loss': 14.97506046295166, 'val_acc': 0.9761146306991577}\n",
      "Epoch [146], val_loss: 14.3441, val_acc: 0.9759\n",
      "145\t{'val_loss': 17.628965377807617, 'val_acc': 0.9745222926139832}\n",
      "Epoch [147], val_loss: 14.3704, val_acc: 0.9773\n",
      "146\t{'val_loss': 16.931262969970703, 'val_acc': 0.9765127301216125}\n",
      "Epoch [148], val_loss: 16.0464, val_acc: 0.9763\n",
      "147\t{'val_loss': 18.30170249938965, 'val_acc': 0.9741241931915283}\n",
      "Epoch [149], val_loss: 17.2689, val_acc: 0.9733\n",
      "148\t{'val_loss': 18.63446617126465, 'val_acc': 0.9740246534347534}\n",
      "Epoch [150], val_loss: 17.2500, val_acc: 0.9749\n",
      "149\t{'val_loss': 20.692792892456055, 'val_acc': 0.9748208522796631}\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=150, lr=0.001, model=model, \n",
    "               train_loader=train_loader, \n",
    "               val_loader=val_loader, \n",
    "               test_loader=test_loader, \n",
    "               file_path=save_weight_path, \n",
    "               opt_func=torch.optim.Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_365837/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:363.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 20.692792892456055, 'val_acc': 0.9748208522796631}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
