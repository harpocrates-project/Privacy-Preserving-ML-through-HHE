{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on the MIT-BIH Arrhythmia Dataset \n",
    "*Train a Float Neural Net on Float Data (balanced data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print(f'torch version: {torch.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "project_path\n",
    "train_path_x = project_path / 'data' / 'mit-bih' / 'csv' / 'mitbih_balanced_x_train.csv'\n",
    "train_path_y = project_path / 'data' / 'mit-bih' / 'csv' / 'mitbih_balanced_bin_y_train.csv'\n",
    "test_path_x = project_path / 'data' / 'mit-bih' / 'csv' / 'mitbih_balanced_x_test.csv'\n",
    "test_path_y = project_path / 'data' / 'mit-bih' / 'csv' / 'mitbih_balanced_bin_y_test.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBinaryECG(Dataset):\n",
    "    def __init__(self, \n",
    "                 train_path_x: Path, \n",
    "                 train_path_y: Path,\n",
    "                 test_path_x: Path,\n",
    "                 test_path_y: Path, \n",
    "                 train=True):\n",
    "        if train:\n",
    "            self.x = np.loadtxt(train_path_x, delimiter=\",\", dtype=float)\n",
    "            self.x = torch.tensor(self.x, dtype=torch.float)\n",
    "            self.x = torch.unsqueeze(self.x, 1)\n",
    "\n",
    "            self.y = np.loadtxt(train_path_y, delimiter=\",\", dtype=float)\n",
    "            self.y = torch.tensor(self.y)\n",
    "\n",
    "        else:\n",
    "            self.x = np.loadtxt(test_path_x, delimiter=\",\", dtype=float)\n",
    "            self.x = torch.tensor(self.x, dtype=torch.float)\n",
    "            self.x = torch.unsqueeze(self.x, 1)\n",
    "            \n",
    "            self.y = np.loadtxt(test_path_y, delimiter=\",\", dtype=float)\n",
    "            self.y = torch.tensor(self.y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = BalancedBinaryECG(train_path_x, train_path_y, test_path_x, test_path_y, train=True)\n",
    "test_dataset = BalancedBinaryECG(train_path_x, train_path_y, test_path_x, test_path_y, train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 1, 128])\n",
      "torch.Size([6000])\n",
      "torch.Size([6000, 1, 128])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x.shape)\n",
    "print(train_dataset.y.shape)\n",
    "print(test_dataset.x.shape)\n",
    "print(test_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8ec5696d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = train_dataset.x[0]\n",
    "print(x0.shape)\n",
    "plt.plot(x0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.y = tensor([1., 1., 0.,  ..., 1., 1., 0.], dtype=torch.float64)\n",
      "there are 3000 normal beats and 3000 abnormal beats in the train dataset\n",
      "there are 3000 normal beats and 3000 abnormal beats in the train dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_dataset.y = }\")\n",
    "\n",
    "unique, train_counts = np.unique(train_dataset.y, return_counts=True)\n",
    "print(f\"there are {train_counts[0]} normal beats and {train_counts[1]} abnormal beats in the train dataset\")\n",
    "unique, test_counts = np.unique(test_dataset.y, return_counts=True)\n",
    "print(f\"there are {test_counts[0]} normal beats and {test_counts[1]} abnormal beats in the train dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters, random seed and the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'device: {torch.cuda.get_device_name(0)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel(\n",
      "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 3409 / 6000\n",
      "training loss = 0.6807\n",
      "training accuracy = 0.5682\n",
      "num_test_corrects / test_total_examples = 3205 / 6000\n",
      "testing accuracy = 0.5342\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 3920 / 6000\n",
      "training loss = 0.6525\n",
      "training accuracy = 0.6533\n",
      "num_test_corrects / test_total_examples = 3618 / 6000\n",
      "testing accuracy = 0.6030\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 4116 / 6000\n",
      "training loss = 0.6310\n",
      "training accuracy = 0.6860\n",
      "num_test_corrects / test_total_examples = 3933 / 6000\n",
      "testing accuracy = 0.6555\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 4225 / 6000\n",
      "training loss = 0.6140\n",
      "training accuracy = 0.7042\n",
      "num_test_corrects / test_total_examples = 3998 / 6000\n",
      "testing accuracy = 0.6663\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 4285 / 6000\n",
      "training loss = 0.6003\n",
      "training accuracy = 0.7142\n",
      "num_test_corrects / test_total_examples = 4068 / 6000\n",
      "testing accuracy = 0.6780\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 4299 / 6000\n",
      "training loss = 0.5890\n",
      "training accuracy = 0.7165\n",
      "num_test_corrects / test_total_examples = 4100 / 6000\n",
      "testing accuracy = 0.6833\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 4310 / 6000\n",
      "training loss = 0.5796\n",
      "training accuracy = 0.7183\n",
      "num_test_corrects / test_total_examples = 4124 / 6000\n",
      "testing accuracy = 0.6873\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 4324 / 6000\n",
      "training loss = 0.5716\n",
      "training accuracy = 0.7207\n",
      "num_test_corrects / test_total_examples = 4147 / 6000\n",
      "testing accuracy = 0.6912\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 4328 / 6000\n",
      "training loss = 0.5648\n",
      "training accuracy = 0.7213\n",
      "num_test_corrects / test_total_examples = 4163 / 6000\n",
      "testing accuracy = 0.6938\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 4344 / 6000\n",
      "training loss = 0.5589\n",
      "training accuracy = 0.7240\n",
      "num_test_corrects / test_total_examples = 4186 / 6000\n",
      "testing accuracy = 0.6977\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 4357 / 6000\n",
      "training loss = 0.5537\n",
      "training accuracy = 0.7262\n",
      "num_test_corrects / test_total_examples = 4210 / 6000\n",
      "testing accuracy = 0.7017\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 4375 / 6000\n",
      "training loss = 0.5491\n",
      "training accuracy = 0.7292\n",
      "num_test_corrects / test_total_examples = 4226 / 6000\n",
      "testing accuracy = 0.7043\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 4391 / 6000\n",
      "training loss = 0.5450\n",
      "training accuracy = 0.7318\n",
      "num_test_corrects / test_total_examples = 4246 / 6000\n",
      "testing accuracy = 0.7077\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 4405 / 6000\n",
      "training loss = 0.5412\n",
      "training accuracy = 0.7342\n",
      "num_test_corrects / test_total_examples = 4260 / 6000\n",
      "testing accuracy = 0.7100\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 4419 / 6000\n",
      "training loss = 0.5378\n",
      "training accuracy = 0.7365\n",
      "num_test_corrects / test_total_examples = 4264 / 6000\n",
      "testing accuracy = 0.7107\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 4432 / 6000\n",
      "training loss = 0.5347\n",
      "training accuracy = 0.7387\n",
      "num_test_corrects / test_total_examples = 4277 / 6000\n",
      "testing accuracy = 0.7128\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 4449 / 6000\n",
      "training loss = 0.5318\n",
      "training accuracy = 0.7415\n",
      "num_test_corrects / test_total_examples = 4292 / 6000\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 4461 / 6000\n",
      "training loss = 0.5292\n",
      "training accuracy = 0.7435\n",
      "num_test_corrects / test_total_examples = 4301 / 6000\n",
      "testing accuracy = 0.7168\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 4474 / 6000\n",
      "training loss = 0.5267\n",
      "training accuracy = 0.7457\n",
      "num_test_corrects / test_total_examples = 4304 / 6000\n",
      "testing accuracy = 0.7173\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 4481 / 6000\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7468\n",
      "num_test_corrects / test_total_examples = 4305 / 6000\n",
      "testing accuracy = 0.7175\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 4483 / 6000\n",
      "training loss = 0.5222\n",
      "training accuracy = 0.7472\n",
      "num_test_corrects / test_total_examples = 4311 / 6000\n",
      "testing accuracy = 0.7185\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 4492 / 6000\n",
      "training loss = 0.5201\n",
      "training accuracy = 0.7487\n",
      "num_test_corrects / test_total_examples = 4316 / 6000\n",
      "testing accuracy = 0.7193\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 4496 / 6000\n",
      "training loss = 0.5181\n",
      "training accuracy = 0.7493\n",
      "num_test_corrects / test_total_examples = 4325 / 6000\n",
      "testing accuracy = 0.7208\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 4505 / 6000\n",
      "training loss = 0.5163\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 4337 / 6000\n",
      "testing accuracy = 0.7228\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 4514 / 6000\n",
      "training loss = 0.5145\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 4346 / 6000\n",
      "testing accuracy = 0.7243\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 4531 / 6000\n",
      "training loss = 0.5128\n",
      "training accuracy = 0.7552\n",
      "num_test_corrects / test_total_examples = 4353 / 6000\n",
      "testing accuracy = 0.7255\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 4540 / 6000\n",
      "training loss = 0.5112\n",
      "training accuracy = 0.7567\n",
      "num_test_corrects / test_total_examples = 4356 / 6000\n",
      "testing accuracy = 0.7260\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 4546 / 6000\n",
      "training loss = 0.5097\n",
      "training accuracy = 0.7577\n",
      "num_test_corrects / test_total_examples = 4367 / 6000\n",
      "testing accuracy = 0.7278\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 4557 / 6000\n",
      "training loss = 0.5082\n",
      "training accuracy = 0.7595\n",
      "num_test_corrects / test_total_examples = 4375 / 6000\n",
      "testing accuracy = 0.7292\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 4559 / 6000\n",
      "training loss = 0.5067\n",
      "training accuracy = 0.7598\n",
      "num_test_corrects / test_total_examples = 4383 / 6000\n",
      "testing accuracy = 0.7305\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 4560 / 6000\n",
      "training loss = 0.5054\n",
      "training accuracy = 0.7600\n",
      "num_test_corrects / test_total_examples = 4389 / 6000\n",
      "testing accuracy = 0.7315\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 4565 / 6000\n",
      "training loss = 0.5040\n",
      "training accuracy = 0.7608\n",
      "num_test_corrects / test_total_examples = 4405 / 6000\n",
      "testing accuracy = 0.7342\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 4579 / 6000\n",
      "training loss = 0.5027\n",
      "training accuracy = 0.7632\n",
      "num_test_corrects / test_total_examples = 4410 / 6000\n",
      "testing accuracy = 0.7350\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 4584 / 6000\n",
      "training loss = 0.5015\n",
      "training accuracy = 0.7640\n",
      "num_test_corrects / test_total_examples = 4419 / 6000\n",
      "testing accuracy = 0.7365\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 4585 / 6000\n",
      "training loss = 0.5003\n",
      "training accuracy = 0.7642\n",
      "num_test_corrects / test_total_examples = 4425 / 6000\n",
      "testing accuracy = 0.7375\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 4593 / 6000\n",
      "training loss = 0.4991\n",
      "training accuracy = 0.7655\n",
      "num_test_corrects / test_total_examples = 4431 / 6000\n",
      "testing accuracy = 0.7385\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 4595 / 6000\n",
      "training loss = 0.4980\n",
      "training accuracy = 0.7658\n",
      "num_test_corrects / test_total_examples = 4433 / 6000\n",
      "testing accuracy = 0.7388\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 4596 / 6000\n",
      "training loss = 0.4969\n",
      "training accuracy = 0.7660\n",
      "num_test_corrects / test_total_examples = 4435 / 6000\n",
      "testing accuracy = 0.7392\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 4604 / 6000\n",
      "training loss = 0.4958\n",
      "training accuracy = 0.7673\n",
      "num_test_corrects / test_total_examples = 4440 / 6000\n",
      "testing accuracy = 0.7400\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 4608 / 6000\n",
      "training loss = 0.4948\n",
      "training accuracy = 0.7680\n",
      "num_test_corrects / test_total_examples = 4446 / 6000\n",
      "testing accuracy = 0.7410\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 4608 / 6000\n",
      "training loss = 0.4938\n",
      "training accuracy = 0.7680\n",
      "num_test_corrects / test_total_examples = 4457 / 6000\n",
      "testing accuracy = 0.7428\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 4612 / 6000\n",
      "training loss = 0.4928\n",
      "training accuracy = 0.7687\n",
      "num_test_corrects / test_total_examples = 4462 / 6000\n",
      "testing accuracy = 0.7437\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 4621 / 6000\n",
      "training loss = 0.4918\n",
      "training accuracy = 0.7702\n",
      "num_test_corrects / test_total_examples = 4466 / 6000\n",
      "testing accuracy = 0.7443\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 4623 / 6000\n",
      "training loss = 0.4909\n",
      "training accuracy = 0.7705\n",
      "num_test_corrects / test_total_examples = 4469 / 6000\n",
      "testing accuracy = 0.7448\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 4626 / 6000\n",
      "training loss = 0.4900\n",
      "training accuracy = 0.7710\n",
      "num_test_corrects / test_total_examples = 4477 / 6000\n",
      "testing accuracy = 0.7462\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 4628 / 6000\n",
      "training loss = 0.4891\n",
      "training accuracy = 0.7713\n",
      "num_test_corrects / test_total_examples = 4482 / 6000\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 4632 / 6000\n",
      "training loss = 0.4882\n",
      "training accuracy = 0.7720\n",
      "num_test_corrects / test_total_examples = 4483 / 6000\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 4635 / 6000\n",
      "training loss = 0.4874\n",
      "training accuracy = 0.7725\n",
      "num_test_corrects / test_total_examples = 4491 / 6000\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 4642 / 6000\n",
      "training loss = 0.4865\n",
      "training accuracy = 0.7737\n",
      "num_test_corrects / test_total_examples = 4495 / 6000\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 4646 / 6000\n",
      "training loss = 0.4857\n",
      "training accuracy = 0.7743\n",
      "num_test_corrects / test_total_examples = 4498 / 6000\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 4653 / 6000\n",
      "training loss = 0.4849\n",
      "training accuracy = 0.7755\n",
      "num_test_corrects / test_total_examples = 4502 / 6000\n",
      "testing accuracy = 0.7503\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 4654 / 6000\n",
      "training loss = 0.4841\n",
      "training accuracy = 0.7757\n",
      "num_test_corrects / test_total_examples = 4505 / 6000\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 4655 / 6000\n",
      "training loss = 0.4834\n",
      "training accuracy = 0.7758\n",
      "num_test_corrects / test_total_examples = 4505 / 6000\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 4659 / 6000\n",
      "training loss = 0.4826\n",
      "training accuracy = 0.7765\n",
      "num_test_corrects / test_total_examples = 4507 / 6000\n",
      "testing accuracy = 0.7512\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 4665 / 6000\n",
      "training loss = 0.4819\n",
      "training accuracy = 0.7775\n",
      "num_test_corrects / test_total_examples = 4512 / 6000\n",
      "testing accuracy = 0.7520\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 4665 / 6000\n",
      "training loss = 0.4812\n",
      "training accuracy = 0.7775\n",
      "num_test_corrects / test_total_examples = 4513 / 6000\n",
      "testing accuracy = 0.7522\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 4669 / 6000\n",
      "training loss = 0.4805\n",
      "training accuracy = 0.7782\n",
      "num_test_corrects / test_total_examples = 4516 / 6000\n",
      "testing accuracy = 0.7527\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 4671 / 6000\n",
      "training loss = 0.4798\n",
      "training accuracy = 0.7785\n",
      "num_test_corrects / test_total_examples = 4524 / 6000\n",
      "testing accuracy = 0.7540\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 4675 / 6000\n",
      "training loss = 0.4791\n",
      "training accuracy = 0.7792\n",
      "num_test_corrects / test_total_examples = 4528 / 6000\n",
      "testing accuracy = 0.7547\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 4675 / 6000\n",
      "training loss = 0.4784\n",
      "training accuracy = 0.7792\n",
      "num_test_corrects / test_total_examples = 4529 / 6000\n",
      "testing accuracy = 0.7548\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 4677 / 6000\n",
      "training loss = 0.4777\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 4532 / 6000\n",
      "testing accuracy = 0.7553\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 4679 / 6000\n",
      "training loss = 0.4771\n",
      "training accuracy = 0.7798\n",
      "num_test_corrects / test_total_examples = 4535 / 6000\n",
      "testing accuracy = 0.7558\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 4678 / 6000\n",
      "training loss = 0.4765\n",
      "training accuracy = 0.7797\n",
      "num_test_corrects / test_total_examples = 4540 / 6000\n",
      "testing accuracy = 0.7567\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 4677 / 6000\n",
      "training loss = 0.4758\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 4545 / 6000\n",
      "testing accuracy = 0.7575\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 4677 / 6000\n",
      "training loss = 0.4752\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 4549 / 6000\n",
      "testing accuracy = 0.7582\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 4677 / 6000\n",
      "training loss = 0.4746\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 4554 / 6000\n",
      "testing accuracy = 0.7590\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 4681 / 6000\n",
      "training loss = 0.4740\n",
      "training accuracy = 0.7802\n",
      "num_test_corrects / test_total_examples = 4556 / 6000\n",
      "testing accuracy = 0.7593\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 4684 / 6000\n",
      "training loss = 0.4734\n",
      "training accuracy = 0.7807\n",
      "num_test_corrects / test_total_examples = 4565 / 6000\n",
      "testing accuracy = 0.7608\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 4688 / 6000\n",
      "training loss = 0.4729\n",
      "training accuracy = 0.7813\n",
      "num_test_corrects / test_total_examples = 4567 / 6000\n",
      "testing accuracy = 0.7612\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 4685 / 6000\n",
      "training loss = 0.4723\n",
      "training accuracy = 0.7808\n",
      "num_test_corrects / test_total_examples = 4567 / 6000\n",
      "testing accuracy = 0.7612\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 4686 / 6000\n",
      "training loss = 0.4717\n",
      "training accuracy = 0.7810\n",
      "num_test_corrects / test_total_examples = 4571 / 6000\n",
      "testing accuracy = 0.7618\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 4686 / 6000\n",
      "training loss = 0.4712\n",
      "training accuracy = 0.7810\n",
      "num_test_corrects / test_total_examples = 4575 / 6000\n",
      "testing accuracy = 0.7625\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 4687 / 6000\n",
      "training loss = 0.4707\n",
      "training accuracy = 0.7812\n",
      "num_test_corrects / test_total_examples = 4578 / 6000\n",
      "testing accuracy = 0.7630\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 4690 / 6000\n",
      "training loss = 0.4701\n",
      "training accuracy = 0.7817\n",
      "num_test_corrects / test_total_examples = 4580 / 6000\n",
      "testing accuracy = 0.7633\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 4693 / 6000\n",
      "training loss = 0.4696\n",
      "training accuracy = 0.7822\n",
      "num_test_corrects / test_total_examples = 4586 / 6000\n",
      "testing accuracy = 0.7643\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 4694 / 6000\n",
      "training loss = 0.4691\n",
      "training accuracy = 0.7823\n",
      "num_test_corrects / test_total_examples = 4586 / 6000\n",
      "testing accuracy = 0.7643\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 4699 / 6000\n",
      "training loss = 0.4686\n",
      "training accuracy = 0.7832\n",
      "num_test_corrects / test_total_examples = 4590 / 6000\n",
      "testing accuracy = 0.7650\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 4700 / 6000\n",
      "training loss = 0.4681\n",
      "training accuracy = 0.7833\n",
      "num_test_corrects / test_total_examples = 4595 / 6000\n",
      "testing accuracy = 0.7658\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 4700 / 6000\n",
      "training loss = 0.4676\n",
      "training accuracy = 0.7833\n",
      "num_test_corrects / test_total_examples = 4598 / 6000\n",
      "testing accuracy = 0.7663\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 4701 / 6000\n",
      "training loss = 0.4671\n",
      "training accuracy = 0.7835\n",
      "num_test_corrects / test_total_examples = 4600 / 6000\n",
      "testing accuracy = 0.7667\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 4704 / 6000\n",
      "training loss = 0.4666\n",
      "training accuracy = 0.7840\n",
      "num_test_corrects / test_total_examples = 4603 / 6000\n",
      "testing accuracy = 0.7672\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 4706 / 6000\n",
      "training loss = 0.4661\n",
      "training accuracy = 0.7843\n",
      "num_test_corrects / test_total_examples = 4606 / 6000\n",
      "testing accuracy = 0.7677\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 4707 / 6000\n",
      "training loss = 0.4657\n",
      "training accuracy = 0.7845\n",
      "num_test_corrects / test_total_examples = 4613 / 6000\n",
      "testing accuracy = 0.7688\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 4708 / 6000\n",
      "training loss = 0.4652\n",
      "training accuracy = 0.7847\n",
      "num_test_corrects / test_total_examples = 4616 / 6000\n",
      "testing accuracy = 0.7693\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 4708 / 6000\n",
      "training loss = 0.4647\n",
      "training accuracy = 0.7847\n",
      "num_test_corrects / test_total_examples = 4619 / 6000\n",
      "testing accuracy = 0.7698\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 4709 / 6000\n",
      "training loss = 0.4643\n",
      "training accuracy = 0.7848\n",
      "num_test_corrects / test_total_examples = 4622 / 6000\n",
      "testing accuracy = 0.7703\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 4713 / 6000\n",
      "training loss = 0.4639\n",
      "training accuracy = 0.7855\n",
      "num_test_corrects / test_total_examples = 4627 / 6000\n",
      "testing accuracy = 0.7712\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 4711 / 6000\n",
      "training loss = 0.4634\n",
      "training accuracy = 0.7852\n",
      "num_test_corrects / test_total_examples = 4630 / 6000\n",
      "testing accuracy = 0.7717\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 4712 / 6000\n",
      "training loss = 0.4630\n",
      "training accuracy = 0.7853\n",
      "num_test_corrects / test_total_examples = 4632 / 6000\n",
      "testing accuracy = 0.7720\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 4712 / 6000\n",
      "training loss = 0.4626\n",
      "training accuracy = 0.7853\n",
      "num_test_corrects / test_total_examples = 4634 / 6000\n",
      "testing accuracy = 0.7723\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 4713 / 6000\n",
      "training loss = 0.4621\n",
      "training accuracy = 0.7855\n",
      "num_test_corrects / test_total_examples = 4634 / 6000\n",
      "testing accuracy = 0.7723\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 4714 / 6000\n",
      "training loss = 0.4617\n",
      "training accuracy = 0.7857\n",
      "num_test_corrects / test_total_examples = 4635 / 6000\n",
      "testing accuracy = 0.7725\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 4719 / 6000\n",
      "training loss = 0.4613\n",
      "training accuracy = 0.7865\n",
      "num_test_corrects / test_total_examples = 4640 / 6000\n",
      "testing accuracy = 0.7733\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 4721 / 6000\n",
      "training loss = 0.4609\n",
      "training accuracy = 0.7868\n",
      "num_test_corrects / test_total_examples = 4640 / 6000\n",
      "testing accuracy = 0.7733\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 4725 / 6000\n",
      "training loss = 0.4605\n",
      "training accuracy = 0.7875\n",
      "num_test_corrects / test_total_examples = 4640 / 6000\n",
      "testing accuracy = 0.7733\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 4731 / 6000\n",
      "training loss = 0.4601\n",
      "training accuracy = 0.7885\n",
      "num_test_corrects / test_total_examples = 4643 / 6000\n",
      "testing accuracy = 0.7738\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 4734 / 6000\n",
      "training loss = 0.4597\n",
      "training accuracy = 0.7890\n",
      "num_test_corrects / test_total_examples = 4648 / 6000\n",
      "testing accuracy = 0.7747\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 4732 / 6000\n",
      "training loss = 0.4593\n",
      "training accuracy = 0.7887\n",
      "num_test_corrects / test_total_examples = 4651 / 6000\n",
      "testing accuracy = 0.7752\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 4735 / 6000\n",
      "training loss = 0.4589\n",
      "training accuracy = 0.7892\n",
      "num_test_corrects / test_total_examples = 4654 / 6000\n",
      "testing accuracy = 0.7757\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 4740 / 6000\n",
      "training loss = 0.4586\n",
      "training accuracy = 0.7900\n",
      "num_test_corrects / test_total_examples = 4657 / 6000\n",
      "testing accuracy = 0.7762\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 4745 / 6000\n",
      "training loss = 0.4582\n",
      "training accuracy = 0.7908\n",
      "num_test_corrects / test_total_examples = 4658 / 6000\n",
      "testing accuracy = 0.7763\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 4745 / 6000\n",
      "training loss = 0.4578\n",
      "training accuracy = 0.7908\n",
      "num_test_corrects / test_total_examples = 4662 / 6000\n",
      "testing accuracy = 0.7770\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 4747 / 6000\n",
      "training loss = 0.4574\n",
      "training accuracy = 0.7912\n",
      "num_test_corrects / test_total_examples = 4664 / 6000\n",
      "testing accuracy = 0.7773\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 4748 / 6000\n",
      "training loss = 0.4571\n",
      "training accuracy = 0.7913\n",
      "num_test_corrects / test_total_examples = 4667 / 6000\n",
      "testing accuracy = 0.7778\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 4749 / 6000\n",
      "training loss = 0.4567\n",
      "training accuracy = 0.7915\n",
      "num_test_corrects / test_total_examples = 4667 / 6000\n",
      "testing accuracy = 0.7778\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 4753 / 6000\n",
      "training loss = 0.4564\n",
      "training accuracy = 0.7922\n",
      "num_test_corrects / test_total_examples = 4666 / 6000\n",
      "testing accuracy = 0.7777\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 4756 / 6000\n",
      "training loss = 0.4560\n",
      "training accuracy = 0.7927\n",
      "num_test_corrects / test_total_examples = 4669 / 6000\n",
      "testing accuracy = 0.7782\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 4753 / 6000\n",
      "training loss = 0.4557\n",
      "training accuracy = 0.7922\n",
      "num_test_corrects / test_total_examples = 4669 / 6000\n",
      "testing accuracy = 0.7782\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 4755 / 6000\n",
      "training loss = 0.4553\n",
      "training accuracy = 0.7925\n",
      "num_test_corrects / test_total_examples = 4671 / 6000\n",
      "testing accuracy = 0.7785\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 4755 / 6000\n",
      "training loss = 0.4550\n",
      "training accuracy = 0.7925\n",
      "num_test_corrects / test_total_examples = 4672 / 6000\n",
      "testing accuracy = 0.7787\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 4756 / 6000\n",
      "training loss = 0.4547\n",
      "training accuracy = 0.7927\n",
      "num_test_corrects / test_total_examples = 4673 / 6000\n",
      "testing accuracy = 0.7788\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 4756 / 6000\n",
      "training loss = 0.4543\n",
      "training accuracy = 0.7927\n",
      "num_test_corrects / test_total_examples = 4676 / 6000\n",
      "testing accuracy = 0.7793\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 4755 / 6000\n",
      "training loss = 0.4540\n",
      "training accuracy = 0.7925\n",
      "num_test_corrects / test_total_examples = 4677 / 6000\n",
      "testing accuracy = 0.7795\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 4758 / 6000\n",
      "training loss = 0.4537\n",
      "training accuracy = 0.7930\n",
      "num_test_corrects / test_total_examples = 4679 / 6000\n",
      "testing accuracy = 0.7798\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 4760 / 6000\n",
      "training loss = 0.4534\n",
      "training accuracy = 0.7933\n",
      "num_test_corrects / test_total_examples = 4682 / 6000\n",
      "testing accuracy = 0.7803\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 4760 / 6000\n",
      "training loss = 0.4530\n",
      "training accuracy = 0.7933\n",
      "num_test_corrects / test_total_examples = 4682 / 6000\n",
      "testing accuracy = 0.7803\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 4759 / 6000\n",
      "training loss = 0.4527\n",
      "training accuracy = 0.7932\n",
      "num_test_corrects / test_total_examples = 4687 / 6000\n",
      "testing accuracy = 0.7812\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 4760 / 6000\n",
      "training loss = 0.4524\n",
      "training accuracy = 0.7933\n",
      "num_test_corrects / test_total_examples = 4689 / 6000\n",
      "testing accuracy = 0.7815\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 4764 / 6000\n",
      "training loss = 0.4521\n",
      "training accuracy = 0.7940\n",
      "num_test_corrects / test_total_examples = 4692 / 6000\n",
      "testing accuracy = 0.7820\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 4766 / 6000\n",
      "training loss = 0.4518\n",
      "training accuracy = 0.7943\n",
      "num_test_corrects / test_total_examples = 4695 / 6000\n",
      "testing accuracy = 0.7825\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 4766 / 6000\n",
      "training loss = 0.4515\n",
      "training accuracy = 0.7943\n",
      "num_test_corrects / test_total_examples = 4699 / 6000\n",
      "testing accuracy = 0.7832\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 4766 / 6000\n",
      "training loss = 0.4512\n",
      "training accuracy = 0.7943\n",
      "num_test_corrects / test_total_examples = 4698 / 6000\n",
      "testing accuracy = 0.7830\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 4767 / 6000\n",
      "training loss = 0.4509\n",
      "training accuracy = 0.7945\n",
      "num_test_corrects / test_total_examples = 4700 / 6000\n",
      "testing accuracy = 0.7833\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 4767 / 6000\n",
      "training loss = 0.4506\n",
      "training accuracy = 0.7945\n",
      "num_test_corrects / test_total_examples = 4699 / 6000\n",
      "testing accuracy = 0.7832\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 4768 / 6000\n",
      "training loss = 0.4503\n",
      "training accuracy = 0.7947\n",
      "num_test_corrects / test_total_examples = 4701 / 6000\n",
      "testing accuracy = 0.7835\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 4770 / 6000\n",
      "training loss = 0.4500\n",
      "training accuracy = 0.7950\n",
      "num_test_corrects / test_total_examples = 4702 / 6000\n",
      "testing accuracy = 0.7837\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 4769 / 6000\n",
      "training loss = 0.4497\n",
      "training accuracy = 0.7948\n",
      "num_test_corrects / test_total_examples = 4702 / 6000\n",
      "testing accuracy = 0.7837\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 4769 / 6000\n",
      "training loss = 0.4495\n",
      "training accuracy = 0.7948\n",
      "num_test_corrects / test_total_examples = 4704 / 6000\n",
      "testing accuracy = 0.7840\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 4768 / 6000\n",
      "training loss = 0.4492\n",
      "training accuracy = 0.7947\n",
      "num_test_corrects / test_total_examples = 4704 / 6000\n",
      "testing accuracy = 0.7840\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 4771 / 6000\n",
      "training loss = 0.4489\n",
      "training accuracy = 0.7952\n",
      "num_test_corrects / test_total_examples = 4708 / 6000\n",
      "testing accuracy = 0.7847\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 4772 / 6000\n",
      "training loss = 0.4486\n",
      "training accuracy = 0.7953\n",
      "num_test_corrects / test_total_examples = 4709 / 6000\n",
      "testing accuracy = 0.7848\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 4775 / 6000\n",
      "training loss = 0.4484\n",
      "training accuracy = 0.7958\n",
      "num_test_corrects / test_total_examples = 4715 / 6000\n",
      "testing accuracy = 0.7858\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 4775 / 6000\n",
      "training loss = 0.4481\n",
      "training accuracy = 0.7958\n",
      "num_test_corrects / test_total_examples = 4716 / 6000\n",
      "testing accuracy = 0.7860\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 4776 / 6000\n",
      "training loss = 0.4478\n",
      "training accuracy = 0.7960\n",
      "num_test_corrects / test_total_examples = 4717 / 6000\n",
      "testing accuracy = 0.7862\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 4778 / 6000\n",
      "training loss = 0.4476\n",
      "training accuracy = 0.7963\n",
      "num_test_corrects / test_total_examples = 4718 / 6000\n",
      "testing accuracy = 0.7863\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 4778 / 6000\n",
      "training loss = 0.4473\n",
      "training accuracy = 0.7963\n",
      "num_test_corrects / test_total_examples = 4720 / 6000\n",
      "testing accuracy = 0.7867\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 4780 / 6000\n",
      "training loss = 0.4470\n",
      "training accuracy = 0.7967\n",
      "num_test_corrects / test_total_examples = 4720 / 6000\n",
      "testing accuracy = 0.7867\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 4781 / 6000\n",
      "training loss = 0.4468\n",
      "training accuracy = 0.7968\n",
      "num_test_corrects / test_total_examples = 4722 / 6000\n",
      "testing accuracy = 0.7870\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 4782 / 6000\n",
      "training loss = 0.4465\n",
      "training accuracy = 0.7970\n",
      "num_test_corrects / test_total_examples = 4721 / 6000\n",
      "testing accuracy = 0.7868\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 4783 / 6000\n",
      "training loss = 0.4463\n",
      "training accuracy = 0.7972\n",
      "num_test_corrects / test_total_examples = 4725 / 6000\n",
      "testing accuracy = 0.7875\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 4785 / 6000\n",
      "training loss = 0.4460\n",
      "training accuracy = 0.7975\n",
      "num_test_corrects / test_total_examples = 4727 / 6000\n",
      "testing accuracy = 0.7878\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 4787 / 6000\n",
      "training loss = 0.4458\n",
      "training accuracy = 0.7978\n",
      "num_test_corrects / test_total_examples = 4730 / 6000\n",
      "testing accuracy = 0.7883\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 4788 / 6000\n",
      "training loss = 0.4455\n",
      "training accuracy = 0.7980\n",
      "num_test_corrects / test_total_examples = 4731 / 6000\n",
      "testing accuracy = 0.7885\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 4790 / 6000\n",
      "training loss = 0.4453\n",
      "training accuracy = 0.7983\n",
      "num_test_corrects / test_total_examples = 4732 / 6000\n",
      "testing accuracy = 0.7887\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 4790 / 6000\n",
      "training loss = 0.4450\n",
      "training accuracy = 0.7983\n",
      "num_test_corrects / test_total_examples = 4734 / 6000\n",
      "testing accuracy = 0.7890\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 4791 / 6000\n",
      "training loss = 0.4448\n",
      "training accuracy = 0.7985\n",
      "num_test_corrects / test_total_examples = 4737 / 6000\n",
      "testing accuracy = 0.7895\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 4795 / 6000\n",
      "training loss = 0.4446\n",
      "training accuracy = 0.7992\n",
      "num_test_corrects / test_total_examples = 4736 / 6000\n",
      "testing accuracy = 0.7893\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 4796 / 6000\n",
      "training loss = 0.4443\n",
      "training accuracy = 0.7993\n",
      "num_test_corrects / test_total_examples = 4737 / 6000\n",
      "testing accuracy = 0.7895\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 4798 / 6000\n",
      "training loss = 0.4441\n",
      "training accuracy = 0.7997\n",
      "num_test_corrects / test_total_examples = 4737 / 6000\n",
      "testing accuracy = 0.7895\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 4797 / 6000\n",
      "training loss = 0.4439\n",
      "training accuracy = 0.7995\n",
      "num_test_corrects / test_total_examples = 4737 / 6000\n",
      "testing accuracy = 0.7895\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 4798 / 6000\n",
      "training loss = 0.4436\n",
      "training accuracy = 0.7997\n",
      "num_test_corrects / test_total_examples = 4737 / 6000\n",
      "testing accuracy = 0.7895\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 4798 / 6000\n",
      "training loss = 0.4434\n",
      "training accuracy = 0.7997\n",
      "num_test_corrects / test_total_examples = 4738 / 6000\n",
      "testing accuracy = 0.7897\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 4799 / 6000\n",
      "training loss = 0.4432\n",
      "training accuracy = 0.7998\n",
      "num_test_corrects / test_total_examples = 4739 / 6000\n",
      "testing accuracy = 0.7898\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 4799 / 6000\n",
      "training loss = 0.4430\n",
      "training accuracy = 0.7998\n",
      "num_test_corrects / test_total_examples = 4740 / 6000\n",
      "testing accuracy = 0.7900\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 4800 / 6000\n",
      "training loss = 0.4427\n",
      "training accuracy = 0.8000\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 4800 / 6000\n",
      "training loss = 0.4425\n",
      "training accuracy = 0.8000\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 4799 / 6000\n",
      "training loss = 0.4423\n",
      "training accuracy = 0.7998\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 4799 / 6000\n",
      "training loss = 0.4421\n",
      "training accuracy = 0.7998\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 4800 / 6000\n",
      "training loss = 0.4419\n",
      "training accuracy = 0.8000\n",
      "num_test_corrects / test_total_examples = 4742 / 6000\n",
      "testing accuracy = 0.7903\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 4801 / 6000\n",
      "training loss = 0.4417\n",
      "training accuracy = 0.8002\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 4802 / 6000\n",
      "training loss = 0.4414\n",
      "training accuracy = 0.8003\n",
      "num_test_corrects / test_total_examples = 4740 / 6000\n",
      "testing accuracy = 0.7900\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 4805 / 6000\n",
      "training loss = 0.4412\n",
      "training accuracy = 0.8008\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 4805 / 6000\n",
      "training loss = 0.4410\n",
      "training accuracy = 0.8008\n",
      "num_test_corrects / test_total_examples = 4740 / 6000\n",
      "testing accuracy = 0.7900\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 4806 / 6000\n",
      "training loss = 0.4408\n",
      "training accuracy = 0.8010\n",
      "num_test_corrects / test_total_examples = 4740 / 6000\n",
      "testing accuracy = 0.7900\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 4806 / 6000\n",
      "training loss = 0.4406\n",
      "training accuracy = 0.8010\n",
      "num_test_corrects / test_total_examples = 4740 / 6000\n",
      "testing accuracy = 0.7900\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 4807 / 6000\n",
      "training loss = 0.4404\n",
      "training accuracy = 0.8012\n",
      "num_test_corrects / test_total_examples = 4741 / 6000\n",
      "testing accuracy = 0.7902\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 4806 / 6000\n",
      "training loss = 0.4402\n",
      "training accuracy = 0.8010\n",
      "num_test_corrects / test_total_examples = 4742 / 6000\n",
      "testing accuracy = 0.7903\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 4805 / 6000\n",
      "training loss = 0.4400\n",
      "training accuracy = 0.8008\n",
      "num_test_corrects / test_total_examples = 4745 / 6000\n",
      "testing accuracy = 0.7908\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 4806 / 6000\n",
      "training loss = 0.4398\n",
      "training accuracy = 0.8010\n",
      "num_test_corrects / test_total_examples = 4746 / 6000\n",
      "testing accuracy = 0.7910\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 4808 / 6000\n",
      "training loss = 0.4396\n",
      "training accuracy = 0.8013\n",
      "num_test_corrects / test_total_examples = 4746 / 6000\n",
      "testing accuracy = 0.7910\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 4807 / 6000\n",
      "training loss = 0.4394\n",
      "training accuracy = 0.8012\n",
      "num_test_corrects / test_total_examples = 4744 / 6000\n",
      "testing accuracy = 0.7907\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 4810 / 6000\n",
      "training loss = 0.4392\n",
      "training accuracy = 0.8017\n",
      "num_test_corrects / test_total_examples = 4747 / 6000\n",
      "testing accuracy = 0.7912\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 4810 / 6000\n",
      "training loss = 0.4390\n",
      "training accuracy = 0.8017\n",
      "num_test_corrects / test_total_examples = 4747 / 6000\n",
      "testing accuracy = 0.7912\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 4811 / 6000\n",
      "training loss = 0.4388\n",
      "training accuracy = 0.8018\n",
      "num_test_corrects / test_total_examples = 4749 / 6000\n",
      "testing accuracy = 0.7915\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 4811 / 6000\n",
      "training loss = 0.4386\n",
      "training accuracy = 0.8018\n",
      "num_test_corrects / test_total_examples = 4749 / 6000\n",
      "testing accuracy = 0.7915\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 4813 / 6000\n",
      "training loss = 0.4385\n",
      "training accuracy = 0.8022\n",
      "num_test_corrects / test_total_examples = 4751 / 6000\n",
      "testing accuracy = 0.7918\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 4815 / 6000\n",
      "training loss = 0.4383\n",
      "training accuracy = 0.8025\n",
      "num_test_corrects / test_total_examples = 4755 / 6000\n",
      "testing accuracy = 0.7925\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 4816 / 6000\n",
      "training loss = 0.4381\n",
      "training accuracy = 0.8027\n",
      "num_test_corrects / test_total_examples = 4756 / 6000\n",
      "testing accuracy = 0.7927\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 4816 / 6000\n",
      "training loss = 0.4379\n",
      "training accuracy = 0.8027\n",
      "num_test_corrects / test_total_examples = 4756 / 6000\n",
      "testing accuracy = 0.7927\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 4816 / 6000\n",
      "training loss = 0.4377\n",
      "training accuracy = 0.8027\n",
      "num_test_corrects / test_total_examples = 4756 / 6000\n",
      "testing accuracy = 0.7927\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 4818 / 6000\n",
      "training loss = 0.4375\n",
      "training accuracy = 0.8030\n",
      "num_test_corrects / test_total_examples = 4756 / 6000\n",
      "testing accuracy = 0.7927\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 4820 / 6000\n",
      "training loss = 0.4373\n",
      "training accuracy = 0.8033\n",
      "num_test_corrects / test_total_examples = 4757 / 6000\n",
      "testing accuracy = 0.7928\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 4821 / 6000\n",
      "training loss = 0.4372\n",
      "training accuracy = 0.8035\n",
      "num_test_corrects / test_total_examples = 4759 / 6000\n",
      "testing accuracy = 0.7932\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 4824 / 6000\n",
      "training loss = 0.4370\n",
      "training accuracy = 0.8040\n",
      "num_test_corrects / test_total_examples = 4761 / 6000\n",
      "testing accuracy = 0.7935\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 4824 / 6000\n",
      "training loss = 0.4368\n",
      "training accuracy = 0.8040\n",
      "num_test_corrects / test_total_examples = 4762 / 6000\n",
      "testing accuracy = 0.7937\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 4824 / 6000\n",
      "training loss = 0.4366\n",
      "training accuracy = 0.8040\n",
      "num_test_corrects / test_total_examples = 4762 / 6000\n",
      "testing accuracy = 0.7937\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 4825 / 6000\n",
      "training loss = 0.4365\n",
      "training accuracy = 0.8042\n",
      "num_test_corrects / test_total_examples = 4762 / 6000\n",
      "testing accuracy = 0.7937\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 4824 / 6000\n",
      "training loss = 0.4363\n",
      "training accuracy = 0.8040\n",
      "num_test_corrects / test_total_examples = 4763 / 6000\n",
      "testing accuracy = 0.7938\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 4824 / 6000\n",
      "training loss = 0.4361\n",
      "training accuracy = 0.8040\n",
      "num_test_corrects / test_total_examples = 4764 / 6000\n",
      "testing accuracy = 0.7940\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 4823 / 6000\n",
      "training loss = 0.4359\n",
      "training accuracy = 0.8038\n",
      "num_test_corrects / test_total_examples = 4765 / 6000\n",
      "testing accuracy = 0.7942\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 4825 / 6000\n",
      "training loss = 0.4358\n",
      "training accuracy = 0.8042\n",
      "num_test_corrects / test_total_examples = 4766 / 6000\n",
      "testing accuracy = 0.7943\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 4826 / 6000\n",
      "training loss = 0.4356\n",
      "training accuracy = 0.8043\n",
      "num_test_corrects / test_total_examples = 4767 / 6000\n",
      "testing accuracy = 0.7945\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 4829 / 6000\n",
      "training loss = 0.4354\n",
      "training accuracy = 0.8048\n",
      "num_test_corrects / test_total_examples = 4770 / 6000\n",
      "testing accuracy = 0.7950\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 4828 / 6000\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8047\n",
      "num_test_corrects / test_total_examples = 4770 / 6000\n",
      "testing accuracy = 0.7950\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 4828 / 6000\n",
      "training loss = 0.4351\n",
      "training accuracy = 0.8047\n",
      "num_test_corrects / test_total_examples = 4770 / 6000\n",
      "testing accuracy = 0.7950\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 4829 / 6000\n",
      "training loss = 0.4349\n",
      "training accuracy = 0.8048\n",
      "num_test_corrects / test_total_examples = 4771 / 6000\n",
      "testing accuracy = 0.7952\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 4829 / 6000\n",
      "training loss = 0.4348\n",
      "training accuracy = 0.8048\n",
      "num_test_corrects / test_total_examples = 4771 / 6000\n",
      "testing accuracy = 0.7952\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 4829 / 6000\n",
      "training loss = 0.4346\n",
      "training accuracy = 0.8048\n",
      "num_test_corrects / test_total_examples = 4771 / 6000\n",
      "testing accuracy = 0.7952\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 4829 / 6000\n",
      "training loss = 0.4345\n",
      "training accuracy = 0.8048\n",
      "num_test_corrects / test_total_examples = 4772 / 6000\n",
      "testing accuracy = 0.7953\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 4830 / 6000\n",
      "training loss = 0.4343\n",
      "training accuracy = 0.8050\n",
      "num_test_corrects / test_total_examples = 4774 / 6000\n",
      "testing accuracy = 0.7957\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 4831 / 6000\n",
      "training loss = 0.4341\n",
      "training accuracy = 0.8052\n",
      "num_test_corrects / test_total_examples = 4774 / 6000\n",
      "testing accuracy = 0.7957\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 4831 / 6000\n",
      "training loss = 0.4340\n",
      "training accuracy = 0.8052\n",
      "num_test_corrects / test_total_examples = 4775 / 6000\n",
      "testing accuracy = 0.7958\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 4834 / 6000\n",
      "training loss = 0.4338\n",
      "training accuracy = 0.8057\n",
      "num_test_corrects / test_total_examples = 4776 / 6000\n",
      "testing accuracy = 0.7960\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 4835 / 6000\n",
      "training loss = 0.4337\n",
      "training accuracy = 0.8058\n",
      "num_test_corrects / test_total_examples = 4778 / 6000\n",
      "testing accuracy = 0.7963\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 4837 / 6000\n",
      "training loss = 0.4335\n",
      "training accuracy = 0.8062\n",
      "num_test_corrects / test_total_examples = 4778 / 6000\n",
      "testing accuracy = 0.7963\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 4838 / 6000\n",
      "training loss = 0.4334\n",
      "training accuracy = 0.8063\n",
      "num_test_corrects / test_total_examples = 4779 / 6000\n",
      "testing accuracy = 0.7965\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 4838 / 6000\n",
      "training loss = 0.4332\n",
      "training accuracy = 0.8063\n",
      "num_test_corrects / test_total_examples = 4778 / 6000\n",
      "testing accuracy = 0.7963\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 4838 / 6000\n",
      "training loss = 0.4331\n",
      "training accuracy = 0.8063\n",
      "num_test_corrects / test_total_examples = 4779 / 6000\n",
      "testing accuracy = 0.7965\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 4837 / 6000\n",
      "training loss = 0.4329\n",
      "training accuracy = 0.8062\n",
      "num_test_corrects / test_total_examples = 4780 / 6000\n",
      "testing accuracy = 0.7967\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 4838 / 6000\n",
      "training loss = 0.4328\n",
      "training accuracy = 0.8063\n",
      "num_test_corrects / test_total_examples = 4784 / 6000\n",
      "testing accuracy = 0.7973\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 4837 / 6000\n",
      "training loss = 0.4326\n",
      "training accuracy = 0.8062\n",
      "num_test_corrects / test_total_examples = 4786 / 6000\n",
      "testing accuracy = 0.7977\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 4838 / 6000\n",
      "training loss = 0.4325\n",
      "training accuracy = 0.8063\n",
      "num_test_corrects / test_total_examples = 4790 / 6000\n",
      "testing accuracy = 0.7983\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 4839 / 6000\n",
      "training loss = 0.4323\n",
      "training accuracy = 0.8065\n",
      "num_test_corrects / test_total_examples = 4791 / 6000\n",
      "testing accuracy = 0.7985\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 4839 / 6000\n",
      "training loss = 0.4322\n",
      "training accuracy = 0.8065\n",
      "num_test_corrects / test_total_examples = 4793 / 6000\n",
      "testing accuracy = 0.7988\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 4842 / 6000\n",
      "training loss = 0.4320\n",
      "training accuracy = 0.8070\n",
      "num_test_corrects / test_total_examples = 4794 / 6000\n",
      "testing accuracy = 0.7990\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4319\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4795 / 6000\n",
      "testing accuracy = 0.7992\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 4844 / 6000\n",
      "training loss = 0.4317\n",
      "training accuracy = 0.8073\n",
      "num_test_corrects / test_total_examples = 4795 / 6000\n",
      "testing accuracy = 0.7992\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4316\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4795 / 6000\n",
      "testing accuracy = 0.7992\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 4842 / 6000\n",
      "training loss = 0.4314\n",
      "training accuracy = 0.8070\n",
      "num_test_corrects / test_total_examples = 4797 / 6000\n",
      "testing accuracy = 0.7995\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4313\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4798 / 6000\n",
      "testing accuracy = 0.7997\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4312\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4799 / 6000\n",
      "testing accuracy = 0.7998\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4310\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4800 / 6000\n",
      "testing accuracy = 0.8000\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 4843 / 6000\n",
      "training loss = 0.4309\n",
      "training accuracy = 0.8072\n",
      "num_test_corrects / test_total_examples = 4800 / 6000\n",
      "testing accuracy = 0.8000\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 4845 / 6000\n",
      "training loss = 0.4307\n",
      "training accuracy = 0.8075\n",
      "num_test_corrects / test_total_examples = 4800 / 6000\n",
      "testing accuracy = 0.8000\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 4846 / 6000\n",
      "training loss = 0.4306\n",
      "training accuracy = 0.8077\n",
      "num_test_corrects / test_total_examples = 4801 / 6000\n",
      "testing accuracy = 0.8002\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 4846 / 6000\n",
      "training loss = 0.4305\n",
      "training accuracy = 0.8077\n",
      "num_test_corrects / test_total_examples = 4801 / 6000\n",
      "testing accuracy = 0.8002\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 4847 / 6000\n",
      "training loss = 0.4303\n",
      "training accuracy = 0.8078\n",
      "num_test_corrects / test_total_examples = 4802 / 6000\n",
      "testing accuracy = 0.8003\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 4848 / 6000\n",
      "training loss = 0.4302\n",
      "training accuracy = 0.8080\n",
      "num_test_corrects / test_total_examples = 4802 / 6000\n",
      "testing accuracy = 0.8003\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 4848 / 6000\n",
      "training loss = 0.4301\n",
      "training accuracy = 0.8080\n",
      "num_test_corrects / test_total_examples = 4804 / 6000\n",
      "testing accuracy = 0.8007\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 4848 / 6000\n",
      "training loss = 0.4299\n",
      "training accuracy = 0.8080\n",
      "num_test_corrects / test_total_examples = 4804 / 6000\n",
      "testing accuracy = 0.8007\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 4849 / 6000\n",
      "training loss = 0.4298\n",
      "training accuracy = 0.8082\n",
      "num_test_corrects / test_total_examples = 4805 / 6000\n",
      "testing accuracy = 0.8008\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 4850 / 6000\n",
      "training loss = 0.4297\n",
      "training accuracy = 0.8083\n",
      "num_test_corrects / test_total_examples = 4805 / 6000\n",
      "testing accuracy = 0.8008\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 4851 / 6000\n",
      "training loss = 0.4295\n",
      "training accuracy = 0.8085\n",
      "num_test_corrects / test_total_examples = 4807 / 6000\n",
      "testing accuracy = 0.8012\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 4852 / 6000\n",
      "training loss = 0.4294\n",
      "training accuracy = 0.8087\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 4854 / 6000\n",
      "training loss = 0.4293\n",
      "training accuracy = 0.8090\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 4855 / 6000\n",
      "training loss = 0.4292\n",
      "training accuracy = 0.8092\n",
      "num_test_corrects / test_total_examples = 4809 / 6000\n",
      "testing accuracy = 0.8015\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 4856 / 6000\n",
      "training loss = 0.4290\n",
      "training accuracy = 0.8093\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 4856 / 6000\n",
      "training loss = 0.4289\n",
      "training accuracy = 0.8093\n",
      "num_test_corrects / test_total_examples = 4809 / 6000\n",
      "testing accuracy = 0.8015\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 4857 / 6000\n",
      "training loss = 0.4288\n",
      "training accuracy = 0.8095\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 4858 / 6000\n",
      "training loss = 0.4286\n",
      "training accuracy = 0.8097\n",
      "num_test_corrects / test_total_examples = 4807 / 6000\n",
      "testing accuracy = 0.8012\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 4858 / 6000\n",
      "training loss = 0.4285\n",
      "training accuracy = 0.8097\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 4858 / 6000\n",
      "training loss = 0.4284\n",
      "training accuracy = 0.8097\n",
      "num_test_corrects / test_total_examples = 4808 / 6000\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 4858 / 6000\n",
      "training loss = 0.4283\n",
      "training accuracy = 0.8097\n",
      "num_test_corrects / test_total_examples = 4809 / 6000\n",
      "testing accuracy = 0.8015\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 4857 / 6000\n",
      "training loss = 0.4281\n",
      "training accuracy = 0.8095\n",
      "num_test_corrects / test_total_examples = 4809 / 6000\n",
      "testing accuracy = 0.8015\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 4860 / 6000\n",
      "training loss = 0.4280\n",
      "training accuracy = 0.8100\n",
      "num_test_corrects / test_total_examples = 4810 / 6000\n",
      "testing accuracy = 0.8017\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 4860 / 6000\n",
      "training loss = 0.4279\n",
      "training accuracy = 0.8100\n",
      "num_test_corrects / test_total_examples = 4810 / 6000\n",
      "testing accuracy = 0.8017\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4278\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4813 / 6000\n",
      "testing accuracy = 0.8022\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4277\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4814 / 6000\n",
      "testing accuracy = 0.8023\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4275\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4815 / 6000\n",
      "testing accuracy = 0.8025\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4274\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4816 / 6000\n",
      "testing accuracy = 0.8027\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4273\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4816 / 6000\n",
      "testing accuracy = 0.8027\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 4861 / 6000\n",
      "training loss = 0.4272\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 4816 / 6000\n",
      "testing accuracy = 0.8027\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 4862 / 6000\n",
      "training loss = 0.4271\n",
      "training accuracy = 0.8103\n",
      "num_test_corrects / test_total_examples = 4818 / 6000\n",
      "testing accuracy = 0.8030\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 4862 / 6000\n",
      "training loss = 0.4269\n",
      "training accuracy = 0.8103\n",
      "num_test_corrects / test_total_examples = 4818 / 6000\n",
      "testing accuracy = 0.8030\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 4863 / 6000\n",
      "training loss = 0.4268\n",
      "training accuracy = 0.8105\n",
      "num_test_corrects / test_total_examples = 4818 / 6000\n",
      "testing accuracy = 0.8030\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 4865 / 6000\n",
      "training loss = 0.4267\n",
      "training accuracy = 0.8108\n",
      "num_test_corrects / test_total_examples = 4818 / 6000\n",
      "testing accuracy = 0.8030\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 4866 / 6000\n",
      "training loss = 0.4266\n",
      "training accuracy = 0.8110\n",
      "num_test_corrects / test_total_examples = 4817 / 6000\n",
      "testing accuracy = 0.8028\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 4867 / 6000\n",
      "training loss = 0.4265\n",
      "training accuracy = 0.8112\n",
      "num_test_corrects / test_total_examples = 4817 / 6000\n",
      "testing accuracy = 0.8028\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 4869 / 6000\n",
      "training loss = 0.4264\n",
      "training accuracy = 0.8115\n",
      "num_test_corrects / test_total_examples = 4817 / 6000\n",
      "testing accuracy = 0.8028\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 4869 / 6000\n",
      "training loss = 0.4263\n",
      "training accuracy = 0.8115\n",
      "num_test_corrects / test_total_examples = 4818 / 6000\n",
      "testing accuracy = 0.8030\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 4871 / 6000\n",
      "training loss = 0.4261\n",
      "training accuracy = 0.8118\n",
      "num_test_corrects / test_total_examples = 4821 / 6000\n",
      "testing accuracy = 0.8035\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 4871 / 6000\n",
      "training loss = 0.4260\n",
      "training accuracy = 0.8118\n",
      "num_test_corrects / test_total_examples = 4822 / 6000\n",
      "testing accuracy = 0.8037\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 4873 / 6000\n",
      "training loss = 0.4259\n",
      "training accuracy = 0.8122\n",
      "num_test_corrects / test_total_examples = 4823 / 6000\n",
      "testing accuracy = 0.8038\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 4873 / 6000\n",
      "training loss = 0.4258\n",
      "training accuracy = 0.8122\n",
      "num_test_corrects / test_total_examples = 4824 / 6000\n",
      "testing accuracy = 0.8040\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 4873 / 6000\n",
      "training loss = 0.4257\n",
      "training accuracy = 0.8122\n",
      "num_test_corrects / test_total_examples = 4824 / 6000\n",
      "testing accuracy = 0.8040\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 4875 / 6000\n",
      "training loss = 0.4256\n",
      "training accuracy = 0.8125\n",
      "num_test_corrects / test_total_examples = 4826 / 6000\n",
      "testing accuracy = 0.8043\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 4877 / 6000\n",
      "training loss = 0.4255\n",
      "training accuracy = 0.8128\n",
      "num_test_corrects / test_total_examples = 4828 / 6000\n",
      "testing accuracy = 0.8047\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 4878 / 6000\n",
      "training loss = 0.4254\n",
      "training accuracy = 0.8130\n",
      "num_test_corrects / test_total_examples = 4828 / 6000\n",
      "testing accuracy = 0.8047\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 4879 / 6000\n",
      "training loss = 0.4253\n",
      "training accuracy = 0.8132\n",
      "num_test_corrects / test_total_examples = 4831 / 6000\n",
      "testing accuracy = 0.8052\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 4880 / 6000\n",
      "training loss = 0.4252\n",
      "training accuracy = 0.8133\n",
      "num_test_corrects / test_total_examples = 4832 / 6000\n",
      "testing accuracy = 0.8053\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 4880 / 6000\n",
      "training loss = 0.4250\n",
      "training accuracy = 0.8133\n",
      "num_test_corrects / test_total_examples = 4832 / 6000\n",
      "testing accuracy = 0.8053\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 4880 / 6000\n",
      "training loss = 0.4249\n",
      "training accuracy = 0.8133\n",
      "num_test_corrects / test_total_examples = 4832 / 6000\n",
      "testing accuracy = 0.8053\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 4880 / 6000\n",
      "training loss = 0.4248\n",
      "training accuracy = 0.8133\n",
      "num_test_corrects / test_total_examples = 4834 / 6000\n",
      "testing accuracy = 0.8057\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 4881 / 6000\n",
      "training loss = 0.4247\n",
      "training accuracy = 0.8135\n",
      "num_test_corrects / test_total_examples = 4834 / 6000\n",
      "testing accuracy = 0.8057\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 4881 / 6000\n",
      "training loss = 0.4246\n",
      "training accuracy = 0.8135\n",
      "num_test_corrects / test_total_examples = 4834 / 6000\n",
      "testing accuracy = 0.8057\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 4881 / 6000\n",
      "training loss = 0.4245\n",
      "training accuracy = 0.8135\n",
      "num_test_corrects / test_total_examples = 4835 / 6000\n",
      "testing accuracy = 0.8058\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 4882 / 6000\n",
      "training loss = 0.4244\n",
      "training accuracy = 0.8137\n",
      "num_test_corrects / test_total_examples = 4835 / 6000\n",
      "testing accuracy = 0.8058\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 4883 / 6000\n",
      "training loss = 0.4243\n",
      "training accuracy = 0.8138\n",
      "num_test_corrects / test_total_examples = 4836 / 6000\n",
      "testing accuracy = 0.8060\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 4883 / 6000\n",
      "training loss = 0.4242\n",
      "training accuracy = 0.8138\n",
      "num_test_corrects / test_total_examples = 4838 / 6000\n",
      "testing accuracy = 0.8063\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 4883 / 6000\n",
      "training loss = 0.4241\n",
      "training accuracy = 0.8138\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4240\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4239\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4238\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4237\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4236\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4839 / 6000\n",
      "testing accuracy = 0.8065\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 4884 / 6000\n",
      "training loss = 0.4235\n",
      "training accuracy = 0.8140\n",
      "num_test_corrects / test_total_examples = 4840 / 6000\n",
      "testing accuracy = 0.8067\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 4882 / 6000\n",
      "training loss = 0.4234\n",
      "training accuracy = 0.8137\n",
      "num_test_corrects / test_total_examples = 4841 / 6000\n",
      "testing accuracy = 0.8068\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 4885 / 6000\n",
      "training loss = 0.4233\n",
      "training accuracy = 0.8142\n",
      "num_test_corrects / test_total_examples = 4842 / 6000\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 4886 / 6000\n",
      "training loss = 0.4232\n",
      "training accuracy = 0.8143\n",
      "num_test_corrects / test_total_examples = 4842 / 6000\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 4886 / 6000\n",
      "training loss = 0.4231\n",
      "training accuracy = 0.8143\n",
      "num_test_corrects / test_total_examples = 4842 / 6000\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 4888 / 6000\n",
      "training loss = 0.4230\n",
      "training accuracy = 0.8147\n",
      "num_test_corrects / test_total_examples = 4845 / 6000\n",
      "testing accuracy = 0.8075\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 4890 / 6000\n",
      "training loss = 0.4229\n",
      "training accuracy = 0.8150\n",
      "num_test_corrects / test_total_examples = 4845 / 6000\n",
      "testing accuracy = 0.8075\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 4891 / 6000\n",
      "training loss = 0.4228\n",
      "training accuracy = 0.8152\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 4891 / 6000\n",
      "training loss = 0.4227\n",
      "training accuracy = 0.8152\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 4892 / 6000\n",
      "training loss = 0.4226\n",
      "training accuracy = 0.8153\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 4892 / 6000\n",
      "training loss = 0.4225\n",
      "training accuracy = 0.8153\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 4893 / 6000\n",
      "training loss = 0.4224\n",
      "training accuracy = 0.8155\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 4893 / 6000\n",
      "training loss = 0.4223\n",
      "training accuracy = 0.8155\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 4893 / 6000\n",
      "training loss = 0.4222\n",
      "training accuracy = 0.8155\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 4892 / 6000\n",
      "training loss = 0.4221\n",
      "training accuracy = 0.8153\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 4891 / 6000\n",
      "training loss = 0.4220\n",
      "training accuracy = 0.8152\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 4892 / 6000\n",
      "training loss = 0.4219\n",
      "training accuracy = 0.8153\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 4892 / 6000\n",
      "training loss = 0.4219\n",
      "training accuracy = 0.8153\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 4893 / 6000\n",
      "training loss = 0.4218\n",
      "training accuracy = 0.8155\n",
      "num_test_corrects / test_total_examples = 4848 / 6000\n",
      "testing accuracy = 0.8080\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 4894 / 6000\n",
      "training loss = 0.4217\n",
      "training accuracy = 0.8157\n",
      "num_test_corrects / test_total_examples = 4849 / 6000\n",
      "testing accuracy = 0.8082\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 4894 / 6000\n",
      "training loss = 0.4216\n",
      "training accuracy = 0.8157\n",
      "num_test_corrects / test_total_examples = 4849 / 6000\n",
      "testing accuracy = 0.8082\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 4894 / 6000\n",
      "training loss = 0.4215\n",
      "training accuracy = 0.8157\n",
      "num_test_corrects / test_total_examples = 4849 / 6000\n",
      "testing accuracy = 0.8082\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 4894 / 6000\n",
      "training loss = 0.4214\n",
      "training accuracy = 0.8157\n",
      "num_test_corrects / test_total_examples = 4849 / 6000\n",
      "testing accuracy = 0.8082\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 4896 / 6000\n",
      "training loss = 0.4213\n",
      "training accuracy = 0.8160\n",
      "num_test_corrects / test_total_examples = 4851 / 6000\n",
      "testing accuracy = 0.8085\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 4896 / 6000\n",
      "training loss = 0.4212\n",
      "training accuracy = 0.8160\n",
      "num_test_corrects / test_total_examples = 4851 / 6000\n",
      "testing accuracy = 0.8085\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 4897 / 6000\n",
      "training loss = 0.4211\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 4851 / 6000\n",
      "testing accuracy = 0.8085\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 4897 / 6000\n",
      "training loss = 0.4210\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 4852 / 6000\n",
      "testing accuracy = 0.8087\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 4897 / 6000\n",
      "training loss = 0.4209\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 4852 / 6000\n",
      "testing accuracy = 0.8087\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 4897 / 6000\n",
      "training loss = 0.4209\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 4852 / 6000\n",
      "testing accuracy = 0.8087\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 4897 / 6000\n",
      "training loss = 0.4208\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 4855 / 6000\n",
      "testing accuracy = 0.8092\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 4896 / 6000\n",
      "training loss = 0.4207\n",
      "training accuracy = 0.8160\n",
      "num_test_corrects / test_total_examples = 4856 / 6000\n",
      "testing accuracy = 0.8093\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 4896 / 6000\n",
      "training loss = 0.4206\n",
      "training accuracy = 0.8160\n",
      "num_test_corrects / test_total_examples = 4857 / 6000\n",
      "testing accuracy = 0.8095\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 4896 / 6000\n",
      "training loss = 0.4205\n",
      "training accuracy = 0.8160\n",
      "num_test_corrects / test_total_examples = 4859 / 6000\n",
      "testing accuracy = 0.8098\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 4898 / 6000\n",
      "training loss = 0.4204\n",
      "training accuracy = 0.8163\n",
      "num_test_corrects / test_total_examples = 4860 / 6000\n",
      "testing accuracy = 0.8100\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 4899 / 6000\n",
      "training loss = 0.4203\n",
      "training accuracy = 0.8165\n",
      "num_test_corrects / test_total_examples = 4860 / 6000\n",
      "testing accuracy = 0.8100\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 4899 / 6000\n",
      "training loss = 0.4202\n",
      "training accuracy = 0.8165\n",
      "num_test_corrects / test_total_examples = 4862 / 6000\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 4900 / 6000\n",
      "training loss = 0.4202\n",
      "training accuracy = 0.8167\n",
      "num_test_corrects / test_total_examples = 4862 / 6000\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 4902 / 6000\n",
      "training loss = 0.4201\n",
      "training accuracy = 0.8170\n",
      "num_test_corrects / test_total_examples = 4862 / 6000\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 4902 / 6000\n",
      "training loss = 0.4200\n",
      "training accuracy = 0.8170\n",
      "num_test_corrects / test_total_examples = 4862 / 6000\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 4903 / 6000\n",
      "training loss = 0.4199\n",
      "training accuracy = 0.8172\n",
      "num_test_corrects / test_total_examples = 4863 / 6000\n",
      "testing accuracy = 0.8105\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 4903 / 6000\n",
      "training loss = 0.4198\n",
      "training accuracy = 0.8172\n",
      "num_test_corrects / test_total_examples = 4863 / 6000\n",
      "testing accuracy = 0.8105\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 4904 / 6000\n",
      "training loss = 0.4197\n",
      "training accuracy = 0.8173\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 4904 / 6000\n",
      "training loss = 0.4196\n",
      "training accuracy = 0.8173\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 4906 / 6000\n",
      "training loss = 0.4196\n",
      "training accuracy = 0.8177\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 4906 / 6000\n",
      "training loss = 0.4195\n",
      "training accuracy = 0.8177\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 4907 / 6000\n",
      "training loss = 0.4194\n",
      "training accuracy = 0.8178\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 4907 / 6000\n",
      "training loss = 0.4193\n",
      "training accuracy = 0.8178\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 4909 / 6000\n",
      "training loss = 0.4192\n",
      "training accuracy = 0.8182\n",
      "num_test_corrects / test_total_examples = 4864 / 6000\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 4910 / 6000\n",
      "training loss = 0.4191\n",
      "training accuracy = 0.8183\n",
      "num_test_corrects / test_total_examples = 4866 / 6000\n",
      "testing accuracy = 0.8110\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 4910 / 6000\n",
      "training loss = 0.4191\n",
      "training accuracy = 0.8183\n",
      "num_test_corrects / test_total_examples = 4866 / 6000\n",
      "testing accuracy = 0.8110\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 4910 / 6000\n",
      "training loss = 0.4190\n",
      "training accuracy = 0.8183\n",
      "num_test_corrects / test_total_examples = 4866 / 6000\n",
      "testing accuracy = 0.8110\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 4911 / 6000\n",
      "training loss = 0.4189\n",
      "training accuracy = 0.8185\n",
      "num_test_corrects / test_total_examples = 4866 / 6000\n",
      "testing accuracy = 0.8110\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 4911 / 6000\n",
      "training loss = 0.4188\n",
      "training accuracy = 0.8185\n",
      "num_test_corrects / test_total_examples = 4867 / 6000\n",
      "testing accuracy = 0.8112\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 4912 / 6000\n",
      "training loss = 0.4187\n",
      "training accuracy = 0.8187\n",
      "num_test_corrects / test_total_examples = 4867 / 6000\n",
      "testing accuracy = 0.8112\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 4912 / 6000\n",
      "training loss = 0.4187\n",
      "training accuracy = 0.8187\n",
      "num_test_corrects / test_total_examples = 4867 / 6000\n",
      "testing accuracy = 0.8112\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 4913 / 6000\n",
      "training loss = 0.4186\n",
      "training accuracy = 0.8188\n",
      "num_test_corrects / test_total_examples = 4868 / 6000\n",
      "testing accuracy = 0.8113\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 4913 / 6000\n",
      "training loss = 0.4185\n",
      "training accuracy = 0.8188\n",
      "num_test_corrects / test_total_examples = 4868 / 6000\n",
      "testing accuracy = 0.8113\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 4916 / 6000\n",
      "training loss = 0.4184\n",
      "training accuracy = 0.8193\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 4915 / 6000\n",
      "training loss = 0.4183\n",
      "training accuracy = 0.8192\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 4916 / 6000\n",
      "training loss = 0.4183\n",
      "training accuracy = 0.8193\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 4916 / 6000\n",
      "training loss = 0.4182\n",
      "training accuracy = 0.8193\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 4917 / 6000\n",
      "training loss = 0.4181\n",
      "training accuracy = 0.8195\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 4917 / 6000\n",
      "training loss = 0.4180\n",
      "training accuracy = 0.8195\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 4918 / 6000\n",
      "training loss = 0.4179\n",
      "training accuracy = 0.8197\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 4918 / 6000\n",
      "training loss = 0.4179\n",
      "training accuracy = 0.8197\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 4919 / 6000\n",
      "training loss = 0.4178\n",
      "training accuracy = 0.8198\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 4921 / 6000\n",
      "training loss = 0.4177\n",
      "training accuracy = 0.8202\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 4921 / 6000\n",
      "training loss = 0.4176\n",
      "training accuracy = 0.8202\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 4921 / 6000\n",
      "training loss = 0.4176\n",
      "training accuracy = 0.8202\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 4922 / 6000\n",
      "training loss = 0.4175\n",
      "training accuracy = 0.8203\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 4922 / 6000\n",
      "training loss = 0.4174\n",
      "training accuracy = 0.8203\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 4923 / 6000\n",
      "training loss = 0.4173\n",
      "training accuracy = 0.8205\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 4924 / 6000\n",
      "training loss = 0.4172\n",
      "training accuracy = 0.8207\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 4924 / 6000\n",
      "training loss = 0.4172\n",
      "training accuracy = 0.8207\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 4925 / 6000\n",
      "training loss = 0.4171\n",
      "training accuracy = 0.8208\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 4925 / 6000\n",
      "training loss = 0.4170\n",
      "training accuracy = 0.8208\n",
      "num_test_corrects / test_total_examples = 4870 / 6000\n",
      "testing accuracy = 0.8117\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 4925 / 6000\n",
      "training loss = 0.4169\n",
      "training accuracy = 0.8208\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 4925 / 6000\n",
      "training loss = 0.4169\n",
      "training accuracy = 0.8208\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 4926 / 6000\n",
      "training loss = 0.4168\n",
      "training accuracy = 0.8210\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 4927 / 6000\n",
      "training loss = 0.4167\n",
      "training accuracy = 0.8212\n",
      "num_test_corrects / test_total_examples = 4869 / 6000\n",
      "testing accuracy = 0.8115\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4167\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4871 / 6000\n",
      "testing accuracy = 0.8118\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4166\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4871 / 6000\n",
      "testing accuracy = 0.8118\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4165\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4872 / 6000\n",
      "testing accuracy = 0.8120\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4164\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4872 / 6000\n",
      "testing accuracy = 0.8120\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 4928 / 6000\n",
      "training loss = 0.4164\n",
      "training accuracy = 0.8213\n",
      "num_test_corrects / test_total_examples = 4872 / 6000\n",
      "testing accuracy = 0.8120\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 4928 / 6000\n",
      "training loss = 0.4163\n",
      "training accuracy = 0.8213\n",
      "num_test_corrects / test_total_examples = 4872 / 6000\n",
      "testing accuracy = 0.8120\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 4928 / 6000\n",
      "training loss = 0.4162\n",
      "training accuracy = 0.8213\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 4928 / 6000\n",
      "training loss = 0.4161\n",
      "training accuracy = 0.8213\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4161\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4160\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 4930 / 6000\n",
      "training loss = 0.4159\n",
      "training accuracy = 0.8217\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 4930 / 6000\n",
      "training loss = 0.4159\n",
      "training accuracy = 0.8217\n",
      "num_test_corrects / test_total_examples = 4873 / 6000\n",
      "testing accuracy = 0.8122\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 4930 / 6000\n",
      "training loss = 0.4158\n",
      "training accuracy = 0.8217\n",
      "num_test_corrects / test_total_examples = 4874 / 6000\n",
      "testing accuracy = 0.8123\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 4930 / 6000\n",
      "training loss = 0.4157\n",
      "training accuracy = 0.8217\n",
      "num_test_corrects / test_total_examples = 4875 / 6000\n",
      "testing accuracy = 0.8125\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4156\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4875 / 6000\n",
      "testing accuracy = 0.8125\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4156\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4876 / 6000\n",
      "testing accuracy = 0.8127\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4155\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4876 / 6000\n",
      "testing accuracy = 0.8127\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4154\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4877 / 6000\n",
      "testing accuracy = 0.8128\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4154\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 4929 / 6000\n",
      "training loss = 0.4153\n",
      "training accuracy = 0.8215\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 4930 / 6000\n",
      "training loss = 0.4152\n",
      "training accuracy = 0.8217\n",
      "num_test_corrects / test_total_examples = 4878 / 6000\n",
      "testing accuracy = 0.8130\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 4931 / 6000\n",
      "training loss = 0.4152\n",
      "training accuracy = 0.8218\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 4931 / 6000\n",
      "training loss = 0.4151\n",
      "training accuracy = 0.8218\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 4931 / 6000\n",
      "training loss = 0.4150\n",
      "training accuracy = 0.8218\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 4932 / 6000\n",
      "training loss = 0.4150\n",
      "training accuracy = 0.8220\n",
      "num_test_corrects / test_total_examples = 4879 / 6000\n",
      "testing accuracy = 0.8132\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4149\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4880 / 6000\n",
      "testing accuracy = 0.8133\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4148\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4880 / 6000\n",
      "testing accuracy = 0.8133\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4147\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4882 / 6000\n",
      "testing accuracy = 0.8137\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4147\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4882 / 6000\n",
      "testing accuracy = 0.8137\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4146\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4882 / 6000\n",
      "testing accuracy = 0.8137\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 4934 / 6000\n",
      "training loss = 0.4145\n",
      "training accuracy = 0.8223\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4145\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4144\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4143\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4143\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4142\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4883 / 6000\n",
      "testing accuracy = 0.8138\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4141\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4884 / 6000\n",
      "testing accuracy = 0.8140\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4141\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4884 / 6000\n",
      "testing accuracy = 0.8140\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4884 / 6000\n",
      "testing accuracy = 0.8140\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 4936 / 6000\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8227\n",
      "num_test_corrects / test_total_examples = 4884 / 6000\n",
      "testing accuracy = 0.8140\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4139\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4884 / 6000\n",
      "testing accuracy = 0.8140\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 4934 / 6000\n",
      "training loss = 0.4138\n",
      "training accuracy = 0.8223\n",
      "num_test_corrects / test_total_examples = 4885 / 6000\n",
      "testing accuracy = 0.8142\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 4934 / 6000\n",
      "training loss = 0.4138\n",
      "training accuracy = 0.8223\n",
      "num_test_corrects / test_total_examples = 4885 / 6000\n",
      "testing accuracy = 0.8142\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 4935 / 6000\n",
      "training loss = 0.4137\n",
      "training accuracy = 0.8225\n",
      "num_test_corrects / test_total_examples = 4885 / 6000\n",
      "testing accuracy = 0.8142\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 4937 / 6000\n",
      "training loss = 0.4136\n",
      "training accuracy = 0.8228\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 4937 / 6000\n",
      "training loss = 0.4136\n",
      "training accuracy = 0.8228\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 4939 / 6000\n",
      "training loss = 0.4135\n",
      "training accuracy = 0.8232\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 4940 / 6000\n",
      "training loss = 0.4134\n",
      "training accuracy = 0.8233\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 4940 / 6000\n",
      "training loss = 0.4134\n",
      "training accuracy = 0.8233\n",
      "num_test_corrects / test_total_examples = 4885 / 6000\n",
      "testing accuracy = 0.8142\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 4940 / 6000\n",
      "training loss = 0.4133\n",
      "training accuracy = 0.8233\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 4940 / 6000\n",
      "training loss = 0.4132\n",
      "training accuracy = 0.8233\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 4941 / 6000\n",
      "training loss = 0.4132\n",
      "training accuracy = 0.8235\n",
      "num_test_corrects / test_total_examples = 4886 / 6000\n",
      "testing accuracy = 0.8143\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 4942 / 6000\n",
      "training loss = 0.4131\n",
      "training accuracy = 0.8237\n",
      "num_test_corrects / test_total_examples = 4887 / 6000\n",
      "testing accuracy = 0.8145\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 4942 / 6000\n",
      "training loss = 0.4131\n",
      "training accuracy = 0.8237\n",
      "num_test_corrects / test_total_examples = 4888 / 6000\n",
      "testing accuracy = 0.8147\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 4942 / 6000\n",
      "training loss = 0.4130\n",
      "training accuracy = 0.8237\n",
      "num_test_corrects / test_total_examples = 4889 / 6000\n",
      "testing accuracy = 0.8148\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 4942 / 6000\n",
      "training loss = 0.4129\n",
      "training accuracy = 0.8237\n",
      "num_test_corrects / test_total_examples = 4889 / 6000\n",
      "testing accuracy = 0.8148\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 4943 / 6000\n",
      "training loss = 0.4129\n",
      "training accuracy = 0.8238\n",
      "num_test_corrects / test_total_examples = 4890 / 6000\n",
      "testing accuracy = 0.8150\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4128\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4889 / 6000\n",
      "testing accuracy = 0.8148\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4127\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4891 / 6000\n",
      "testing accuracy = 0.8152\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4127\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4891 / 6000\n",
      "testing accuracy = 0.8152\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4126\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4891 / 6000\n",
      "testing accuracy = 0.8152\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4126\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4891 / 6000\n",
      "testing accuracy = 0.8152\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 4944 / 6000\n",
      "training loss = 0.4125\n",
      "training accuracy = 0.8240\n",
      "num_test_corrects / test_total_examples = 4891 / 6000\n",
      "testing accuracy = 0.8152\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 4945 / 6000\n",
      "training loss = 0.4124\n",
      "training accuracy = 0.8242\n",
      "num_test_corrects / test_total_examples = 4892 / 6000\n",
      "testing accuracy = 0.8153\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 4945 / 6000\n",
      "training loss = 0.4124\n",
      "training accuracy = 0.8242\n",
      "num_test_corrects / test_total_examples = 4892 / 6000\n",
      "testing accuracy = 0.8153\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 4946 / 6000\n",
      "training loss = 0.4123\n",
      "training accuracy = 0.8243\n",
      "num_test_corrects / test_total_examples = 4892 / 6000\n",
      "testing accuracy = 0.8153\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 4946 / 6000\n",
      "training loss = 0.4123\n",
      "training accuracy = 0.8243\n",
      "num_test_corrects / test_total_examples = 4893 / 6000\n",
      "testing accuracy = 0.8155\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 4946 / 6000\n",
      "training loss = 0.4122\n",
      "training accuracy = 0.8243\n",
      "num_test_corrects / test_total_examples = 4893 / 6000\n",
      "testing accuracy = 0.8155\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 4946 / 6000\n",
      "training loss = 0.4121\n",
      "training accuracy = 0.8243\n",
      "num_test_corrects / test_total_examples = 4892 / 6000\n",
      "testing accuracy = 0.8153\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4121\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4893 / 6000\n",
      "testing accuracy = 0.8155\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4895 / 6000\n",
      "testing accuracy = 0.8158\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4895 / 6000\n",
      "testing accuracy = 0.8158\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4896 / 6000\n",
      "testing accuracy = 0.8160\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4118\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4896 / 6000\n",
      "testing accuracy = 0.8160\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4118\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4897 / 6000\n",
      "testing accuracy = 0.8162\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 4947 / 6000\n",
      "training loss = 0.4117\n",
      "training accuracy = 0.8245\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 4950 / 6000\n",
      "training loss = 0.4117\n",
      "training accuracy = 0.8250\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 4951 / 6000\n",
      "training loss = 0.4116\n",
      "training accuracy = 0.8252\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 4951 / 6000\n",
      "training loss = 0.4115\n",
      "training accuracy = 0.8252\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 4951 / 6000\n",
      "training loss = 0.4115\n",
      "training accuracy = 0.8252\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 4951 / 6000\n",
      "training loss = 0.4114\n",
      "training accuracy = 0.8252\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 4952 / 6000\n",
      "training loss = 0.4114\n",
      "training accuracy = 0.8253\n",
      "num_test_corrects / test_total_examples = 4898 / 6000\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 4952 / 6000\n",
      "training loss = 0.4113\n",
      "training accuracy = 0.8253\n",
      "num_test_corrects / test_total_examples = 4899 / 6000\n",
      "testing accuracy = 0.8165\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 4952 / 6000\n",
      "training loss = 0.4112\n",
      "training accuracy = 0.8253\n",
      "num_test_corrects / test_total_examples = 4901 / 6000\n",
      "testing accuracy = 0.8168\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 4953 / 6000\n",
      "training loss = 0.4112\n",
      "training accuracy = 0.8255\n",
      "num_test_corrects / test_total_examples = 4902 / 6000\n",
      "testing accuracy = 0.8170\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 4953 / 6000\n",
      "training loss = 0.4111\n",
      "training accuracy = 0.8255\n",
      "num_test_corrects / test_total_examples = 4902 / 6000\n",
      "testing accuracy = 0.8170\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 4954 / 6000\n",
      "training loss = 0.4111\n",
      "training accuracy = 0.8257\n",
      "num_test_corrects / test_total_examples = 4904 / 6000\n",
      "testing accuracy = 0.8173\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 4955 / 6000\n",
      "training loss = 0.4110\n",
      "training accuracy = 0.8258\n",
      "num_test_corrects / test_total_examples = 4904 / 6000\n",
      "testing accuracy = 0.8173\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4110\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4905 / 6000\n",
      "testing accuracy = 0.8175\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 4955 / 6000\n",
      "training loss = 0.4109\n",
      "training accuracy = 0.8258\n",
      "num_test_corrects / test_total_examples = 4905 / 6000\n",
      "testing accuracy = 0.8175\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4108\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4108\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4107\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4107\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4106\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4106\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4105\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4906 / 6000\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 4957 / 6000\n",
      "training loss = 0.4105\n",
      "training accuracy = 0.8262\n",
      "num_test_corrects / test_total_examples = 4905 / 6000\n",
      "testing accuracy = 0.8175\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 4957 / 6000\n",
      "training loss = 0.4104\n",
      "training accuracy = 0.8262\n",
      "num_test_corrects / test_total_examples = 4908 / 6000\n",
      "testing accuracy = 0.8180\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4103\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4909 / 6000\n",
      "testing accuracy = 0.8182\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 4956 / 6000\n",
      "training loss = 0.4103\n",
      "training accuracy = 0.8260\n",
      "num_test_corrects / test_total_examples = 4909 / 6000\n",
      "testing accuracy = 0.8182\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 4957 / 6000\n",
      "training loss = 0.4102\n",
      "training accuracy = 0.8262\n",
      "num_test_corrects / test_total_examples = 4909 / 6000\n",
      "testing accuracy = 0.8182\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4102\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4909 / 6000\n",
      "testing accuracy = 0.8182\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4101\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4101\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4100\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4100\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4099\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 4957 / 6000\n",
      "training loss = 0.4098\n",
      "training accuracy = 0.8262\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 4958 / 6000\n",
      "training loss = 0.4098\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 4959 / 6000\n",
      "training loss = 0.4097\n",
      "training accuracy = 0.8265\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 4960 / 6000\n",
      "training loss = 0.4097\n",
      "training accuracy = 0.8267\n",
      "num_test_corrects / test_total_examples = 4910 / 6000\n",
      "testing accuracy = 0.8183\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 4961 / 6000\n",
      "training loss = 0.4096\n",
      "training accuracy = 0.8268\n",
      "num_test_corrects / test_total_examples = 4911 / 6000\n",
      "testing accuracy = 0.8185\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 4961 / 6000\n",
      "training loss = 0.4096\n",
      "training accuracy = 0.8268\n",
      "num_test_corrects / test_total_examples = 4911 / 6000\n",
      "testing accuracy = 0.8185\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 4961 / 6000\n",
      "training loss = 0.4095\n",
      "training accuracy = 0.8268\n",
      "num_test_corrects / test_total_examples = 4911 / 6000\n",
      "testing accuracy = 0.8185\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 4962 / 6000\n",
      "training loss = 0.4095\n",
      "training accuracy = 0.8270\n",
      "num_test_corrects / test_total_examples = 4911 / 6000\n",
      "testing accuracy = 0.8185\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4094\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4094\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4093\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4093\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4092\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4091\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4913 / 6000\n",
      "testing accuracy = 0.8188\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4091\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4914 / 6000\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4090\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4914 / 6000\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 4962 / 6000\n",
      "training loss = 0.4090\n",
      "training accuracy = 0.8270\n",
      "num_test_corrects / test_total_examples = 4914 / 6000\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4089\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4914 / 6000\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4089\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4914 / 6000\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4088\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4915 / 6000\n",
      "testing accuracy = 0.8192\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4088\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4916 / 6000\n",
      "testing accuracy = 0.8193\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4087\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4916 / 6000\n",
      "testing accuracy = 0.8193\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4087\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4916 / 6000\n",
      "testing accuracy = 0.8193\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4086\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4916 / 6000\n",
      "testing accuracy = 0.8193\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 4964 / 6000\n",
      "training loss = 0.4086\n",
      "training accuracy = 0.8273\n",
      "num_test_corrects / test_total_examples = 4917 / 6000\n",
      "testing accuracy = 0.8195\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 4963 / 6000\n",
      "training loss = 0.4085\n",
      "training accuracy = 0.8272\n",
      "num_test_corrects / test_total_examples = 4917 / 6000\n",
      "testing accuracy = 0.8195\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 4962 / 6000\n",
      "training loss = 0.4085\n",
      "training accuracy = 0.8270\n",
      "num_test_corrects / test_total_examples = 4918 / 6000\n",
      "testing accuracy = 0.8197\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearModel(128, 1)\n",
    "lr_model.to(device)\n",
    "print(lr_model)\n",
    "\n",
    "def train(model):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f\"--------- epoch: {e+1} ---------\")\n",
    "        # training\n",
    "        train_loss = 0.0\n",
    "        corrects = 0\n",
    "        total_examples = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # zero the gradients\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # the backward pass\n",
    "            loss = criterion(y_pred, y)  # calculate the loss\n",
    "            loss.backward()  # get the gradients\n",
    "            optimizer.step()  # update the params based on the gradients\n",
    "            # collect training results\n",
    "            train_loss += loss.item()\n",
    "            corrects += torch.sum((y_pred.round() == y))\n",
    "            total_examples += len(y)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(corrects / total_examples)\n",
    "        print(f\"training loss = {train_losses[-1]:.4f}\")\n",
    "        print(f\"training accuracy = {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"num_train_corrects / train_total_examples = {corrects.item()} / {total_examples}\")\n",
    "        # print(total_examples)\n",
    "\n",
    "        # testing\n",
    "        test_corrects = 0\n",
    "        test_total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                # prepare data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).to(torch.float32)\n",
    "                # the forward pass\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.reshape(y.shape)\n",
    "                # collect testing results\n",
    "                test_corrects += torch.sum((y_pred.round() == y))\n",
    "                test_total_examples += len(y)\n",
    "\n",
    "        test_accuracies.append(test_corrects.item() / test_total_examples)\n",
    "        print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "        print(f\"testing accuracy = {test_accuracies[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "train_losses, train_accuracies, test_accuracies = train(lr_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8ecd873d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7eUlEQVR4nO3deVxU593//zfIJjgjCLKI4oq4VTEaqwajWTCJd79GTWqapHfU2Cyab5M2d28bvfvT9m5i+r2TGJvF5s6dNHdqs9klezQ01raJggqSRHGJikRAREFkkHXA6/cHMIYiMoPAGeD1fDw+D5kz5xw+c7SZd69zrnN8JBkBAAB4MV+rGwAAAGgNgQUAAHg9AgsAAPB6BBYAAOD1CCwAAMDrEVgAAIDXI7AAAACvR2ABAABez8/qBtrTgAEDVFZWZnUbAADAAzabTSdOnLjkOt0msAwYMED5+flWtwEAANogNjb2kqGl2wSWxpGV2NhYRlkAAOgibDab8vPzW/3u7jaBpVFZWRmBBQCAboaLbgEAgNcjsAAAAK9HYAEAAF6PwAIAALwegQUAAHg9AgsAAPB6BBYAAOD1CCwAAMDrEVgAAIDXI7AAAACvR2ABAABej8ACAAC8Xrd7+GF7m/H92xQxKFapf3hHJ49kW90OAAA9EiMsrUi88Tol3fFdhQ+KtboVAAB6LAJLK2prnJIkv4AAizsBAKDnIrC0os5ZH1h6+XP2DAAAqxBYWuEaYfFnhAUAAKsQWFpRW1MjSfIL8Le4EwAAei4CSyvqamslSb38CSwAAFiFwNIKRlgAALAegaUVzBICAMB6BJZWMEsIAADrEVhawSwhAACsR2BpBSMsAABYj8DSilon17AAAGA1AksrmCUEAID1CCytqHPW34fFj/uwAABgGQJLKxpHWLhxHAAA1iGwtIL7sAAAYD0CSysaZwn5MUsIAADLEFha0ThLqBcjLAAAWIbA0grXCAuzhAAAsAyBpRVcdAsAgPUILK24cGt+AgsAAFYhsLTiwkW3BBYAAKxCYGlF4whLL65hAQDAMgSWVtQ6G2/NzywhAACsQmBpBbfmBwDAegSWVjBLCAAA6xFYWnHh1vwEFgAArEJgaUXjLKFefn7y8eVwAQBgBb6BW9E4wiJxWggAAKsQWFrR+CwhidNCAABYhcDSirpvBhZGWAAAsASBxQ2NM4UILAAAWIPA4obG00JcwwIAgDUILG6oY2ozAACWalNgWbZsmbKzs1VZWan09HQlJSVdcv2AgAA9+uijysnJUVVVlY4cOaIlS5a43l+0aJGMMc0qMDCwLe21uwv3YuH2/AAAWMHP0w0WLlyo9evXa/ny5dq+fbvuu+8+bd68WWPGjFFubu5Ft9m0aZOioqK0dOlSHTlyRJGRkfLza/qrS0tLlZCQ0GRZdXW1p+11iMbnCfEARAAArOFxYHn44Yf18ssv6+WXX5Yk/fjHP9YNN9ygZcuWadWqVc3Wv+GGGzRz5kwNGzZMJSUlkqSvv/662XrGGBUWFnraTqdwVtcHFn8vGfEBAKCn8eiUkL+/vyZNmqSUlJQmy1NSUjR9+vSLbjN37lylp6drxYoVysvL06FDh/TEE08oKCioyXp9+vRRTk6OcnNz9f777ysxMfGSvQQEBMhmszWpjuJsGOnxD+SUEAAAVvAosERERMjPz6/ZSEhhYaGio6Mvus2wYcOUlJSkcePGaf78+frRj36kW2+9Vc8//7xrnYMHD2rx4sWaO3eubr/9dlVVVWn79u0aMWJEi72sXLlSDofDVfn5+Z58FI/UVjUGFkZYAACwQpsuujXGNHnt4+PTbJnrF/j6yhijO++8U7t379bmzZv18MMPa/Hixa5Rlp07d+q1117Tl19+qc8++0wLFy7UV199pR/+8Ict9vD444/Lbre7KjY2ti0fxS2uEZYgAgsAAFbw6BqWoqIi1dbWNhtNiYyMbPH6k4KCAuXn58vhcLiWHThwQL6+vho4cKCOHDnSbBtjjHbv3q34+PgWe6mpqVFNww3dOhrXsAAAYC2PRlicTqcyMjKUnJzcZHlycrJ27Nhx0W22b9+uAQMGKCQkxLVs5MiRqqurU15eXou/KzExUQUFBZ6012EaR1j8CCwAAFjC41NC69at0w9+8AMtWbJEo0aN0rp16xQXF6cXXnhBkrR27Vq9+uqrrvVff/11FRcX65VXXtHo0aM1Y8YMPfHEE/rtb3+rqqoqSdLq1as1e/ZsDR06VBMmTNDLL7+sxMRE1z6t5uQaFgAALOXxtOZNmzYpPDxcq1evVkxMjPbt26c5c+bo+PHjkqSYmBjFxcW51i8vL1dycrKeffZZpaenq7i4WJs2bdLPfvYz1zqhoaF68cUXFR0drdLSUmVmZurqq6/W7t272+EjXj6uYQEAwFo+ki5+tWwXY7PZ5HA4ZLfbVVZW1q77vnnFj3T1v96mrS/9Th/9+jftum8AAHoyd7+/eZaQGy5cw8J9WAAAsAKBxQ0XbhzHKSEAAKxAYHEDF90CAGAtAosbamu46BYAACsRWNzgGmEJ4BoWAACsQGBxA9OaAQCwFoHFDdyaHwAAaxFY3OCa1swICwAAliCwuIFrWAAAsBaBxQ1cwwIAgLUILG6o5RoWAAAsRWBxA3e6BQDAWgQWNzRew8KzhAAAsAaBxQ3fHGHx8fGxuBsAAHoeAosbGq9hkRhlAQDACgQWNzSOsEhcxwIAgBUILG44X1enOmetJKY2AwBgBQKLm2qqqiRJ/kFBFncCAEDPQ2BxU01lpSQpsHdvizsBAKDnIbC4qaayfoQloDcjLAAAdDYCi5saR1gCGGEBAKDTEVjcxAgLAADWIbC4iREWAACsQ2Bx04URFgILAACdjcDipgsjLJwSAgCgsxFY3OQaYQlmhAUAgM5GYHETIywAAFiHwOImrmEBAMA6BBY3McICAIB1CCxuYlozAADWIbC4qaaCG8cBAGAVAoubGGEBAMA6BBY3VXNrfgAALENgcRMjLAAAWIfA4iYefggAgHUILG5qHGEJZIQFAIBOR2BxE6eEAACwDoHFTY2nhPyDAuXjy2EDAKAz8c3rpuryCtfPgTwAEQCATkVgcVNtTY1qnU5JUmBIsMXdAADQsxBYPNA4yhIUEmJxJwAA9CwEFg9UnSuXJAX2IbAAANCZCCweqC6vDyxBnBICAKBTEVg80HhKKJBTQgAAdCoCiweqGkdYOCUEAECnIrB4wHUNSzCnhAAA6EwEFg+4ZgkxwgIAQKcisHig8ZQQ92EBAKBzEVg8UH2ucZYQIywAAHQmAosHqio4JQQAgBUILB6o5qJbAAAs0abAsmzZMmVnZ6uyslLp6elKSkq65PoBAQF69NFHlZOTo6qqKh05ckRLlixpss6CBQuUlZWlqqoqZWVlad68eW1prUNVNd6HhREWAAA6lceBZeHChVq/fr0ee+wxTZw4UZ9++qk2b96sQYMGtbjNpk2bdN1112np0qVKSEjQ7bffroMHD7renzp1qt566y1t3LhREyZM0MaNG7Vp0yZNmTKlbZ+qg1y40y2BBQCAzmY8qbS0NLNhw4Ymy/bv32/Wrl170fVvuOEGU1JSYsLCwlrc55tvvmk++uijJss2b95sXn/9dbf7stlsxhhjbDabR5/HkxqSON48tTfVPPLBpg77HRRFURTVk8rd72+PRlj8/f01adIkpaSkNFmekpKi6dOnX3SbuXPnKj09XStWrFBeXp4OHTqkJ554QkFBQa51pk2b1myfH3/8cYv7lOpPM9lstibV0bjTLQAA1vDzZOWIiAj5+fmpsLCwyfLCwkJFR0dfdJthw4YpKSlJVVVVmj9/viIiIrRhwwb169dPS5culSRFR0d7tE9JWrlypX7+85970v5lqyo7J0nqbevTqb8XAICerk0X3Rpjmrz28fFptsz1C3x9ZYzRnXfeqd27d2vz5s16+OGHtXjx4iajLJ7sU5Ief/xx2e12V8XGxrblo3ik0lEmSfILCJB/UGCH/z4AAFDPo8BSVFSk2traZiMfkZGRzUZIGhUUFCg/P18Oh8O17MCBA/L19dXAgQMlSSdPnvRon5JUU1OjsrKyJtXRqisqVFdbK0nq3QmnoAAAQD2PAovT6VRGRoaSk5ObLE9OTtaOHTsuus327ds1YMAAhXxjZs3IkSNVV1envLw8SVJqamqzfc6ePbvFfVrJdVrITmABAKAzeXQ178KFC011dbVZsmSJGTVqlFm3bp0pKyszcXFxRpJZu3atefXVV13rh4SEmOPHj5tNmzaZ0aNHmxkzZphDhw6ZF1980bXOtGnTjNPpNCtWrDAJCQlmxYoVpqamxkyZMqXdrzK+3Hrkg03mqb2pZkjieMuvrKYoiqKorl4efH97vvNly5aZY8eOmaqqKpOenm5mzJjheu+VV14x27Zta7J+QkKCSUlJMeXl5eb48ePmySefNEFBQU3WueWWW8yBAwdMdXW12b9/v5k/f35HfeDLqofeeNk8tTfVjL76Ksv/kimKoiiqq5e7398+DT90eTabTQ6HQ3a7vUOvZ7n3v9crYfq39frKXyjjgy0d9nsAAOgJ3P3+5llCHqp0XcPC1GYAADoLgcVDjVObe9vtFncCAEDPQWDxUGXD9GxuHgcAQOchsHiokmnNAAB0OgKLhyoaTgkFE1gAAOg0BBYPVTUEliDudAsAQKchsHiIERYAADofgcVDFaWlkqTg0L4WdwIAQM9BYPFQeUl9YAkhsAAA0GkILB4qP3tWkuQfGKiA3r2tbQYAgB6CwOKhmsoqOauqJUkhYYyyAADQGQgsbdA4yhISGmppHwAA9BQEljZwXccSFmptIwAA9BAEljZwjbBwSggAgE5BYGmD8pKzkjglBABAZyGwtEH52cZTQoywAADQGQgsbcAICwAAnYvA0gauERZuHgcAQKcgsLRB4whLn35h1jYCAEAPQWBpA0fxGUmSLbyfxZ0AANAzEFja4ByBBQCATkVgaQNHUbEkqbfdJr+AAIu7AQCg+yOwtEFV2TnV1tRIYpQFAIDOQGBpo8ZRlj4EFgAAOhyBpY3OFZdIkuwRBBYAADoagaWNyhpGWGwR4RZ3AgBA90dgaaOyhplCnBICAKDjEVjaqDGw2BlhAQCgwxFY2qjxlJC9f4TFnQAA0P0RWNrobOEpSVLfyP4WdwIAQPdHYGmj0sLTkggsAAB0BgJLG5U2jLDYIvrJt1cvi7sBAKB7I7C00bkzJapz1sq3Vy/ZuBcLAAAdisDSRsYYlZ5uOC0UFWlxNwAAdG8ElsvgOFUkietYAADoaASWy9A4UyiUERYAADoUgeUyNF54GxodZXEnAAB0bwSWy3Amv0CS1C82xuJOAADo3ggsl6E474QkKXxgrMWdAADQvRFYLsOZvHxJUr+BAyzuBACA7o3AchnOnKg/JdTb1ke97XaLuwEAoPsisFwGZ1W1HKfrpzaHM8oCAECHIbBcJtd1LIO4jgUAgI5CYLlMxY3XsTBTCACADkNguUxnmCkEAECHI7BcpjP5jYGFa1gAAOgoBJbL1HgNC1ObAQDoOASWy9R4DUtYTLR8e/WyuBsAALonAstlcpwqUm1NjXr5+alvFE9tBgCgIxBYLpMxxvVMIS68BQCgYxBY2kFRbp4kqf/gOIs7AQCge2pTYFm2bJmys7NVWVmp9PR0JSUltbjuzJkzZYxpVgkJCa51Fi1adNF1AgMD29Jepys8miNJih4x1NpGAADopvw83WDhwoVav369li9fru3bt+u+++7T5s2bNWbMGOXm5ra43ciRI+VwOFyvT58+3eT90tLSJiFGkqqrqz1tzxKFR7MlSVHDCSwAAHQEjwPLww8/rJdfflkvv/yyJOnHP/6xbrjhBi1btkyrVq1qcbtTp06ptLS0xfeNMSosLPS0Ha9w8sgxSVL0iGEWdwIAQPfk0Skhf39/TZo0SSkpKU2Wp6SkaPr06ZfcNjMzUydOnNAnn3yiWbNmNXu/T58+ysnJUW5urt5//30lJiZecn8BAQGy2WxNyiqF2TmSJFt4P4WE9rWsDwAAuiuPAktERIT8/PyajYQUFhYqOjr6otsUFBTonnvu0S233KIFCxbo0KFD2rp1q2bMmOFa5+DBg1q8eLHmzp2r22+/XVVVVdq+fbtGjBjRYi8rV66Uw+FwVX5+vicfpV3VVFa6ZgpxWggAgI5h3K2YmBhjjDFTp05tsnzVqlXmwIEDbu/nvffeM++++26L7/v4+JjMzEzz61//usV1AgICjM1mc9WAAQOMMcbYbDa3+2jPWvr8k+apvalm2sL5lvx+iqIoiuqKZbPZ3Pr+9miEpaioSLW1tc1GUyIjIz26/iQtLU3x8fEtvm+M0e7duy+5Tk1NjcrKypqUlQq5jgUAgA7jUWBxOp3KyMhQcnJyk+XJycnasWOH2/uZOHGiCgoKLrlOYmJiq+t4k5NHGwILp4QAAGh3Hs8SWrdunTZu3Kj09HSlpqbq3nvvVVxcnF544QVJ0tq1axUbG6tFixZJkh566CHl5OQoKytLAQEB+v73v69bb71VCxYscO1z9erVSktL0+HDh2W32/Xggw8qMTFRDzzwQDt9zI538ghTmwEA6CgeB5ZNmzYpPDxcq1evVkxMjPbt26c5c+bo+PHjkqSYmBjFxV2442tAQICefPJJxcbGqrKyUllZWZozZ442b97sWic0NFQvvviioqOjVVpaqszMTF199dXavXt3O3zEznHqWI6khplCYaEqLzlraT8AAHQnPqq/mKXLs9lscjgcstvtll3P8sgHm9R/8CD9970P6avUXZb0AABAV+Lu9zfPEmpHeVkHJEmDxo22uBMAALoXAks7Ot4YWMYSWAAAaE8ElnaUl3VQEiMsAAC0NwJLO8rbf0jn6+oUGhUpW0S41e0AANBtEFjaUU1lpeu5QpwWAgCg/RBY2lne/obTQmNHWdwJAADdB4GlneXuY6YQAADtjcDSznKZKQQAQLsjsLSz/IOH5ayqVp9+YYocOtjqdgAA6BYILO2szunU11/ukyQNmzzR4m4AAOgeCCwd4Gh6piRp+KREaxsBAKCbILB0gOyMzyUxwgIAQHshsHSAr7/cp1qnU6FRkQofGGt1OwAAdHkElg7grKpW7t79kqRhkxOtbQYAgG6AwNJBjjacFhrOaSEAAC4bgaWDHN29R5IUP/VKizsBAKDrI7B0kOyMz1VTWaXQqEhFxw+3uh0AALo0AksHqa2p0dH0+lGW0UlTLe4GAICujcDSgQ5+lipJSriKwAIAwOUgsHSgA5+mSZKGXjFBgcHBFncDAEDXRWDpQMW5eSo6nic/f3/FT51sdTsAAHRZBJYO1nhaaMzVV1ncCQAAXReBpYPt2/apJGnsNTPk26uXxd0AANA1EVg62NH0PSo/W6o+/cI09IoJVrcDAECXRGDpYOdr65TVMMoy/vpZ1jYDAEAXRWDpBF/+ZZsk6VvXzZKPj4/F3QAA0PUQWDrBV2m7VVl2Tn2j+itu/Fir2wEAoMshsHSCOqdTWX+rPy10xZzZFncDAEDXQ2DpJHs+TJEkJd54vXz9mC0EAIAnCCyd5HDabjmKitWnX5hGXTXN6nYAAOhSCCyd5HxdnfZ8+LEkafLcmyzuBgCAroXA0oky3t8iSRoz8yr1ttss7gYAgK6DwNKJThw6rBNfHZF/YKAmfecGq9sBAKDLILB0srQ/vitJmnrrPGsbAQCgCyGwdLKMD7aoprJKMfHDNSRxvNXtAADQJRBYOllV2Tl9vuUTSdK0hfOsbQYAgC6CwGKBHZveliRNmH2t+oSHWdwNAADej8Bigdx9+/X1l1nyDwzUVd+71ep2AADwegQWi/zt1dclSVfdtkD+QYEWdwMAgHcjsFhk39a/qzgvXyFhoZo8d47V7QAA4NUILBY5X1enf2x8U5I0867b5ePLXwUAAC3hW9JCu97+UBWlDvUfPEhjZ82wuh0AALwWgcVCNZWV2vHWnyVJ1/3gLou7AQDAexFYLPbpa5tUXVGpuG+N0ZiZSVa3AwCAVyKwWOzcmRJtf+MPkqQbH7hHPj4+FncEAID3IbB4gW2vvKaqc+WKHT1S466baXU7AAB4HQKLF6godegfv39LknTD8h8wygIAwD8hsHiJf2x8U5WOMsXED9fEOclWtwMAgFchsHiJSkeZtr3ymiRpzkPLuPstAADfQGDxIn/f+KbOnChQWEy0Zi66w+p2AADwGm0KLMuWLVN2drYqKyuVnp6upKSWp+POnDlTxphmlZCQ0GS9BQsWKCsrS1VVVcrKytK8efPa0lqXVltdrQ+f3iBJuvbuf5W9f4TFHQEA4B08DiwLFy7U+vXr9dhjj2nixIn69NNPtXnzZg0aNOiS240cOVLR0dGuOnz4sOu9qVOn6q233tLGjRs1YcIEbdy4UZs2bdKUKVM8/0Rd3OdbPlHO53sVGNxbcx663+p2AADwGsaTSktLMxs2bGiybP/+/Wbt2rUXXX/mzJnGGGP69u3b4j7ffPNN89FHHzVZtnnzZvP666+73ZfNZjPGGGOz2Tz6PN5Yg8aNMU/tTTVP7U01QyZ8y/J+KIqiKKqjyt3vb49GWPz9/TVp0iSlpKQ0WZ6SkqLp06dfctvMzEydOHFCn3zyiWbNmtXkvWnTpjXb58cff9zqPrur3H37tevtDyRJt675qXz9elncEQAA1vIosERERMjPz0+FhYVNlhcWFio6Ovqi2xQUFOiee+7RLbfcogULFujQoUPaunWrZsy48LC/6Ohoj/YpSQEBAbLZbE2qO3n/qWd17kyJYuKHa9aiO61uBwAAS7XpoltjTJPXPj4+zZY1+uqrr/TSSy8pMzNTaWlpeuCBB/Thhx/qJz/5SZv3KUkrV66Uw+FwVX5+fls+iteqKHXovSeekSTNvv9uhQ+MtbgjAACs41FgKSoqUm1tbbORj8jIyGYjJJeSlpam+Ph41+uTJ096vM/HH39cdrvdVbGx3e8LPeODLfoqbbf8gwJ165qfcgdcAECP5VFgcTqdysjIUHJy0zuxJicna8eOHW7vZ+LEiSooKHC9Tk1NbbbP2bNnX3KfNTU1Kisra1Ld0Z9++V+qqazSyKlXavptC6xuBwAAy3h0Ne/ChQtNdXW1WbJkiRk1apRZt26dKSsrM3FxcUaSWbt2rXn11Vdd6z/00EPm5ptvNiNGjDBjxowxa9euNcYYM3/+fNc606ZNM06n06xYscIkJCSYFStWmJqaGjNlypR2v8q4K1bSHbeap/ammsd3bTMRcQMt74eiKIqi2qs8+P72fOfLli0zx44dM1VVVSY9Pd3MmDHD9d4rr7xitm3b5nr97//+7+bw4cOmoqLCFBcXm3/84x/mpptuarbPW265xRw4cMBUV1eb/fv3Nwk07fyBu1z5+PiY+/7nGfPU3lTzw9+/aHx79bK8J4qiKIpqj3L3+9un4Ycuz2azyeFwyG63d8vTQ6FRkfrJ26+pt62PNj/3oj7571esbgkAgMvm7vc3zxLqIs4WntLba5+SJN2wbKmGT55ocUcAAHQeAksXkvHBFu165wP59uql7//Xf6pPeJjVLQEA0CkILF3Mnx97UgWHj8reP0J3Pv5z+fjyVwgA6P74tutinFXV2viTn6m6olIjp03R9fcutrolAAA6HIGlCyrMztGfHn1CkjR72VKNmjHN4o4AAOhYBJYuKuP9zUr94zvy9fXV9//ffypq2BCrWwIAoMMQWLqwtx97SkczMtXb1kd3P/uEgvvarW4JAIAOQWDpwupqa/Xqj1epOO+EIuIG6q6nHpOvXy+r2wIAoN0RWLq48pKz+u0P/11V5eWK//ZkLfiPn7S+EQAAXQyBpRs4eSRbr/305zp//rym3TpPs5cttbolAADaFYGlm9j/98/050eflCTdsPwHmrZwvsUdAQDQfggs3UjqH97WxxtekiQt+I+faHzyNRZ3BABA+yCwdDMpv3lZO976s3x9fXXnr36ukdOutLolAAAuG4GlG/rz2qf0Rcpf5RcQoLufeUIjpkyyuiUAAC4LgaUbMufP67WfrlHW3z6Tf1Cg7n72CQ3j6c4AgC6MwNJN1dXW6tWHV+nApzsUGNxbP3j+SQ29YoLVbQEA0CYElm6szunU//5opQ7t2KnA4GDd85t1iv/2ZKvbAgDAYwSWbq62pka/ffCnrtCy9PknNXZWktVtAQDgEQJLD1BbXa3f/nCF9m79u/wDA7Xo6cc1cc5sq9sCAMBtBJYeoramRr/7t/9Q+nub1cvPT3c8vkZX3X6r1W0BAOAWAksPcr6uTm/+7Jf67I0/ytfXVwtW/Zv+z09+KB8fH6tbAwDgkggsPYwxRm+vfUofrv+NJGnWojt011OPyT8o0OLOAABoGYGlh/rry7/T71esVm1NjcYnX6NlLz2nPv3CrG4LAICLIrD0YJmb/6IX7nlQFaUODZ4wTg++9j+Kjh9udVsAADRDYOnhju35Qs98/x4V5eYpfGCsHvz9/2jCDddZ3RYAAE0QWKDTOcf169uXNtyrpbfuevJR/cuPl8vHl38eAADvwDcSJEkVpQ69tPzftO23v5ckXXv3v+qeDU8puK/d4s4AACCw4BvO19Xpg6ef18af/EzVFZVKuGqqHv7DqxqSON7q1gAAPRyBBc18/vFWPfuv9+h0znGFxURr+SvP67ofLOJ+LQAAyxBYcFEFXx3V07ctUcYHW9TLz09zHrpf9/73etnC+1ndGgCgByKwoEXVFRV6feUv9Ob/96iqKyo1ctoU/dufNvLwRABApyOwoFW73/lQ67+3RCe+OiJbeD/d/ewTuu2X/6GgPiFWtwYA6CEILHDLqWNf69e3L9W23/5e58+f15R539FP/vx7xX97stWtAQB6AAIL3FZbU6MPnn5eGxYvU1FunsJionX/S89q/sqHFdA7yOr2AADdGIEFHjuW+aWeuuUubX/zT5KkpDu+q39/+3WNmjHN4s4AAN2VjyRjdRPtwWazyeFwyG63q6yszOp2eoyR06bou2seUb/YGEnS51s+0Tv/b73Kioot7gwA0BW4+/3NCAsuy1epu/TE/Dv0t/99XXW1tUq88Xr99N03NO2787lvCwCg3TDCgnYTO2qkbl3zU8WNGyNJyvlir955/GnlZh2wuDMAgLdy9/ubwIJ25ePrq6u+d4tuevA+BYXUT3ve9fYH+ujXv1FZ8RmLuwMAeBsCCyxlj+yvf3lomSbPvUmSVHWuXH/571f06e/fUl1trcXdAQC8BYEFXmHwhHGa99MfK+5b9aeJTucc14frN2jv1r9b3BkAwBsQWOA1fHx8NHnuTZrzo+WyR4RLkr7+Yp8+XL9BR9MzLe4OAGAlAgu8TmBIsGYtvlMz77pdgcG9JUkHPkvVh09vUMFXRyzuDgBgBQILvJYtvJ+S779bU2+5Wb38/XT+/HllfpSij59/ScV5+Va3BwDoRAQWeL3wQQN10/+9RxPnzJYk1dXWKuODLdr6P6+q6Hiexd0BADoDgQVdRuzokbrpwfs1Oqn+1v7n6+q058MUffI//6vTOcct7g4A0JEILOhyBo0bo9n3360xM6+SVB9cPt/yiT558X9VmJ1jbXMAgA5BYEGXNXBMgpLvv1vjrrnatSxr26fa9srvdSzzSws7AwC0NwILurzYUSN1/b2LNe66mfL1rX/sVc4Xe/W3V17Tvm2fypw/b3GHAIDLRWBBt9F/SJxm3nW7Js+9Sf6BgZLqb0D399+9qfT3P5KzqtriDgEAbUVgQbfTJzxMSXd8V1fddouC+9olSRUOh3b9+QNtf+tPOpN3wuIOAQCecvf727ctO1+2bJmys7NVWVmp9PR0JSUlubXd9OnT5XQ6lZnZ9O6mixYtkjGmWQU2/L9pQJLOFZdoy7Mv6pfJ8/TOr55WUW6egu12zVp8h1Z++Afd/ewTSpj+bfn4+FjdKgCgnfl5usHChQu1fv16LV++XNu3b9d9992nzZs3a8yYMcrNzW1xO7vdrt/97nfaunWroqKimr1fWlqqhISEJsuqqxnqR3M1lZX69LVN+uyNP2rUVVN11R23anTSNI2dlaSxs5J0Oue4tr/5J+1+7yNVlZ2zul0AQDvw+JRQWlqa9uzZo+XLl7uW7d+/X++8845WrVrV4nZvvPGGDh8+rLq6Os2bN08TJ050vbdo0SKtX79eYWFhnn+CBpwS6tkiBg/SVbfdoivn/Yt62/pIkpxV1foi5a/a+ef3lJ3xubUNAgAuqkNOCfn7+2vSpElKSUlpsjwlJUXTp09vcbvFixdr+PDh+sUvftHiOn369FFOTo5yc3P1/vvvKzEx8ZK9BAQEyGazNSn0XEVf5+rd/1qv/7xurv74n/+lgsNH5R8UqMlzb9ID//sb/fS9N3XNkjvVJ7ztoRgAYB2PAktERIT8/PxUWFjYZHlhYaGio6Mvus2IESP0q1/9Snfeeafq6uouus7Bgwe1ePFizZ07V7fffruqqqq0fft2jRgxosVeVq5cKYfD4ar8fJ5Bg/rTRal/eFtPLvi+1t++VKl/fEdV5eWKHDpY33n4/2r1X97Toqcf15iZSfL162V1uwAAN3l8DYskGdP0LJKPj0+zZZLk6+ur119/XWvWrNHhw4db3N/OnTu1c+dO1+vt27drz549+uEPf6iHHnroots8/vjjWrduneu1zWYjtKCJ3H37lbtvv977r2eUeOP1+vYt/0dDJnxL46+fpfHXz9K5MyX6fMsnyvhgi47v3W91uwCAS/AosBQVFam2trbZaEpkZGSzURepPkRceeWVmjhxop577jlJ9SHG19dXTqdTs2fP1rZt25ptZ4zR7t27FR8f32IvNTU1qqmp8aR99FA1lZXa9fb72vX2+4oeMUxT5n9HE+fMlj0iXEl3fFdJd3xXp3OOK+PDj7Xng495YjQAeKE2XXSbkZGhBx54wLUsKytL7777brOLbn18fDRmzJgmy5YvX65rr71Wt956q44dO6aKioqL/p5du3Zp7969Wrp0qVt9cdEtPOHbq5fip16pSd+5QeOunanA4N6u945lfqnMj1L05V+2qaz4jIVdAkD35+73t8enhNatW6eNGzcqPT1dqampuvfeexUXF6cXXnhBkrR27VrFxsa67q2SlZXVZPtTp06pqqqqyfLVq1crLS1Nhw8flt1u14MPPqjExMQmoQhoT+fr6nRoe5oObU9TQO/e+tZ1MzXpOzcofuqVGjpxvIZOHK95Kx9Wdsbn+uLjrdr7yd8ILwBgIY8Dy6ZNmxQeHq7Vq1crJiZG+/bt05w5c3T8+HFJUkxMjOLi4jzaZ2hoqF588UVFR0ertLRUmZmZuvrqq7V7925P2wM8VlNZqYwPtijjgy2y949Q4k3XK/GG6zV4/FiNuPIKjbjyCs1f9W+EFwCwELfmB1oQFhOt8cnXaMIN12nw+LGu5efPn9exPV9o37Z/KOuvn3LNCwBcBp4lBLSjlsKLJBUcPqqsv32mrG3/UO6+AxedMQcAuDgCC9BBwmKiNfaaGRp3zdUaNjlRvfwunFl1nC5S1t8/U9ZfP9XhnemqZSYbAFwSgQXoBL3tNo1KmqZx18zQqKRpCuoT4nqvuqJSR3fv0cHtaTr4aSqnjgDgIggsQCfr5e+v4ZMnaty1V2vsrCSFRjd9yOfpr3N18LNUHdyepqO798hZxcM9AYDAAlgsZuRwjUqaplFXTdXQiRPUy//CqSNndbWy0zN1cPtOHdqepsLsHOsaBQALEVgALxIYEqz4b09WwlVTNSppqvoNiGnyvuN0kY7sytDhtHQd3pmukoKTFnUKAJ2LwAJ4scihg+tHX5KmatgVifIPCmzyflFung7vTNeRnRk6sitD586UWNQpAHQsAgvQRfgFBGjwhHGK//ZkxX97sgaNG91k5pFUP3X6yK4MZWd8ruw9n+tcMQEGQPdAYAG6qMDgYA2blKj4qZM1YsokxY4a2Wyd0znHlb3nC1eAOZN3woJOAeDyEViAbiIkLFQjpkzSsEmJGnbFBEXHD5evr2+TdUoLTyt7z+cNAeYLFR7J5gZ2ALoEAgvQTfW22zQkcbyGTZqgYVckauDYUfLz92+yTqWjTMf3ZunrL7P09Zf79PWX+1XpcFjUMQC0jMAC9BD+QYGKGzdGwyZP1LArJmjwhHEKDA5utt6pY18r54u99SHmi306eSRb5vx5CzoGgAsILEAP5durl2Lihytu/FgNmfAtDR4/Vv2HNH+CenVFhY7v3a+vv8xS7r79yss6qLOFpyzoGEBPRmAB4BIS2ldx3xqrwRPGafD4sYr71tgmjxFoVFZ8RrlZB5S3/5Dysg4oN+ugHKeLLOgYQE9BYAHQIh9fX0UNG6LB48dq8PhxGjh2lKJHDGs2nVqqv6ldbtbB+gDTEGTKis9Y0DWA7ojAAsAjfoGBGjByuAaNHa2BY0dp4JhRih4+VL69ejVbt7TwtPIPfaUTh46o4NBh5R86rKLjeVwTA8BjBBYAl80/KFADEuI1aOwoDRwzWgPHJChq2JCLhpjqikqdPJKtE4cON9QRFXx1RNUVFRZ0DqCrILAA6BABvYM0YGS8YhJGKHbUSA0YOUIxI0cooHfQRdcvOp6nEw2jMCcPH1XB4WydyT/BaAwASQQWq9sBehQfX19FxA1UbEK8YhLiNSBhhGITRqpvVP+Lru+sqtbJ7GMqPHJMJ48c1ckjx3TySDYPfQR6IAILAMuFhIVqQEK8BowcoQGj4hUzYrgihw2Wf2DgRdevKi+vDzFH6wNMYzFTCei+CCwAvJKPr6/CB8UqevgwRccPU/TwoYoeMUyRQwarl3/zWUqSVOFw6NSxr3Xq2Nc6nXPc9XNRbp7O19Z18icA0J4ILAC6FF+/XuofN0jR8cNdISZ6xDBFxA286EW+klTnrFVxXr5OHcvRqW8EmVPHvlalg/8OAF0BgQVAt+AXEKD+Qwap/5DBihw6WFFDB6v/0MGKHBJ30UcQNCorPqNTORcCTNHXeSo6nqvivBOqczo78RMAuBQCC4Bur29Uf0UOHaLIoYMv1JA4hUZHtbjN+fPndbagUEXHc1V0PE+nj+eq+Hieio7nqTjvhGprajrxEwAgsADosQKDg9V/yCBFDh3sGpmJGDRQEYMHKiik+SMJGp0/f15nTxaqqCHAFH2dq6LchjCTm0+YAToAgQUALqJPeJj6xw1SRNxAhccNVP+4Qa4/L/Z8pUbnz5+X43SRzuSdUHHeCZ3JP6Ez+QU6k39CxXn5cpwqkjHd4j+nQKcisACAh/r0C2sYiakPNBGDYht+HqTetj6X3La2pkYlJ05+I8ycUHF+gc7k5as4r0CVDkcnfQqgayGwAEA7CgkLVb/YAQqPjVG/gbHqNzBG4bED1G/gAIVFR7c4JbtRZdm5htGY+jBTcuKkSgpO6mxBoc6cOEmgQY9FYAGATuLbq5f6RvZXv4EDXCEmfOAA9YsdoH6xMbL3j2h1H9UVFfUh5mShSk7UB5mSggKVFNS/dpwu0vk67jmD7ofAAgBewj8oUP0GxNSP0AyqDzJhMdEKjYlSWEy0bOH9Wt1HXW2tSk+d1tmCQp09WT8q88+hpqayshM+DdC+CCwA0EX4BQYqNDpS/QZEKzQ6WmEDohXWEGZCY6IUGh0lP3//VvdT6SjT2cJTKi08rdJTp1VaeKrh9SmdLTyt0sLTnHqC1yGwAEA34ePjI1tEuEJjotQvJlqhMQ2hJjpKYQPqQ02w3e7Wvmoqq74RZC4WbE6p/MxZZjyh0xBYAKAHCQwJVt/I/gqNjlTfyP7qGxWpvlGRCo2KVN+o/gqNilRIWKhb+6p1OuU4VXQhyJw6rbLTxSo9XSTHN6q6vKJjPxR6BAILAKAJv8BA9e0fob7RkQqN6u8KNPbI/q5gY4sIl6+vr1v7q66okON0cZMQ4zhdLEdRkRynLiyrOlfewZ8MXRmBBQDgMV+/XrJHRNQHmaiGEZv+/WXvHy57/whXXeome/+sprKqeag5fboh3BTLceq0yorPqKKU62t6Ine/vy994wAAQI9yvrZOZ0/Wz0S6lIDevS+EmIhw2SP7N/wZIXtERMOf4epttymgd1D9jfjiBl5yn7VOp86dKVFZ8RmdKz6jsqIzKituqKLiC8sJNz0SgQUA4LGaykrXM5cuxT8osD7A9G851NgjIxRst8vP31+hDaepWvPNcFNWVKxzxSWEm26OwAIA6DDOqmoV5+WrOC//kuv18veXrV+Y+oT3ky0iXLbwfrKF91Of8DDZI8Lrl4f3ky2i32WFm3PFZ3TuTInOFZfoXMlZlZeUqOxMicrPnNW5khKdO1MiZ1V1e318tCMCCwDAcnVOp842zEpqTWO4sX0zyDSEmfqQ068h5IR5HG4kqbqiUuUlZ+uDTUlJ/c8EHMsRWAAAXYrH4Sb8m0Gmn0LCwtSnX6hCwkJl6xfmet2nX5j8AgIUGNxbgcG91S82xq1+3A045WfPqvxsKdPB24jAAgDotuqcTrcuIm4UGBKsPmFh6hMepj5hoQ1hpn0DTq3TqYqzpSpvrJKzqih1qLzkrMpLS1VeUtrw/lmVn3Wo/OxZVZWdu5zD0C0QWAAAaFBdXqHq8opWr7lp5EnACQkNlX9QoPz8/V3Tw91VV1tbH2oagkzF2YaAc7Z5uKkPQaWqKivrVncsJrAAANBGngYc/6BAhfTtq5CwUAWH9lXINyo4tH55SF+7gsNCG9brq8DgYPXy83Od2nLX+bo6VZQ6mozeNL6uONvwp8OhirMXlpefLfXah2gSWAAA6CTOqmqdrXLv+ptGfgEBDeHGrpDQ+pGb4L72hnBTH2qahp9QBfUJkW+vXg2jPWHS0MFu/75ap7Mh1JR+I9TUB5odb/3Z7XDW3ggsAAB4sdqaGjlOnZbj1Gm3t+nl59c0xDSEnOC+dgXb7QoO7XvhdUOFhPaVX0BA/SmriHDZI8Kb7feLv/yVwAIAANpHXW1t/Q30ioo92i6gd5CC7Xb1bggwFwJN/c8l+QUd1HHrCCwAAEBS/XOfaiqrPDpl1VnceyQnAACAhQgsAADA6xFYAACA1yOwAAAAr9emwLJs2TJlZ2ersrJS6enpSkpKcmu76dOny+l0KjMzs9l7CxYsUFZWlqqqqpSVlaV58+a1pTUAANBNGU9q4cKFprq62ixdutSMGjXKPP3006asrMwMGjToktvZ7XZz5MgRs2XLFpOZmdnkvalTpxqn02keeeQRk5CQYB555BFTU1NjpkyZ4nZfNpvNGGOMzWbz6PNQFEVRFGVdufv97dPwg9vS0tK0Z88eLV++3LVs//79euedd7Rq1aoWt3vjjTd0+PBh1dXVad68eZo4caLrvTfffFN2u11z5sxxLdu8ebNKSkp0xx13uNWXzWaTw+GQ3W5XWVmZJx8JAABYxN3vb49OCfn7+2vSpElKSUlpsjwlJUXTp09vcbvFixdr+PDh+sUvfnHR96dNm9Zsnx9//PEl9xkQECCbzdakAABA9+RRYImIiJCfn58KC5s+pruwsFDR0dEX3WbEiBH61a9+pTvvvFN1dXUXXSc6OtqjfUrSypUr5XA4XJWfb82tggEAQMdr00W3//y4ah8fn4s+wtrX11evv/661qxZo8OHD7fLPhs9/vjjstvtroqNjfXgEwAAgK7Eo1vzFxUVqba2ttnIR2RkZLMREqn+vNSVV16piRMn6rnnnpNUH2J8fX3ldDo1e/Zsbdu2TSdPnnR7n41qampUU1PjSfsAAKCL8miExel0KiMjQ8nJyU2WJycna8eOHc3WdzgcGjdunBITE131wgsv6ODBg0pMTNTOnTslSampqc32OXv27IvuEwAA9EweTT9qnNa8ZMkSM2rUKLNu3TpTVlZm4uLijCSzdu1a8+qrr7a4/Zo1a5pNa542bZpxOp1mxYoVJiEhwaxYsYJpzRRFURTVA8rd72+Pn9a8adMmhYeHa/Xq1YqJidG+ffs0Z84cHT9+XJIUExOjuLg4j/aZmpqq733ve3r00Uf1y1/+UkePHtVtt92mXbt2edoes4UAAOhC3P3e9vg+LN5qwIABzBQCAKCLio2N1YkTJ1p8v9sEFqk+tLT3TeNsNpvy8/MVGxvLDek6GMe6c3CcOwfHufNwrDtHRx5nm812ybAieThLyNu19mEvR1lZGf9D6CQc687Bce4cHOfOw7HuHB1xnN3ZH09rBgAAXo/AAgAAvB6BpRXV1dX6+c9/rurqaqtb6fY41p2D49w5OM6dh2PdOaw+zt3qolsAANA9McICAAC8HoEFAAB4PQILAADwegQWAADg9QgsrVi2bJmys7NVWVmp9PR0JSUlWd1SlzJjxgy99957ys/PlzFGN998c7N11qxZo/z8fFVUVGjbtm0aM2ZMk/cDAgL0zDPP6PTp0zp37pzeffddxcbGdtZH6BIeeeQR7dq1Sw6HQ4WFhXr77bc1cuTIZutxrC/P/fffry+++EKlpaUqLS3Vjh07dOONNzZZh2Pc/h555BEZY/T00083Wc6xvnxr1qyRMaZJFRQUNFvHW46z5U9q9NZqfDL10qVLzahRo8zTTz9tysrKzKBBgyzvravUjTfeaH75y1+a+fPnG2OMufnmm5u8v2LFClNaWmrmz59vxo4da9544w2Tn59v+vTp41pnw4YNJjc311x33XUmMTHRbN261WRmZhpfX1/LP5+31ObNm82iRYvMmDFjzPjx4837779vcnJyTHBwMMe6Hes73/mOuemmm0x8fLyJj483jz76qKmurjZjxozhGHdQTZ482WRnZ5vPP//cPP30067lHOv2qTVr1pi9e/eaqKgoV0VERHjrcbb+gHlrpaWlmQ0bNjRZtn//frN27VrLe+uKdbHAcuLECbNixQrX64CAAFNSUmLuvfdeI8nY7XZTXV1tFi5c6FonJibG1NbWmtmzZ1v+mby1IiIijDHGzJgxg2PdwVVcXGzuvvtujnEHVEhIiDl06JC57rrrzLZt25oEFo51+9SaNWtMZmZmi+9703HmlFAL/P39NWnSJKWkpDRZnpKSounTp1vUVfcydOhQxcTENDnGNTU1+vvf/+46xpMmTVJAQECTdQoKCrRv3z7+Hi6hb9++kqQzZ85I4lh3BF9fX912220KCQlRamoqx7gDPP/88/rwww+1devWJss51u0rPj5e+fn5ys7O1htvvKGhQ4dK8r7j3K0eftieIiIi5Ofnp8LCwibLCwsLFR0dbVFX3UvjcbzYMR48eLBrnerqap09e7bZOvw9tGzdunX69NNPlZWVJYlj3Z7GjRun1NRUBQUF6dy5c5o/f74OHDigadOmSeIYt5fbbrtNV1xxha688spm7/Hvuf3s3LlTd911l7766itFRUXpZz/7mXbs2KGxY8d63XEmsLTCGNPktY+PT7NluDxtOcb8PbTsueee0/jx4y96gTjH+vIdOnRIiYmJCg0N1S233KJXX31VM2fOdL3PMb58AwcO1K9//WvNnj37kreB51hfvi1btrh+3rdvn1JTU3X06FEtWrRIaWlpkrznOHNKqAVFRUWqra1tlhAjIyObpU20zcmTJyXpksf45MmTCgwMVGhoaIvr4IJnnnlGc+fO1TXXXKP8/HzXco51+3E6nTp69KgyMjK0atUqffHFF3rooYc4xu1o0qRJioqKUkZGhpxOp5xOp2bNmqUHH3xQTqfTdaw41u2voqJCe/fuVXx8vNf9myawtMDpdCojI0PJyclNlicnJ2vHjh0WddW9HDt2TAUFBU2Osb+/v2bOnOk6xhkZGaqpqWmyTnR0tMaNG8ffwz959tlntWDBAl177bXKyclp8h7HuuP4+PgoMDCQY9yOtm7dqnHjxikxMdFVu3fv1muvvabExERlZ2dzrDtIQECARo8erYKCAq/8N235VcreWo3TmpcsWWJGjRpl1q1bZ8rKykxcXJzlvXWVCgkJMRMmTDATJkwwxhjzox/9yEyYMME1NXzFihWmpKTEzJs3z4wdO9a89tprF50yd/z4cXPttdeaxMRE88knnzA18Z/q+eefNyUlJebqq69uMj0xKCjItQ7H+vLrscceM0lJSWbw4MFm3Lhx5tFHHzW1tbXm+uuv5xh3cP3zLCGOdfvUE088Ya6++mozZMgQM2XKFPPee++Z0tJS1/eclx1n6w+YN9eyZcvMsWPHTFVVlUlPT28yTZRqvWbOnGku5pVXXnGts2bNGnPixAlTWVlp/va3v5mxY8c22UdgYKB55plnTFFRkSkvLzfvvfeeGThwoOWfzZuqJYsWLWqyHsf68uqll15y/fegsLDQ/OUvf3GFFY5xx9Y/BxaOdftU431VqqurTV5envnjH/9oRo8e7ZXH2afhBwAAAK/FNSwAAMDrEVgAAIDXI7AAAACvR2ABAABej8ACAAC8HoEFAAB4PQILAADwegQWAADg9QgsAADA6xFYAACA1yOwAAAAr0dgAQAAXu//B1fccarje626AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8683aa0a0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracies = [acc.cpu() for acc in train_accuracies]\n",
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8682fc8b0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7WklEQVR4nO3deXhU5f3//9dMJjszBAghG0GQHWQRtUCDuAWFWhW0WGu/RaRVY7VYbVGoRdsq2q+KaCs/P1Y+fpFaFW3rUo2m4oYQlERUICyyhoQQkhCSkG2y3L8/IKNjEjITkpwheT6u674aztzn5H0O2nl5n3Pu2ybJCAAAIIDZrS4AAACgNQQWAAAQ8AgsAAAg4BFYAABAwCOwAACAgEdgAQAAAY/AAgAAAh6BBQAABDyH1QW0p/j4eJWXl1tdBgAA8IPT6dTBgwdP2qfLBJb4+Hjl5eVZXQYAAGiDhISEk4aWLhNYGkdWEhISGGUBAOA04XQ6lZeX1+p3d5cJLI3Ky8sJLAAAdDE8dAsAAAIegQUAAAQ8AgsAAAh4BBYAABDwCCwAACDgEVgAAEDAI7AAAICAR2ABAAABj8ACAAACHoEFAAAEPAILAAAIeAQWAAAQ8Lrc4ocAAKDteifEKSQ8XJIUEhGu6KRE9ejdSwPOGqWXFy+Ru6rKkroILAAAdDGhkRHqk5ggm80mu8OhId87R1GxMd59IiIUPSBRNtmOb7DZFJ2UqMioni0ed93L/9KezE0dWXqLCCwAAFjIZrMpNDJC0Un9ZbM386SGMTrnyhlKHDlMQY7jX9uRvaLUOz6uQ+qpc7tVVX5MklRfV6fCfTmqLC1TbvYOlRzM75Df6QsCCwAAnaRHn14afO4E9Yrrp6GTvydX32j1TeqvoOD2/zouLz6i+ro6SVLxgTzt+fwLNdTVez6vr61T4f4c1dfWeraVFRbr4I6vPfsFEgILAACnICQ8TM4+fRQVG6PeCXHqP3qkwnpENukXGRWlwd+bIEdwcLPHKSsqVl2N22ubPcguV99o7f3iK61/8Z+qrqiQJLmrqlWUk6uGhvrmDqX62jpVlZWf4pkFFgILAAAnOEJCZIzRgLGj5fjWqEfPmL7q0ae37PYgRQ9IVHBIiMqKinXG2LM0YOxov35H3radOrxvvw7t2qOczVtVlJOriqOlqqmobLa/zW6XaWg4pfPqCggsAIAux+4IUkRPl2yyqU//RIU5j494mPoGlRUVydW3r2x2m1zR0UoaM1JJo0cqMqqnomL7ten31dXW6kjuQR07UqKczdkqPVzYpI+7ulr7vtisQ1/v9uvYhJXjCCwAgIAV0dOlPokJGjr5PDmCgxUR1VO94mK9+gQ5HOp7Rn85gkO89gsOC23z760oOeoVOqrKj+lI3kEZY1Ry8JBqKirVOzFehfv266v3PtKx4iNqqG/+9gzaB4EFANCpInq6FNHTpb4DkhTe06nh35+oId87R3XuWrmrqz2v1QY5HIro6Tql39XQ0KDyomKVFRZJkoJDQ9UrPlZH8vJV53ar+liFDmzJ1v6vslVacFilhUUKjQhXcW6e1wOqsB6BBQC6odCICDn79lFFSamqysokSb3iYmUPClJJ/iGv0YKIni5FfGtuDlPf4NUndsiZihs8SL3i4zwBIzg0RH0H9Jc96FtfM7bjk5L1SUzwq9aayirt+ixLRw8VyF1VrcL9OTL139wmMTIqzj2o6hOv4jbucyQ3T8YYv34XAheBBQC6kH6DzlDiyOGSjj+sGZ2UqJCIcAWHhKjvGUmKjOqp3onxCgkPl/3EnB81lVWyB9kVHHr8Fkpdbe23XnW1KTQivMnvaexjs9kVEh7md501lZUqzj2oyqOlyv96tza/96EkKTQy0nPrRZKO5heoprL5h1HRvRBYACAA2YOCFJ2UqL4D+mvweefI7giSdPxtFVd0H/VOjFe4s4f3TjZbi6/MNqeq/JjCnT08gaSutlamvkHBYaFNjlNVVi6j4yHCERyikPAwT5/62jrlbMnW0fxDOlpw/LkPYxpUtP+A3FXVXsepOHpUOVu2eY2GAL4gsABAgOjRu5cGn3u2Bp0zXmf/4NKmgcQHDfX12vfFZk9QKD1cqPLiIycCRK4qS8t0eN9+VZWVq6LkqMKcPRThcsoYo7LDRWqor1fPfjGyB30z42plWblXwLDZbOoZ09cTor77OdARCCwA0AlsNpuCToxIRPR0aXjyJAWHhSqsR6T6JCYoJDxMI6cme91+qa6oUEVJqb7esNHz0Gjj2yplhUUqO1zU5PdUV1b6FR6qy4816X/0UMFJ9zHG6GjBYZ9/B9AeCCwA4CdHaKgGjBml8TNSNHDcmNZ3sNkU1S+m2dlPvyv/693avfFzZX+0TjszPuOhUeAEAgsAfEtYj0ivBehsNpv6npGkAWNGK37YEAWHhWrg+DHqGdP3lH5P3radKtyfozp37fH1XOrqVHwgT5vf+5CQAjSDwAKg27EHnZgF1WZT9ID+iujp0qAJ4zR+eorPQaS6okIHtmzThldeU3nxkVb7V5SW6Wj+IRljZBqM3FVVp3oaQLdCYAHQ5fVOjNekH12lmIEDZLcHKemskerRu5fP+5cXH9H+r7bowJZtqj52TDWVVfry3fcJHUAnIrAA6HLCnD0U1S9G46ZforNnTDvpRGVHDxWovPiIDu/dr01p72n3xizVuWu9+jDlOmA9AguALmHIxHM14fLLNGDMKMUMHNDk892Zm7Qp7b+qr61TeVGxdqz/lCACnEbaFFhSU1P129/+VnFxcdq6davuuOMOffLJJy32/8lPfqIFCxZoyJAhKi0t1TvvvKPf/OY3OnLkm/u+s2bN0p/+9CedeeaZ2r17t373u9/ptddea0t5ALoAuyNI8UMHK+msUYpOSlSQw6FBE8apd2L88Q5GKs7NU/WxCoW7nIofOthr/5rKKuVmb9f6l/6pA9k7VHwg14KzANBe/A4ss2fP1rJly3Trrbdq3bp1uvnmm5WWlqaRI0fqwIEDTfp///vf1/PPP69f//rXevPNN5WQkKCnn35azz77rGbNmiVJmjhxol5++WX9/ve/17///W/NnDlTq1evVnJysj777LNTP0sAAcEeFKRe8XGqLC1TZJRLRw7mexaY69mvr3r06qXoAf2VNHqkJs2+SqERESc9XsLwoZ6fGxoatOGV17T1o090YHO2Ko6Wdui5AOhcNkl+vT+3YcMGff7557r11ls927Kzs/Xaa69p0aJFTfrfddddSk1N1eDB3/zXz2233aYFCxYoKSlJkvTSSy/J5XJpxowZnj5paWkqKSnRT37yE5/qcjqdKisrk8vlUnl5uT+nBKCdOUJD1W/QALn69lXv+FgNnXyezpwwXuEup1c/d1W1qsrL5ezTW/agoCbHqSwtU87mbB3atUf1dXUqyT+kXZ9lqb6uTkEOh2LOSFJQSIjq3W4dyN6hssOFnXWKANqJr9/ffo2wBAcHa8KECXr44Ye9tqenp2vy5MnN7rN+/Xo9+OCDmj59utLS0hQTE6NrrrlGb731lqfPpEmT9Pjjj3vt9+677+qOO+7wpzwAnahXfKwGnDVKkb2ijq97M6C/IlxO9T0jSdH9E5uEk0aNYaO+rk4h4WGehfPq6+p07EiJjh46rPKiIn3xzhp98c57J52TpHBfToecG4DA41dgiY6OlsPhUEGB97TNBQUFio2NbXafjIwMXX/99Xr55ZcVFham4OBgvf7667r99ts9fWJjY/06piSFhIQo9MTKotLxhAag/QWHhSpx5HDFDx2soOBgxZ45UCOmfl+u6D4n3a+qrFxHCw6rODdPhfsOaFNaukoPF6riyFGFRITLXVl1fPXgXlEqPpCnyrJy1dXUdNJZATjdtOmh2+/+F4/NZmvxv4JGjBihJ598Un/84x/17rvvKi4uTo888oiefvpp/fznP2/TMSVp4cKFuv/++9tSPgAfjLpwisZccqHGTrtIwWGhTT6vq63Vwe1fq+TEZGglefmqOHpUhfsP6NiRo8rZvLXFt3BqKiolSYf37pf27u/Q8wDQNfgVWIqKilRXV9dk5CMmJqbJCEmjhQsXat26dXr00UclSZs3b1ZFRYU++eQT3XvvvTp06JAOHTrk1zEl6aGHHtLSpUs9f3Y6ncrLy/PndAA0wxXTV5emztPEa670bCs9XKgDW7fJXVl1YtK0Ndq76SvVud0WVgqgO/ErsNTW1iorK0spKSlerxynpKTo9ddfb3afiIgI1dXVeW2rP/FfXTabTdLx20YpKSlatmyZp8+0adO0fv36Fmtxu91y83+WwCkJ6xGpiKie6pMQr8qyMiUMH6Yf3nWbInq6JEkZr76mrDfStHfTVxZXCqC78/uW0NKlS7Vq1SplZmYqIyNDN910k5KSkvT0009LkpYsWaKEhATNmTNHkvTmm2/qb3/7m2655RbPLaFly5bp008/VX5+viTpiSee0Mcff6wFCxbo9ddf15VXXqlLLrlEycnJ7XiqAPqekaTYwYPUJzFBZ54zXsOTJzb7dk5u9g795/Gn9PWGjRZUCQBN+R1YVq9erT59+mjx4sWKi4vTli1bNGPGDOXkHH9aPy4uzvO6siStXLlSTqdTt912mx577DEdPXpU77//vu6++25Pn4yMDP34xz/WAw88oD/96U/avXu3rr32WuZgAdogJDxcE390pZx9eis4NFR9ByTJHmRXZFSUEkYMbdK/zu1WWWGxwl1OHfp6t77+LEtrnn2eB2ABBBS/52EJVMzDgu4qJDxMiSOHa8CYUUoaM1qDzh7b4sJ+DfX1ytu+UyX5Bcr5aouyP1qngj37OrdgAPiWDpmHBYC1HCEhGj/9Ep0783IlDBuqIIdDQSHBstvtXv1K8g9py/sfq67GrcL9OXJX16i2ulr7v9qq8qJii6oHgLYjsAAB6oxxYzTl+h+pd8LxtXNsdpuik/or3NmjSd+jBYe1/8stytmcrf1fHf/f+traJv0A4HRFYAEChM1mU2TvKI1InqTYwWcq+fofyREc3KRfSf4hrXvpn8r+8BPVVFapzu3WsSMlFlQMAJ2HwAJ0kMbX9luaAPHcK2do/IxpcvWNlmloUK/4uCajJ1ve/0ifvfaWGuobJEllhYXK37m7xQnZAKCrIrAAfrDZbJLNptDI46sI2+129TtzoAaMGa2ks0aq35kDPc+TuPpGy2a36+D2nQoKCVbF0VJVlZUrNDxc/UePkKtvdLO/4+DOXTr09W599d6H2rLmo5PO+AwA3QWBBfBB/LAh+uFdt+nMc89WQ329gkObTlXfkoFnj212e01lpd7/378rd+s2DT53go4WFCjrP++qqoy33ADguwgsQAtiBg7QtFtu1KAJ49WzX1/P9iCH9782pYcLtf+rrcr5aovytu9Ubc3xGZirjx2TaTBKHDlcdTU1CnM5FRwSovq6OuXv3KXcbTtUW318rpPtn2zovBMDgNMQgQX4FrsjSJE9e2rG/FRNuPwyBQUf/1ekvrZOX733gda+sFpVZeU6kpcv03D8uZL67yw98V2Hdu3p8LoBoKsjsKBbs514BuWsi6dq0NljlThquCJcLs/n2R+v0wfPvaDcrdvkrqq2sFIA6N4ILOiWesXFata9v9GgCeMUFhnZ5PPSw4X6+4LF2pP1RecXBwBogsCCbmfopHM163e/Vd8B/SVJ1RUVyvlqqzav+UiF+3J0cMfXqiwr99zyAQBYj8CCLi/c5VKfxHgNmjBOI86frKETz5UkVZQc1d9S71Tuth2EEwAIcAQWdCnhLqcmz54lV98+ih08SGE9eihx5LAm/bL+847WPPu8CnbvtaBKAIC/CCzoMhJGDNX1D/9B/Qad0eSz0sOFOnrosL7esFE7Mz7T7sxNnV8gAKDNCCw47bn6RuuCudcr+bprFORwqLqiQtvXZig3e7tqKquU/dE6HT1UYHWZAIBTQGDBaSs0MkJnXXyBrvjN7YrsFSVJ2vR2ul778zIWAwSALobAgtOOq2+0Rl6QrEtT53nW4zm0e6/eeORJ7VjHjLEA0BURWHBaCHP20BljR2vstIt13szLPdsrSo7q68+y9K8HH1VFyVHrCgQAdCgCCwJOuMupM885W/YguwaMGa3EUcN1xtjRcoSEePrkZu9Q5ptpylj9b9W53RZWCwDoDAQWBIyg4GBNnj1TF/9ijpx9ejf5vHD/Ae3/cos2vPqa9m76yoIKAQBWIbDAcjabTaMvOl8XzfuZks4aKUkqLShU1bFjqq2pUebrb2vfF5uVm73d4koBAFYhsMBSMQMHaPYfFmng+DGSpKqycr395NP69F9vqr621uLqAACBgsACy5z/sx9rxq9uUXBoqKorKpT5RprWvrBaRfsPWF0aACDAEFjQqYIcDgUFB+vCuddrWuo8SdK2TzL06h/+zORuAIAWEVjQaS67/SZdNPf/KCj4m3/s3lq2XO+vWGVhVQCA0wGBBR0mrEekzpv5Q024/DL17NfX682fyrIyvfX4cm149XULKwQAnC4ILOgQZ19+qWYuvFMRLpdnW31tnd549El99u//qM7tVkN9vYUVAgBOJwQWtLvJ187S1ff+VpJUsGefPvnHK8rdtkOH9+5Xdfkxi6sDAJyOCCxoN66+0Zr1u9/orIunSpI+ev5F/WfpU4ykAABOGYEF7WLY9yfq//zfPyrc5VR9bZ3++8xz+u/T/2t1WQCALoLAglPm7NNb1z98v8JdTh3Yuk0vL35Q+Tt3W10WAKALIbDglIyYMlnX3He3IqN6Km/bTv3lpzepvq7O6rIAAF0MgQVtkjhyuGYuulNnjD1LklSUk6u/372YsAIA6BAEFvht+JRJmvvEn+UIDlad2621L7yid5f/TbXVNVaXBgDooggs8Fl0UqImXnOVkq+7Ro7gYG354GO9+oc/q7z4iNWlAQC6OAILfBI75Ezdvup/FBYZKUna8v5HWnnX79RQxyvLAICOR2DBSTlCQ3XprfN0/k+vlSMkRJWlZVrzt5X6aNVLMg0NVpcHAOgmCCxoUY8+vTTvL48q6ayRkqR9X2zWc3fcrWPFJRZXBgDobggsaOLMc8/WpB9dpeHJkxTu7KGKo6V6+fcPaOuHn1hdGgCgmyKwwMugc8brlmf/IrvdLun468p/u/VOFe0/YHFlAIDujMACj8ionpq16C7Z7Xbt2vi5Mlb/W9s/yVD1sQqrSwMAdHMEFkiSYgYO0K3PLZezT2/V1tTohXvuV9nhQqvLAgBAkmS3ugBYL37YEN3y7F/k7NNbhftytOK23xJWAAABhRGWbq7fmQOVuuKviujpUv7Xu/X/zbtNFSVHrS4LAAAvjLB0Y30SE3Tz/zyhiJ4u7f9yi/465xbCCgAgIDHC0g2F9YjUOVdM1/Rf3aKwyEjlf71bf7v1LlWXH7O6NAAAmkVg6WbOm/lDzfrdXQoODZUk7f38Sz3/29+rqqzM4soAAGgZgaUbGTb5e7r2j4skSUcO5uvD//cPrX/pnzLGWFwZAAAn16ZnWFJTU7Vnzx5VVVUpMzNTycnJLfZ97rnnZIxp0rZs2eLpM2fOnGb7hJ4YBcCpc8X01VX3/FqSlPHqa3rw0lla9+KrhBUAwGnB78Aye/ZsLVu2TA8++KDGjx+vtWvXKi0tTf3792+2//z58xUbG+tpiYmJKi4u1iuvvOLVr7S01KtfbGysampq2nZW8HCEhuqGZQ/rvjVvKGbgAJUXH9F/Hvur1WUBAOA340/bsGGDWb58ude27Oxss2TJEp/2v/LKK019fb1JSkrybJszZ44pKSnxq47vNqfTaYwxxul0ntJxukrrP2qEufzO28ydq1eaxzZnmMc2Z5jb//6MiR08yPLaaDQajUZrbL5+f/v1DEtwcLAmTJighx9+2Gt7enq6Jk+e7NMx5s2bp/fee085OTle23v06KF9+/YpKChIX3zxhX7/+9/riy++aPE4ISEhXreMnE6n7yfSxV38izm67LabPOsB1VRW6bn5d+vrDRstrgwAgLbxK7BER0fL4XCooKDAa3tBQYFiY2Nb3T82NlbTp0/XT37yE6/t27dv1w033KDNmzfL5XJp/vz5WrduncaOHatdu3Y1e6yFCxfq/vvv96f8Li+yV5R+/Kd7NXLq9yVJWz/8RHs//0JZb6Uzcy0A4LTn87BNXFycMcaYiRMnem1ftGiR2bZtW6v733PPPaawsNAEBweftJ/NZjObNm0yTzzxRIt9QkJCjNPp9LT4+PhufUuoR59eZuHbr5jHNmeYP2d9ZCb9aKblNdFoNBqN1lrrkFtCRUVFqqurazKaEhMT02TUpTk33nijVq1apdra2pP2M8Zo48aNGjJkSIt93G633G63b4V3cfagIF197wJF909UcW6eVtz2WxXs3mt1WQAAtBu/3hKqra1VVlaWUlJSvLanpKRo/fr1J9136tSpGjJkiFasWOHT7xo3bpzy8/P9Ka9b6p0Yr3v+87LGXHKBJOnvCxYTVgAAXZJfQzezZ882NTU1Zu7cuWb48OFm6dKlpry83PPWz5IlS8zKlSub7Pf888+bjIyMZo+5ePFiM23aNDNw4EAzduxYs2LFCuN2u825557b7kNKXak5QkPN3W+85LkNdOGNP7W8JhqNRqPR/GkdcktIklavXq0+ffpo8eLFiouL05YtWzRjxgzPWz9xcXFKSkry2sflcunqq6/W/Pnzmz1mVFSUnnnmGcXGxqq0tFSbNm3S+eefr40beavlZC6ae71iBg5QaUGhHv/xXJUXFVtdEgAAHcKm48nltOd0OlVWViaXy6Xy8nKry+lQQQ6HZv7uLk265ipJ0t/vvk+b3k63tigAANrA1+9v1hI6zdgdQZqzdIlGXThFDfX1+uC5FwgrAIAuj8BympmWOk+jLpyi2uoa/b87F2r72gyrSwIAoMO1afFDWCM4LFTf//HVkqSX71tCWAEAdBsEltPI92b9UBEul4oO5OqLd96zuhwAADoNgeU0kTBiqC7/9W2SpI9XvSzT0GBxRQAAdB6eYQlwwWGhmvDD6brsl79QcFiotq1dr/Uv/dPqsgAA6FQElgAWFdtPt6/6H0XF9pMkHcjerlW//b2M6RJvogMA4DMCS4AKcjj0s8ceVFRsP5XkH9JHz7+kT//5htxVVVaXBgBApyOwBKjL77pNA8aMUmVpmZ66IVUlBw9ZXRIAAJbhodsAFDt4kJJ/8iNJ0j8W/ZGwAgDo9ggsAcZmt+vyO38pu92uL9Pf17aP11ldEgAAliOwBJhLf/lzjZgyWXW1tXr3qb9ZXQ4AAAGBwBJAbDabZ0HDV//wsAr27LO0HgAAAgWBJYAkjhqhHr17qfpYhbLeetfqcgAACBgElgAy6oJkSdKO9Z+qoa7e4moAAAgcBJYAETvkTE392XWSpK0frLW4GgAAAguBJUD86L67FRIepm2fZOhzbgcBAOCFwBIAzv7BNJ0x9izVVFZq9eIlTL0PAMB3EFgsFhIeph/8+peSpDV/e15lhUUWVwQAQOAhsFhs4o+uUlS/GBXnHtRHz79odTkAAAQkAouFbDabkq+7RpL0/v+uUp3bbXFFAAAEJgKLRRwhIZr31KPqk5igqrJyff6fd6wuCQCAgEVgsUjKzXM1Yspk1dbU6LU/L5O7qtrqkgAACFgOqwvojqKTEnXhjT+VJP19wX3a8v5HFlcEAEBgY4TFAtNS5ynI4VD2x+sIKwAA+IDA0slcMX01fnqKJOmdvz5jcTUAAJweCCyd7JwfTpc9KEi7szYpb9tOq8sBAOC0QGDpRHZHkCZec4UkaeO//2NxNQAAnD4ILJ3onB/OUJ/EBJUXH9GX6e9bXQ4AAKcNAksniRs6WFf89leSjk8Sx2vMAAD4jsDSCeKHDdHPn3pU4c4e2p21Sete/KfVJQEAcFphHpYOFBIeptl/WOR5K6goJ1f/e/sC1dfWWlwZAACnFwJLB5qW+nONn56ihoYG7cncpH89+Kiqy49ZXRYAAKcdAksHcfbpre//+GpJ0spfL2KCOAAATgHPsHSQcdNTFBIeppzN2YQVAABOEYGlg4y+cIok6fO30y2uBACA0x+BpQOEu1waePZYSdLWD9daXA0AAKc/AksHGHPJVAU5HDq4c5eO5B60uhwAAE57BJYOcPYPLpUkbeJ2EAAA7YLA0s7OveoHGnTOeEnSprf/a3E1AAB0DQSWdtSjdy/9aPE9stvtWvfSP1WSf8jqkgAA6BIILO1o9EXnKyjYoQPZ2/WvBx+1uhwAALoMAks7GpNyoSTpy3fXWFwJAABdC4GlnThCQ3XmuWdLkjavYaI4AADaE4GlnfQfNVyO4GCVFRapaP8Bq8sBAKBLIbC0k4Hjx0iS9m76yuJKAADoeggs7eSMcQQWAAA6CoGlHdgdQTrzxNwrez//0uJqAADoetoUWFJTU7Vnzx5VVVUpMzNTycnJLfZ97rnnZIxp0rZs2eLVb9asWdq6dauqq6u1detWXXXVVW0pzRIDx41RWI9IlRcfUd62HVaXAwBAl+N3YJk9e7aWLVumBx98UOPHj9fatWuVlpam/v37N9t//vz5io2N9bTExEQVFxfrlVde8fSZOHGiXn75Za1atUpjx47VqlWrtHr1ap133nltP7NONGLKZEnS9k82yBhjcTUAAHRNxp+2YcMGs3z5cq9t2dnZZsmSJT7tf+WVV5r6+nqTlJTk2fbSSy+Zt99+26tfWlqa+cc//uFzXU6n0xhjjNPp9Ot82qPN/8cK89jmDDN+xrRO/900Go1Go53Ozdfvb79GWIKDgzVhwgSlp3sv6peenq7Jkyf7dIx58+bpvffeU05OjmfbpEmTmhzz3XffPekxQ0JC5HQ6vZoV7I4gxQ09U5KU89VWS2oAAKCr8yuwREdHy+FwqKCgwGt7QUGBYmNjW90/NjZW06dP17PPPttku7/HXLhwocrKyjwtLy/PjzNpP/0GDVRwaKiqyspVnGtNDQAAdHVteuj2u89p2Gw2n57duOGGG3T06FG99tprp3zMhx56SC6Xy9MSEhJ8K76dJY4cJknK277Tkt8PAEB34PCnc1FRkerq6pqMfMTExDQZIWnOjTfeqFWrVqm2ttZr+6FDh/w+ptvtltvt9qP6jpE44nhgyeXtIAAAOoxfIyy1tbXKyspSSkqK1/aUlBStX7/+pPtOnTpVQ4YM0YoVK5p8lpGR0eSY06ZNa/WYgSDhRGDhdWYAADqWX0/zzp4929TU1Ji5c+ea4cOHm6VLl5ry8nLPWz9LliwxK1eubLLf888/bzIyMpo95qRJk0xtba1ZsGCBGTZsmFmwYIFxu93mvPPOa/enjNuz2ex2s+TT981jmzNMzMABlj9pTaPRaDTa6db8+P72/+Cpqalm7969prq62mRmZpopU6Z4PnvuuefMBx984NXf5XKZiooK8/Of/7zFY1599dVm27ZtpqamxmRnZ5uZM2d21Am3W4sZOMA8tjnDLPl0jbHZ7Zb/pdNoNBqNdro1X7+/bSd+OO05nU6VlZXJ5XKpvLy8U37n2T+Ypusf/oP2fv6l/jrnlk75nQAAdCW+fn+zltApGDBmtCQeuAUAoKMRWE7BsO9PlCTt+izL4koAAOjaCCxtFJ2UqL4D+quutlZfb8i0uhwAALo0AksbNY6u7M36UjWVlRZXAwBA10ZgaaP+o0ZIkvZkbbK4EgAAuj4CSxs1Tsmfu40p+QEA6GgEljYIDgtVv0FnSOINIQAAOgOBpQ3ihpwpe1CQyouPqOxwodXlAADQ5RFY2qBx/aDc7O0WVwIAQPdAYGmD/iOHS+J2EAAAnYXA0gaeFZqzCSwAAHQGAoufgoKDFTtkkCRGWAAA6CwEFj/FDh4oR3CwKkvLVHLwkNXlAADQLRBY/NT4OnP+17utLQQAgG6EwOKn6P6JkqSi/QcsrgQAgO6DwOKn6AH9JUlFB3ItrgQAgO6DwOKn6KTjgaWQERYAADoNgcVPfRtHWHIILAAAdBYCix8ieroU0dMlSSrK4ZYQAACdhcDih6jYfpKk8uIjqq2usbgaAAC6DwKLH8JdTklSxdFSiysBAKB7IbD4ofF2UFVZucWVAADQvRBY/BBxYoSlsrTM4koAAOheCCx+YIQFAABrEFj8EO46HlgYYQEAoHMRWPzwzQgLgQUAgM5EYPFD41tClQQWAAA6FYHFD56HbnmGBQCATkVg8UN4T55hAQDACgQWPzSOsPCWEAAAnYvA4ocIF681AwBgBQKLj2x2+zcP3XJLCACATkVg8VG4s4fnZ94SAgCgcxFYfNQ4aVx1RYUa6uotrgYAgO6FwOIjHrgFAMA6BBYfRfBKMwAAliGw+CicERYAACxDYPERIywAAFiHwOIjRlgAALAOgcVHjLAAAGAdAouPGme5ZeFDAAA6H4HFRxE9uSUEAIBVCCw+CveMsHBLCACAzkZg8dE3D90SWAAA6GwEFh/x0C0AANYhsPiocfHDqrJjFlcCAED3Q2DxkSM4RJJU63ZbXAkAAN0PgcVHQcEOSVJDfZ3FlQAA0P20KbCkpqZqz549qqqqUmZmppKTk0/aPyQkRA888ID27dun6upq7dq1S3PnzvV8PmfOHBljmrTQ0NC2lNfubDab5+eGunoLKwEAoHty+LvD7NmztWzZMt16661at26dbr75ZqWlpWnkyJE6cOBAs/usXr1a/fr107x587Rr1y7FxMTI4fD+1aWlpRo2bJjXtpqaGn/L6xD2oCDPzw31BBYAAKxg/GkbNmwwy5cv99qWnZ1tlixZ0mz/Sy+91JSUlJhevXq1eMw5c+aYkpISv+r4bnM6ncYYY5xO5ykdp7kWHBZqHtucYR7bnGFCwsPb/fg0Go1Go3XX5uv3t1+3hIKDgzVhwgSlp6d7bU9PT9fkyZOb3eeKK65QZmamFixYoNzcXO3YsUOPPPKIwsLCvPr16NFD+/bt04EDB/Tmm29q3LhxJ60lJCRETqfTq3UURlgAALCWX4ElOjpaDodDBQUFXtsLCgoUGxvb7D6DBg1ScnKyRo8erZkzZ+qOO+7QNddco6eeesrTZ/v27brhhht0xRVX6LrrrlN1dbXWrVunwYMHt1jLwoULVVZW5ml5eXn+nIpfCCwAAFjP52GbuLg4Y4wxEydO9Nq+aNEis23btmb3effdd01lZaVxuVyebTNnzjT19fUmLCys2X1sNpvZtGmTeeKJJ1qsJSQkxDidTk+Lj4/vsFtCkb2iPLeE2vvYNBqNRqN15+brLSG/HrotKipSXV1dk9GUmJiYJqMujfLz85WXl6eyb01pv23bNtntdiUmJmrXrl1N9jHGaOPGjRoyZEiLtbjdbrk7aU6UxhEWRlcAALCGX7eEamtrlZWVpZSUFK/tKSkpWr9+fbP7rFu3TvHx8YqMjPRsGzp0qOrr65Wbm9vi7xo3bpzy8/P9Ka/DBBFYAACwnF9DN7NnzzY1NTVm7ty5Zvjw4Wbp0qWmvLzcJCUlGUlmyZIlZuXKlZ7+kZGRJicnx6xevdqMGDHCTJkyxezYscM888wznj6LFy8206ZNMwMHDjRjx441K1asMG6325x77rntPqTUltYrPtY8tjnDLPn0fcuHzmg0Go1G60qtQ24JScfnVOnTp48WL16suLg4bdmyRTNmzFBOTo4kKS4uTklJSZ7+FRUVSklJ0V/+8hdlZmaquLhYq1ev1r333uvpExUVpWeeeUaxsbEqLS3Vpk2bdP7552vjxo3+ltch7EGNs9wywgIAgBVsOp5cTntOp1NlZWVyuVwqLy9v12PHDBygu994SZWlZfp98qXtemwAALozX7+/WUvIB40P3dbXsY4QAABWILD4wGY/fpm4JQQAgDUILD4IcvCWEAAAViKw+OCbeVgaLK4EAIDuicDiA94SAgDAWgQWH9iDeIYFAAArEVh8wNT8AABYi8DiAwILAADWIrD4wM5bQgAAWIrA4gO7/URgqSOwAABgBQKLD7glBACAtQgsPvDcEmpgHhYAAKxAYPFBUBC3hAAAsBKBxQc2zzwsLH4IAIAVCCw+aBxhqecZFgAALEFg8UHjQ7eGtYQAALAEgcUHjWsJMcICAIA1CCw+YC0hAACsRWDxAfOwAABgLQKLDwgsAABYi8DiA9YSAgDAWgQWH7CWEAAA1iKw+IARFgAArEVg8QHPsAAAYC0Ciw++CSxMHAcAgBUILD5gHhYAAKxFYPHBNyMsLH4IAIAVCCw+sLP4IQAAliKw+CDIcXwtIRY/BADAGgQWH9jsxy8TIywAAFiDwOKDoCAmjgMAwEoEFh8wcRwAANYisPjA85ZQA4EFAAArEFh8YD/xDAu3hAAAsAaBxQf2E28JcUsIAABrEFh8wFpCAABYi8DiA9YSAgDAWgQWH/CWEAAA1iKw+MDz0C1rCQEAYAkCiw/sTBwHAIClCCw+8NwSauAZFgAArEBg8UFQEK81AwBgJQKLDzyLH3JLCAAASxBYfMBbQgAAWIvA4oPG1ZoNawkBAGAJAosPGt8S4pYQAADWILD4gKn5AQCwFoHFBwQWAACs1abAkpqaqj179qiqqkqZmZlKTk4+af+QkBA98MAD2rdvn6qrq7Vr1y7NnTvXq8+sWbO0detWVVdXa+vWrbrqqqvaUlqHYC0hAACs5XdgmT17tpYtW6YHH3xQ48eP19q1a5WWlqb+/fu3uM/q1at18cUXa968eRo2bJiuu+46bd++3fP5xIkT9fLLL2vVqlUaO3asVq1apdWrV+u8885r21m1M3vQ8cvEQ7cAAFjH+NM2bNhgli9f7rUtOzvbLFmypNn+l156qSkpKTG9evVq8ZgvvfSSefvtt722paWlmX/84x8+1+V0Oo0xxjidTr/Ox5e2eM0b5rHNGSZ+2JB2PzaNRqPRaN25+fr97dcIS3BwsCZMmKD09HSv7enp6Zo8eXKz+1xxxRXKzMzUggULlJubqx07duiRRx5RWFiYp8+kSZOaHPPdd99t8ZjS8dtMTqfTq3UUzy0hpuYHAMASDn86R0dHy+FwqKCgwGt7QUGBYmNjm91n0KBBSk5OVnV1tWbOnKno6GgtX75cvXv31rx58yRJsbGxfh1TkhYuXKj777/fn/LbzGazSZIMD90CAGCJNj10a4zx+rPNZmuyzfML7HYZY3T99ddr48aNSktL05133qkbbrjBa5TFn2NK0kMPPSSXy+VpCQkJbTkVnzSOsJysHgAA0HH8GmEpKipSXV1dk5GPmJiYJiMkjfLz85WXl6eysjLPtm3btslutysxMVG7du3SoUOH/DqmJLndbrndbn/Kb7PGtYR4rRkAAGv4NcJSW1urrKwspaSkeG1PSUnR+vXrm91n3bp1io+PV2RkpGfb0KFDVV9fr9zcXElSRkZGk2NOmzatxWN2tsa3hHiGBQAA6/j1NO/s2bNNTU2NmTt3rhk+fLhZunSpKS8vN0lJSUaSWbJkiVm5cqWnf2RkpMnJyTGrV682I0aMMFOmTDE7duwwzzzzjKfPpEmTTG1trVmwYIEZNmyYWbBggXG73ea8885r96eM29Ie3viheWxzhukVF2v509Q0Go1Go3Wl5sf3t/8HT01NNXv37jXV1dUmMzPTTJkyxfPZc889Zz744AOv/sOGDTPp6emmoqLC5OTkmEcffdSEhYV59bn66qvNtm3bTE1NjcnOzjYzZ87sqBP2u/3584/NY5szTM9+fS3/i6XRaDQarSs1X7+/bSd+OO05nU6VlZXJ5XKpvLy8XY/9yBefyB4UpPsvvFzlRcXtemwAALozX7+/WUvIB9+8JcQzLAAAWIHA0orGOVgkybCWEAAAliCwtMIW9M0l4i0hAACsQWBphd0e5PmZeVgAALAGgaUV9m+NsBhGWAAAsASBpRWNs9xKUgPPsAAAYAkCSysa3xCSpIYGbgkBAGAFAksrvN4SaugSU9YAAHDaIbC04tsjLDzDAgCANQgsrWClZgAArEdgaQUrNQMAYD0CSysa52FhllsAAKxDYGmFzTPCwi0hAACsQmBphc3W+AwLIywAAFiFwNKKxmdYWKkZAADrEFhaYT/xlhDPsAAAYB0CSytsJ+Zh4S0hAACsQ2BphZ15WAAAsByBpRWeZ1gYYQEAwDIEllbYTszDwltCAABYh8DSCpv9+OKHzMMCAIB1CCyt8Mx0y0rNAABYhsDSCp5hAQDAegSWVrBaMwAA1iOwtMLOPCwAAFiOwNIKZroFAMB6BJZWsFozAADWI7C0onG1Zt4SAgDAOgSWVtgZYQEAwHIEllbwDAsAANYjsLSC1ZoBALAegaUVrNYMAID1CCytYKZbAACsR2BpBas1AwBgPQJLK+wnVms2hsACAIBVCCytYIQFAADrEVha8c0zLDx0CwCAVQgsrfhmtWZGWAAAsAqBpRWs1gwAgPUILK34ZqZbbgkBAGAVAksrPLeEGGEBAMAyBJZWeEZYWK0ZAADLEFhaYWO1ZgAALEdgaYX9xDwsrNYMAIB1CCyt+GaEhcACAIBVCCytYLVmAACsR2BpReM8LKzWDACAdQgsrbDZji9+yEy3AABYp02BJTU1VXv27FFVVZUyMzOVnJzcYt+pU6fKGNOkDRs2zNNnzpw5zfYJDQ1tS3ntyrOWEKs1AwBgGYe/O8yePVvLli3TrbfeqnXr1unmm29WWlqaRo4cqQMHDrS439ChQ1VWVub5c2FhodfnpaWlXiFGkmpqavwtr92xWjMAANbzO7DceeedWrFihVasWCFJ+vWvf61LL71UqampWrRoUYv7HT58WKWlpS1+boxRQUGBv+V0OFZrBgDAen7dEgoODtaECROUnp7utT09PV2TJ08+6b6bNm3SwYMH9d577+mCCy5o8nmPHj20b98+HThwQG+++abGjRt30uOFhITI6XR6tY7Aas0AAFjPr8ASHR0th8PRZCSkoKBAsbGxze6Tn5+vX/ziF7r66qs1a9Ys7dixQ2vWrNGUKVM8fbZv364bbrhBV1xxha677jpVV1dr3bp1Gjx4cIu1LFy4UGVlZZ6Wl5fnz6n4jNWaAQCwnt+3hKTjt2++zWazNdnWaOfOndq5c6fnzxs2bFD//v31m9/8RmvXrpUkffrpp/r00089fdatW6fPP/9ct99+u+bPn9/scR966CEtXbrU82en09khoYXVmgEAsJ5fIyxFRUWqq6trMpoSExPj1/MnGzZs0JAhQ1r83BijjRs3nrSP2+1WeXm5V+sIrNYMAID1/AostbW1ysrKUkpKitf2lJQUrV+/3ufjjB8/Xvn5+SftM27cuFb7dAZWawYAwHp+3xJaunSpVq1apczMTGVkZOimm25SUlKSnn76aUnSkiVLlJCQoDlz5kiS5s+fr3379mnr1q0KCQnRT3/6U11zzTWaNWuW55iLFy/Whg0b9PXXX8vlculXv/qVxo0bp1/+8pftdJptx2rNAABYz+/Asnr1avXp00eLFy9WXFyctmzZohkzZignJ0eSFBcXp6SkJE//kJAQPfroo0pISFBVVZW2bt2qGTNmKC0tzdMnKipKzzzzjGJjY1VaWqpNmzbp/PPP18aNG9vhFE8NqzUDAGA9m6Quca/D6XSqrKxMLperXZ9nuf7h+3X2Dy7V6//3CX286qV2Oy4AAPD9+5u1hFphY7VmAAAsR2BpBas1AwBgPQJLKzyrNRNYAACwDIGlFd+sJURgAQDAKgSWVrBaMwAA1iOwtILVmgEAsB6BpRV2VmsGAMByBJZW2FitGQAAyxFYWtH4lhCrNQMAYB0CSysa52FpMF1iQmAAAE5LBJZWeFZrZoQFAADLEFha8c1qzTzDAgCAVfxerbm72fj629r12ecq3JdjdSkAAHRbBJZWbHjlNatLAACg2+OWEAAACHgEFgAAEPAILAAAIOARWAAAQMAjsAAAgIBHYAEAAAGPwAIAAAIegQUAAAQ8AgsAAAh4BBYAABDwCCwAACDgEVgAAEDAI7AAAICA1+VWa3Y6nVaXAAAAfOTr93aXCSyNJ5yXl2dxJQAAwF9Op1Pl5eUtfm6TZDqvnI4VHx9/0pNtC6fTqby8PCUkJLT7seGNa905uM6dg+vcebjWnaMjr7PT6dTBgwdP2qfLjLBIavVkT0V5eTn/InQSrnXn4Dp3Dq5z5+Fad46OuM6+HI+HbgEAQMAjsAAAgIBHYGlFTU2N7r//ftXU1FhdSpfHte4cXOfOwXXuPFzrzmH1de5SD90CAICuiREWAAAQ8AgsAAAg4BFYAABAwCOwAACAgEdgaUVqaqr27NmjqqoqZWZmKjk52eqSTitTpkzRG2+8oby8PBljdOWVVzbpc9999ykvL0+VlZX64IMPNHLkSK/PQ0JC9OSTT6qwsFDHjh3T66+/roSEhM46hdPCPffco88++0xlZWUqKCjQv//9bw0dOrRJP671qbnlllv05ZdfqrS0VKWlpVq/fr0uu+wyrz5c4/Z3zz33yBijxx9/3Gs71/rU3XfffTLGeLX8/PwmfQLlOhta82327NmmpqbGzJs3zwwfPtw8/vjjpry83PTv39/y2k6Xdtlll5k//elPZubMmcYYY6688kqvzxcsWGBKS0vNzJkzzahRo8yLL75o8vLyTI8ePTx9li9fbg4cOGAuvvhiM27cOLNmzRqzadMmY7fbLT+/QGlpaWlmzpw5ZuTIkWbMmDHmzTffNPv27TMRERFc63Zsl19+uZk+fboZMmSIGTJkiHnggQdMTU2NGTlyJNe4g9o555xj9uzZY7744gvz+OOPe7Zzrdun3XfffWbz5s2mX79+nhYdHR2o19n6CxaobcOGDWb58uVe27Kzs82SJUssr+10bM0FloMHD5oFCxZ4/hwSEmJKSkrMTTfdZCQZl8tlampqzOzZsz194uLiTF1dnZk2bZrl5xSoLTo62hhjzJQpU7jWHdyKi4vNjTfeyDXugBYZGWl27NhhLr74YvPBBx94BRaudfu0++67z2zatKnFzwPpOnNLqAXBwcGaMGGC0tPTvbanp6dr8uTJFlXVtQwcOFBxcXFe19jtduujjz7yXOMJEyYoJCTEq09+fr62bNnC38NJ9OzZU5J05MgRSVzrjmC323XttdcqMjJSGRkZXOMO8NRTT+mtt97SmjVrvLZzrdvXkCFDlJeXpz179ujFF1/UwIEDJQXede5Six+2p+joaDkcDhUUFHhtLygoUGxsrEVVdS2N17G5azxgwABPn5qaGh09erRJH/4eWrZ06VKtXbtWW7dulcS1bk+jR49WRkaGwsLCdOzYMc2cOVPbtm3TpEmTJHGN28u1116rs88+W+eee26Tz/jnuf18+umn+tnPfqadO3eqX79+uvfee7V+/XqNGjUq4K4zgaUVxhivP9tstibbcGraco35e2jZX//6V40ZM6bZB8S51qdux44dGjdunKKionT11Vdr5cqVmjp1qudzrvGpS0xM1BNPPKFp06addBp4rvWpe+eddzw/b9myRRkZGdq9e7fmzJmjDRs2SAqc68wtoRYUFRWprq6uSUKMiYlpkjbRNocOHZKkk17jQ4cOKTQ0VFFRUS32wTeefPJJXXHFFbrwwguVl5fn2c61bj+1tbXavXu3srKytGjRIn355ZeaP38+17gdTZgwQf369VNWVpZqa2tVW1urCy64QL/61a9UW1vruVZc6/ZXWVmpzZs3a8iQIQH3zzSBpQW1tbXKyspSSkqK1/aUlBStX7/eoqq6lr179yo/P9/rGgcHB2vq1Kmea5yVlSW32+3VJzY2VqNHj+bv4Tv+8pe/aNasWbrooou0b98+r8+41h3HZrMpNDSUa9yO1qxZo9GjR2vcuHGetnHjRr3wwgsaN26c9uzZw7XuICEhIRoxYoTy8/MD8p9py59SDtTW+Frz3LlzzfDhw83SpUtNeXm5SUpKsry206VFRkaasWPHmrFjxxpjjLnjjjvM2LFjPa+GL1iwwJSUlJirrrrKjBo1yrzwwgvNvjKXk5NjLrroIjNu3Djz3nvv8Wrid9pTTz1lSkpKzPnnn+/1emJYWJinD9f61NuDDz5okpOTzYABA8zo0aPNAw88YOrq6swll1zCNe7g9t23hLjW7dMeeeQRc/7555szzjjDnHfeeeaNN94wpaWlnu+5ALvO1l+wQG6pqalm7969prq62mRmZnq9JkprvU2dOtU057nnnvP0ue+++8zBgwdNVVWV+fDDD82oUaO8jhEaGmqefPJJU1RUZCoqKswbb7xhEhMTLT+3QGotmTNnjlc/rvWptWeffdbz/wcFBQXmv//9ryescI07tn03sHCt26c1zqtSU1NjcnNzzauvvmpGjBgRkNfZduIHAACAgMUzLAAAIOARWAAAQMAjsAAAgIBHYAEAAAGPwAIAAAIegQUAAAQ8AgsAAAh4BBYAABDwCCwAACDgEVgAAEDAI7AAAICAR2ABAAAB7/8HK9tXHIBCIC0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.8273333311080933\n",
      "max test accuracy = 0.8196666666666667\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.2495e-01,  1.6174e-01, -6.5118e-01, -6.3574e-01, -6.2947e-01,\n",
      "         -4.2108e-01, -2.8436e-01, -4.7996e-02,  6.8299e-02,  3.6998e-01,\n",
      "          5.9276e-01,  8.2769e-01,  9.3473e-01,  1.0382e+00,  1.0796e+00,\n",
      "          1.1081e+00,  1.0734e+00,  9.6020e-01,  7.0636e-01,  6.5817e-01,\n",
      "          6.8697e-01,  6.9523e-01,  5.8685e-01,  6.8512e-01,  6.4985e-01,\n",
      "          7.0368e-01,  7.8537e-01,  6.5624e-01,  6.4226e-01,  4.0647e-01,\n",
      "          3.0501e-02, -2.3807e-01, -7.9140e-01, -1.2171e+00, -1.7221e+00,\n",
      "         -2.3386e+00, -2.8535e+00, -3.0609e+00, -3.2898e+00, -3.2997e+00,\n",
      "         -3.2672e+00, -3.3168e+00, -3.1674e+00, -3.0335e+00, -2.5445e+00,\n",
      "         -1.5770e+00, -3.2493e-01,  8.5775e-01,  1.8803e+00,  2.6990e+00,\n",
      "          3.3914e+00,  3.7812e+00,  4.2826e+00,  6.0460e+00,  7.5722e+00,\n",
      "          7.6402e+00,  6.6095e+00,  2.7381e+00, -9.8366e-01, -1.8711e+00,\n",
      "         -2.1034e+00, -3.5843e+00, -4.2217e+00, -1.0797e+00,  1.5913e+00,\n",
      "          2.4595e+00, -2.9675e-01, -3.1679e+00, -4.0004e+00,  4.4881e+00,\n",
      "          1.2194e+01,  5.6140e+00, -3.0352e+00, -4.4690e+00, -4.0453e+00,\n",
      "         -3.1555e+00, -2.0132e+00, -1.5183e+00, -1.2154e+00, -1.8035e+00,\n",
      "         -2.6528e+00, -3.3559e+00, -4.0785e+00, -4.5231e+00, -4.1653e+00,\n",
      "         -2.8834e+00, -1.2093e+00,  4.1633e-01,  1.8760e+00,  2.2655e+00,\n",
      "          2.4512e+00,  2.4879e+00,  2.4602e+00,  2.3848e+00,  2.1193e+00,\n",
      "          1.8259e+00,  1.5169e+00,  1.0402e+00,  4.7520e-01, -3.9675e-02,\n",
      "         -3.4661e-01, -5.0338e-01, -6.1521e-01, -5.8035e-01, -6.2595e-01,\n",
      "         -6.1061e-01, -7.2307e-01, -6.4260e-01, -4.6797e-01, -3.8678e-01,\n",
      "         -3.6339e-01, -1.7084e-01, -1.6757e-01, -1.9496e-02, -4.2653e-02,\n",
      "         -1.4726e-01, -7.0246e-02, -2.4706e-02,  4.5984e-03,  1.6355e-02,\n",
      "          1.7041e-04, -9.4534e-02, -2.1163e-01, -2.2168e-01, -4.3295e-01,\n",
      "         -4.2621e-01, -3.8579e-01, -4.3053e-01]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.1493], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hesplitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399b82d93c603256e32b1f50bab1e44304c1a63f6ab0c291ce7489d4e70c0394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
