{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on the MIT-BIH Arrhythmia Dataset \n",
    "*Float Data and Float Neural Net*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print(f'torch version: {torch.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "project_path\n",
    "train_path = project_path / 'data' / 'mit-bih' / 'mitbih_train.hdf5'\n",
    "test_path = project_path / 'data' / 'mit-bih' / 'mitbih_test.hdf5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryECG(Dataset):\n",
    "    \"\"\"The class used by the client to load the dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset ([type]): [description]\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path: Path, test_path: Path, train=True):\n",
    "        if train:\n",
    "            with h5py.File(train_path, 'r') as hdf:\n",
    "                self.x = hdf['x_train'][:]\n",
    "                self.y = hdf['y_train'][:]\n",
    "                self.binary_y = deepcopy(self.y)  # for binary classification\n",
    "                self.binary_y[self.binary_y > 0] = 1\n",
    "        else:\n",
    "            with h5py.File(test_path, 'r') as hdf:\n",
    "                self.x = hdf['x_test'][:]\n",
    "                self.y = hdf['y_test'][:]\n",
    "                self.binary_y = deepcopy(self.y)\n",
    "                self.binary_y[self.binary_y > 0] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.binary_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = BinaryECG(train=True, train_path=train_path, test_path=test_path)\n",
    "test_dataset = BinaryECG(train=False, train_path=train_path, test_path=test_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13245, 1, 128)\n",
      "(13245,)\n",
      "(13245, 1, 128)\n",
      "(13245,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x.shape)\n",
    "print(train_dataset.y.shape)\n",
    "print(test_dataset.x.shape)\n",
    "print(test_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f50bdc95820>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = train_dataset.x[0]\n",
    "print(x0.shape)\n",
    "plt.plot(x0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47613414, 0.5020567 , 0.50298215, 0.46231327, 0.41928844,\n",
       "        0.41842108, 0.43061455, 0.44688531, 0.46408802, 0.47293354,\n",
       "        0.47899632, 0.48582534, 0.48893095, 0.48258178, 0.47326914,\n",
       "        0.46113493, 0.455493  , 0.47573456, 0.49812282, 0.50142268,\n",
       "        0.49909754, 0.4960816 , 0.49134992, 0.4847633 , 0.47915717,\n",
       "        0.47762139, 0.47941395, 0.48882176, 0.49701847, 0.49347912,\n",
       "        0.48651553, 0.48023663, 0.47407637, 0.46977147, 0.46597436,\n",
       "        0.46211728, 0.45808918, 0.45318487, 0.44824447, 0.44322334,\n",
       "        0.43951277, 0.43997245, 0.4410783 , 0.44093789, 0.43854472,\n",
       "        0.42746798, 0.41866831, 0.42362392, 0.43467756, 0.45355196,\n",
       "        0.46949916, 0.46741965, 0.45934733, 0.45182872, 0.44276687,\n",
       "        0.42654805, 0.41805189, 0.43117256, 0.46263137, 0.53157473,\n",
       "        0.59875968, 0.61784347, 0.62479582, 0.63498664, 0.65088522,\n",
       "        0.67939663, 0.6717122 , 0.53892088, 0.38295176, 0.27504905,\n",
       "        0.19901058, 0.18376322, 0.19361621, 0.21105314, 0.22193609,\n",
       "        0.18308223, 0.14579817, 0.15643591, 0.18355273, 0.21700833,\n",
       "        0.25267785, 0.27851548, 0.30314989, 0.33109226, 0.3600533 ,\n",
       "        0.39449314, 0.42292988, 0.43165459, 0.43040441, 0.4204936 ,\n",
       "        0.40817158, 0.39964169, 0.3963274 , 0.40884301, 0.42161387,\n",
       "        0.41734978, 0.4107301 , 0.41234156, 0.41459165, 0.41121749,\n",
       "        0.40965522, 0.42157927, 0.43068396, 0.41869718, 0.40195289,\n",
       "        0.38904382, 0.38004101, 0.38158553, 0.38670215, 0.39322759,\n",
       "        0.39640089, 0.38446916, 0.37183684, 0.37240077, 0.37561081,\n",
       "        0.37670637, 0.37586566, 0.36948463, 0.36176488, 0.35456541,\n",
       "        0.34983307, 0.35228016, 0.35721287, 0.36245956, 0.36675617,\n",
       "        0.36668541, 0.36517786, 0.36366678]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.binary_y = array([1, 1, 1, ..., 1, 1, 0])\n",
      "train_dataset.y = array([2, 4, 2, ..., 1, 4, 0])\n",
      "there are 3000 normal beats and 10245 abnormal beats in the train dataset\n",
      "there are 3000 normal beats and 10245 abnormal beats in the train dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_dataset.binary_y = }\")\n",
    "print(f\"{train_dataset.y = }\")\n",
    "\n",
    "unique, train_counts = np.unique(train_dataset.binary_y, return_counts=True)\n",
    "print(f\"there are {train_counts[0]} normal beats and {train_counts[1]} abnormal beats in the train dataset\")\n",
    "unique, test_counts = np.unique(test_dataset.binary_y, return_counts=True)\n",
    "print(f\"there are {test_counts[0]} normal beats and {test_counts[1]} abnormal beats in the train dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters, random seed and the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 500 \n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: NVIDIA GeForce RTX 2060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'device: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "lr_model = LinearModel(128, 1)\n",
    "lr_model.to(device)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0007,  0.0474, -0.0727, -0.0650, -0.0340,  0.0237, -0.0018,  0.0701,\n",
      "         -0.0078,  0.0234, -0.0267, -0.0174, -0.0844, -0.0585, -0.0364,  0.0033,\n",
      "          0.0349,  0.0530, -0.0599, -0.0385,  0.0321,  0.0734, -0.0182,  0.0661,\n",
      "         -0.0142,  0.0094,  0.0800, -0.0820, -0.0556, -0.0224, -0.0345,  0.0764,\n",
      "         -0.0573, -0.0407, -0.0618, -0.0828, -0.0516,  0.0760,  0.0394,  0.0428,\n",
      "          0.0046, -0.0453,  0.0150, -0.0825, -0.0639, -0.0456,  0.0558,  0.0518,\n",
      "         -0.0392, -0.0032,  0.0565,  0.0879,  0.0351,  0.0119,  0.0593, -0.0520,\n",
      "          0.0165, -0.0685, -0.0613, -0.0457,  0.0400,  0.0355, -0.0524,  0.0267,\n",
      "          0.0485, -0.0112,  0.0034,  0.0205,  0.0548,  0.0849, -0.0681, -0.0324,\n",
      "          0.0347,  0.0732,  0.0769,  0.0780,  0.0176, -0.0769,  0.0081, -0.0553,\n",
      "         -0.0824,  0.0785,  0.0672, -0.0882,  0.0165, -0.0149, -0.0145, -0.0405,\n",
      "          0.0340, -0.0524,  0.0324,  0.0447,  0.0633,  0.0330, -0.0875, -0.0573,\n",
      "          0.0441,  0.0185, -0.0690, -0.0509,  0.0832,  0.0596, -0.0385, -0.0222,\n",
      "         -0.0842, -0.0016, -0.0666, -0.0682, -0.0049,  0.0133, -0.0362,  0.0524,\n",
      "         -0.0538,  0.0802,  0.0606, -0.0745, -0.0220,  0.0040,  0.0129,  0.0210,\n",
      "          0.0347,  0.0053, -0.0431,  0.0418, -0.0848, -0.0524, -0.0221, -0.0431]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0309], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 10232 / 13245\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7725\n",
      "num_test_corrects / test_total_examples = 10245 / 13245\n",
      "testing accuracy = 0.7735\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 10253 / 13245\n",
      "training loss = 0.4939\n",
      "training accuracy = 0.7741\n",
      "num_test_corrects / test_total_examples = 10251 / 13245\n",
      "testing accuracy = 0.7740\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 10283 / 13245\n",
      "training loss = 0.4756\n",
      "training accuracy = 0.7764\n",
      "num_test_corrects / test_total_examples = 10275 / 13245\n",
      "testing accuracy = 0.7758\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 10311 / 13245\n",
      "training loss = 0.4627\n",
      "training accuracy = 0.7785\n",
      "num_test_corrects / test_total_examples = 10274 / 13245\n",
      "testing accuracy = 0.7757\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 10325 / 13245\n",
      "training loss = 0.4530\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 10307 / 13245\n",
      "testing accuracy = 0.7782\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 10372 / 13245\n",
      "training loss = 0.4453\n",
      "training accuracy = 0.7831\n",
      "num_test_corrects / test_total_examples = 10371 / 13245\n",
      "testing accuracy = 0.7830\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 10441 / 13245\n",
      "training loss = 0.4390\n",
      "training accuracy = 0.7883\n",
      "num_test_corrects / test_total_examples = 10465 / 13245\n",
      "testing accuracy = 0.7901\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 10511 / 13245\n",
      "training loss = 0.4336\n",
      "training accuracy = 0.7936\n",
      "num_test_corrects / test_total_examples = 10551 / 13245\n",
      "testing accuracy = 0.7966\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 10574 / 13245\n",
      "training loss = 0.4290\n",
      "training accuracy = 0.7983\n",
      "num_test_corrects / test_total_examples = 10597 / 13245\n",
      "testing accuracy = 0.8001\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 10635 / 13245\n",
      "training loss = 0.4249\n",
      "training accuracy = 0.8029\n",
      "num_test_corrects / test_total_examples = 10639 / 13245\n",
      "testing accuracy = 0.8032\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 10689 / 13245\n",
      "training loss = 0.4212\n",
      "training accuracy = 0.8070\n",
      "num_test_corrects / test_total_examples = 10688 / 13245\n",
      "testing accuracy = 0.8069\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 10729 / 13245\n",
      "training loss = 0.4178\n",
      "training accuracy = 0.8100\n",
      "num_test_corrects / test_total_examples = 10732 / 13245\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 10764 / 13245\n",
      "training loss = 0.4148\n",
      "training accuracy = 0.8127\n",
      "num_test_corrects / test_total_examples = 10776 / 13245\n",
      "testing accuracy = 0.8136\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 10811 / 13245\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 10813 / 13245\n",
      "testing accuracy = 0.8164\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 10840 / 13245\n",
      "training loss = 0.4093\n",
      "training accuracy = 0.8184\n",
      "num_test_corrects / test_total_examples = 10848 / 13245\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 10876 / 13245\n",
      "training loss = 0.4068\n",
      "training accuracy = 0.8211\n",
      "num_test_corrects / test_total_examples = 10875 / 13245\n",
      "testing accuracy = 0.8211\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 10895 / 13245\n",
      "training loss = 0.4045\n",
      "training accuracy = 0.8226\n",
      "num_test_corrects / test_total_examples = 10897 / 13245\n",
      "testing accuracy = 0.8227\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 10919 / 13245\n",
      "training loss = 0.4023\n",
      "training accuracy = 0.8244\n",
      "num_test_corrects / test_total_examples = 10938 / 13245\n",
      "testing accuracy = 0.8258\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 10942 / 13245\n",
      "training loss = 0.4003\n",
      "training accuracy = 0.8261\n",
      "num_test_corrects / test_total_examples = 10971 / 13245\n",
      "testing accuracy = 0.8283\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 10974 / 13245\n",
      "training loss = 0.3983\n",
      "training accuracy = 0.8285\n",
      "num_test_corrects / test_total_examples = 10992 / 13245\n",
      "testing accuracy = 0.8299\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 10994 / 13245\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8300\n",
      "num_test_corrects / test_total_examples = 11010 / 13245\n",
      "testing accuracy = 0.8313\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 11012 / 13245\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8314\n",
      "num_test_corrects / test_total_examples = 11026 / 13245\n",
      "testing accuracy = 0.8325\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 11033 / 13245\n",
      "training loss = 0.3930\n",
      "training accuracy = 0.8330\n",
      "num_test_corrects / test_total_examples = 11044 / 13245\n",
      "testing accuracy = 0.8338\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 11050 / 13245\n",
      "training loss = 0.3914\n",
      "training accuracy = 0.8343\n",
      "num_test_corrects / test_total_examples = 11058 / 13245\n",
      "testing accuracy = 0.8349\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 11064 / 13245\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8353\n",
      "num_test_corrects / test_total_examples = 11075 / 13245\n",
      "testing accuracy = 0.8362\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 11077 / 13245\n",
      "training loss = 0.3884\n",
      "training accuracy = 0.8363\n",
      "num_test_corrects / test_total_examples = 11092 / 13245\n",
      "testing accuracy = 0.8374\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 11092 / 13245\n",
      "training loss = 0.3870\n",
      "training accuracy = 0.8374\n",
      "num_test_corrects / test_total_examples = 11107 / 13245\n",
      "testing accuracy = 0.8386\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 11105 / 13245\n",
      "training loss = 0.3857\n",
      "training accuracy = 0.8384\n",
      "num_test_corrects / test_total_examples = 11124 / 13245\n",
      "testing accuracy = 0.8399\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 11118 / 13245\n",
      "training loss = 0.3844\n",
      "training accuracy = 0.8394\n",
      "num_test_corrects / test_total_examples = 11139 / 13245\n",
      "testing accuracy = 0.8410\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 11130 / 13245\n",
      "training loss = 0.3831\n",
      "training accuracy = 0.8403\n",
      "num_test_corrects / test_total_examples = 11152 / 13245\n",
      "testing accuracy = 0.8420\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 11149 / 13245\n",
      "training loss = 0.3819\n",
      "training accuracy = 0.8418\n",
      "num_test_corrects / test_total_examples = 11167 / 13245\n",
      "testing accuracy = 0.8431\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 11157 / 13245\n",
      "training loss = 0.3808\n",
      "training accuracy = 0.8424\n",
      "num_test_corrects / test_total_examples = 11171 / 13245\n",
      "testing accuracy = 0.8434\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 11167 / 13245\n",
      "training loss = 0.3797\n",
      "training accuracy = 0.8431\n",
      "num_test_corrects / test_total_examples = 11179 / 13245\n",
      "testing accuracy = 0.8440\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 11177 / 13245\n",
      "training loss = 0.3786\n",
      "training accuracy = 0.8439\n",
      "num_test_corrects / test_total_examples = 11194 / 13245\n",
      "testing accuracy = 0.8451\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 11191 / 13245\n",
      "training loss = 0.3775\n",
      "training accuracy = 0.8449\n",
      "num_test_corrects / test_total_examples = 11199 / 13245\n",
      "testing accuracy = 0.8455\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 11200 / 13245\n",
      "training loss = 0.3765\n",
      "training accuracy = 0.8456\n",
      "num_test_corrects / test_total_examples = 11208 / 13245\n",
      "testing accuracy = 0.8462\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 11206 / 13245\n",
      "training loss = 0.3755\n",
      "training accuracy = 0.8461\n",
      "num_test_corrects / test_total_examples = 11222 / 13245\n",
      "testing accuracy = 0.8473\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 11216 / 13245\n",
      "training loss = 0.3746\n",
      "training accuracy = 0.8468\n",
      "num_test_corrects / test_total_examples = 11226 / 13245\n",
      "testing accuracy = 0.8476\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 11225 / 13245\n",
      "training loss = 0.3737\n",
      "training accuracy = 0.8475\n",
      "num_test_corrects / test_total_examples = 11233 / 13245\n",
      "testing accuracy = 0.8481\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 11231 / 13245\n",
      "training loss = 0.3728\n",
      "training accuracy = 0.8479\n",
      "num_test_corrects / test_total_examples = 11242 / 13245\n",
      "testing accuracy = 0.8488\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 11240 / 13245\n",
      "training loss = 0.3719\n",
      "training accuracy = 0.8486\n",
      "num_test_corrects / test_total_examples = 11249 / 13245\n",
      "testing accuracy = 0.8493\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 11243 / 13245\n",
      "training loss = 0.3711\n",
      "training accuracy = 0.8488\n",
      "num_test_corrects / test_total_examples = 11256 / 13245\n",
      "testing accuracy = 0.8498\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 11252 / 13245\n",
      "training loss = 0.3702\n",
      "training accuracy = 0.8495\n",
      "num_test_corrects / test_total_examples = 11264 / 13245\n",
      "testing accuracy = 0.8504\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 11263 / 13245\n",
      "training loss = 0.3694\n",
      "training accuracy = 0.8504\n",
      "num_test_corrects / test_total_examples = 11266 / 13245\n",
      "testing accuracy = 0.8506\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 11275 / 13245\n",
      "training loss = 0.3687\n",
      "training accuracy = 0.8513\n",
      "num_test_corrects / test_total_examples = 11268 / 13245\n",
      "testing accuracy = 0.8507\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 11278 / 13245\n",
      "training loss = 0.3679\n",
      "training accuracy = 0.8515\n",
      "num_test_corrects / test_total_examples = 11275 / 13245\n",
      "testing accuracy = 0.8513\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 11282 / 13245\n",
      "training loss = 0.3672\n",
      "training accuracy = 0.8518\n",
      "num_test_corrects / test_total_examples = 11280 / 13245\n",
      "testing accuracy = 0.8516\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 11292 / 13245\n",
      "training loss = 0.3664\n",
      "training accuracy = 0.8525\n",
      "num_test_corrects / test_total_examples = 11281 / 13245\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 11303 / 13245\n",
      "training loss = 0.3657\n",
      "training accuracy = 0.8534\n",
      "num_test_corrects / test_total_examples = 11288 / 13245\n",
      "testing accuracy = 0.8522\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 11311 / 13245\n",
      "training loss = 0.3651\n",
      "training accuracy = 0.8540\n",
      "num_test_corrects / test_total_examples = 11291 / 13245\n",
      "testing accuracy = 0.8525\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 11318 / 13245\n",
      "training loss = 0.3644\n",
      "training accuracy = 0.8545\n",
      "num_test_corrects / test_total_examples = 11296 / 13245\n",
      "testing accuracy = 0.8529\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 11321 / 13245\n",
      "training loss = 0.3637\n",
      "training accuracy = 0.8547\n",
      "num_test_corrects / test_total_examples = 11304 / 13245\n",
      "testing accuracy = 0.8535\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 11325 / 13245\n",
      "training loss = 0.3631\n",
      "training accuracy = 0.8550\n",
      "num_test_corrects / test_total_examples = 11308 / 13245\n",
      "testing accuracy = 0.8538\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 11330 / 13245\n",
      "training loss = 0.3625\n",
      "training accuracy = 0.8554\n",
      "num_test_corrects / test_total_examples = 11315 / 13245\n",
      "testing accuracy = 0.8543\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 11337 / 13245\n",
      "training loss = 0.3619\n",
      "training accuracy = 0.8559\n",
      "num_test_corrects / test_total_examples = 11318 / 13245\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 11340 / 13245\n",
      "training loss = 0.3613\n",
      "training accuracy = 0.8562\n",
      "num_test_corrects / test_total_examples = 11322 / 13245\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 11348 / 13245\n",
      "training loss = 0.3607\n",
      "training accuracy = 0.8568\n",
      "num_test_corrects / test_total_examples = 11328 / 13245\n",
      "testing accuracy = 0.8553\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 11354 / 13245\n",
      "training loss = 0.3601\n",
      "training accuracy = 0.8572\n",
      "num_test_corrects / test_total_examples = 11325 / 13245\n",
      "testing accuracy = 0.8550\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 11357 / 13245\n",
      "training loss = 0.3596\n",
      "training accuracy = 0.8575\n",
      "num_test_corrects / test_total_examples = 11332 / 13245\n",
      "testing accuracy = 0.8556\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 11366 / 13245\n",
      "training loss = 0.3590\n",
      "training accuracy = 0.8581\n",
      "num_test_corrects / test_total_examples = 11337 / 13245\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 11374 / 13245\n",
      "training loss = 0.3585\n",
      "training accuracy = 0.8587\n",
      "num_test_corrects / test_total_examples = 11337 / 13245\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 11381 / 13245\n",
      "training loss = 0.3580\n",
      "training accuracy = 0.8593\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 11384 / 13245\n",
      "training loss = 0.3574\n",
      "training accuracy = 0.8595\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 11390 / 13245\n",
      "training loss = 0.3569\n",
      "training accuracy = 0.8599\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 11399 / 13245\n",
      "training loss = 0.3564\n",
      "training accuracy = 0.8606\n",
      "num_test_corrects / test_total_examples = 11345 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 11402 / 13245\n",
      "training loss = 0.3559\n",
      "training accuracy = 0.8609\n",
      "num_test_corrects / test_total_examples = 11347 / 13245\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 11404 / 13245\n",
      "training loss = 0.3555\n",
      "training accuracy = 0.8610\n",
      "num_test_corrects / test_total_examples = 11355 / 13245\n",
      "testing accuracy = 0.8573\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 11406 / 13245\n",
      "training loss = 0.3550\n",
      "training accuracy = 0.8612\n",
      "num_test_corrects / test_total_examples = 11358 / 13245\n",
      "testing accuracy = 0.8575\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 11415 / 13245\n",
      "training loss = 0.3545\n",
      "training accuracy = 0.8618\n",
      "num_test_corrects / test_total_examples = 11360 / 13245\n",
      "testing accuracy = 0.8577\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 11424 / 13245\n",
      "training loss = 0.3541\n",
      "training accuracy = 0.8625\n",
      "num_test_corrects / test_total_examples = 11368 / 13245\n",
      "testing accuracy = 0.8583\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 11434 / 13245\n",
      "training loss = 0.3536\n",
      "training accuracy = 0.8633\n",
      "num_test_corrects / test_total_examples = 11375 / 13245\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 11439 / 13245\n",
      "training loss = 0.3532\n",
      "training accuracy = 0.8636\n",
      "num_test_corrects / test_total_examples = 11377 / 13245\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 11447 / 13245\n",
      "training loss = 0.3528\n",
      "training accuracy = 0.8643\n",
      "num_test_corrects / test_total_examples = 11382 / 13245\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 11454 / 13245\n",
      "training loss = 0.3524\n",
      "training accuracy = 0.8648\n",
      "num_test_corrects / test_total_examples = 11385 / 13245\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 11459 / 13245\n",
      "training loss = 0.3519\n",
      "training accuracy = 0.8652\n",
      "num_test_corrects / test_total_examples = 11388 / 13245\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 11474 / 13245\n",
      "training loss = 0.3515\n",
      "training accuracy = 0.8663\n",
      "num_test_corrects / test_total_examples = 11388 / 13245\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 11476 / 13245\n",
      "training loss = 0.3511\n",
      "training accuracy = 0.8664\n",
      "num_test_corrects / test_total_examples = 11389 / 13245\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 11481 / 13245\n",
      "training loss = 0.3507\n",
      "training accuracy = 0.8668\n",
      "num_test_corrects / test_total_examples = 11392 / 13245\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 11481 / 13245\n",
      "training loss = 0.3503\n",
      "training accuracy = 0.8668\n",
      "num_test_corrects / test_total_examples = 11396 / 13245\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 11486 / 13245\n",
      "training loss = 0.3500\n",
      "training accuracy = 0.8672\n",
      "num_test_corrects / test_total_examples = 11398 / 13245\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 11493 / 13245\n",
      "training loss = 0.3496\n",
      "training accuracy = 0.8677\n",
      "num_test_corrects / test_total_examples = 11403 / 13245\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 11497 / 13245\n",
      "training loss = 0.3492\n",
      "training accuracy = 0.8680\n",
      "num_test_corrects / test_total_examples = 11405 / 13245\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 11500 / 13245\n",
      "training loss = 0.3489\n",
      "training accuracy = 0.8683\n",
      "num_test_corrects / test_total_examples = 11411 / 13245\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 11504 / 13245\n",
      "training loss = 0.3485\n",
      "training accuracy = 0.8686\n",
      "num_test_corrects / test_total_examples = 11419 / 13245\n",
      "testing accuracy = 0.8621\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 11512 / 13245\n",
      "training loss = 0.3481\n",
      "training accuracy = 0.8692\n",
      "num_test_corrects / test_total_examples = 11425 / 13245\n",
      "testing accuracy = 0.8626\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 11515 / 13245\n",
      "training loss = 0.3478\n",
      "training accuracy = 0.8694\n",
      "num_test_corrects / test_total_examples = 11428 / 13245\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 11528 / 13245\n",
      "training loss = 0.3474\n",
      "training accuracy = 0.8704\n",
      "num_test_corrects / test_total_examples = 11432 / 13245\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 11531 / 13245\n",
      "training loss = 0.3471\n",
      "training accuracy = 0.8706\n",
      "num_test_corrects / test_total_examples = 11437 / 13245\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 11537 / 13245\n",
      "training loss = 0.3468\n",
      "training accuracy = 0.8710\n",
      "num_test_corrects / test_total_examples = 11441 / 13245\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 11539 / 13245\n",
      "training loss = 0.3464\n",
      "training accuracy = 0.8712\n",
      "num_test_corrects / test_total_examples = 11446 / 13245\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 11540 / 13245\n",
      "training loss = 0.3461\n",
      "training accuracy = 0.8713\n",
      "num_test_corrects / test_total_examples = 11447 / 13245\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 11544 / 13245\n",
      "training loss = 0.3458\n",
      "training accuracy = 0.8716\n",
      "num_test_corrects / test_total_examples = 11450 / 13245\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 11545 / 13245\n",
      "training loss = 0.3455\n",
      "training accuracy = 0.8716\n",
      "num_test_corrects / test_total_examples = 11454 / 13245\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 11550 / 13245\n",
      "training loss = 0.3452\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 11456 / 13245\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 11550 / 13245\n",
      "training loss = 0.3449\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 11465 / 13245\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 11555 / 13245\n",
      "training loss = 0.3446\n",
      "training accuracy = 0.8724\n",
      "num_test_corrects / test_total_examples = 11466 / 13245\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3443\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11465 / 13245\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3440\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11467 / 13245\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3437\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11471 / 13245\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 11554 / 13245\n",
      "training loss = 0.3434\n",
      "training accuracy = 0.8723\n",
      "num_test_corrects / test_total_examples = 11480 / 13245\n",
      "testing accuracy = 0.8667\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 11558 / 13245\n",
      "training loss = 0.3431\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11485 / 13245\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 11556 / 13245\n",
      "training loss = 0.3428\n",
      "training accuracy = 0.8725\n",
      "num_test_corrects / test_total_examples = 11486 / 13245\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3425\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11487 / 13245\n",
      "testing accuracy = 0.8673\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3423\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11492 / 13245\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 11561 / 13245\n",
      "training loss = 0.3420\n",
      "training accuracy = 0.8729\n",
      "num_test_corrects / test_total_examples = 11495 / 13245\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 11565 / 13245\n",
      "training loss = 0.3417\n",
      "training accuracy = 0.8732\n",
      "num_test_corrects / test_total_examples = 11501 / 13245\n",
      "testing accuracy = 0.8683\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 11568 / 13245\n",
      "training loss = 0.3415\n",
      "training accuracy = 0.8734\n",
      "num_test_corrects / test_total_examples = 11502 / 13245\n",
      "testing accuracy = 0.8684\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 11571 / 13245\n",
      "training loss = 0.3412\n",
      "training accuracy = 0.8736\n",
      "num_test_corrects / test_total_examples = 11508 / 13245\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 11573 / 13245\n",
      "training loss = 0.3410\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 11508 / 13245\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 11577 / 13245\n",
      "training loss = 0.3407\n",
      "training accuracy = 0.8741\n",
      "num_test_corrects / test_total_examples = 11512 / 13245\n",
      "testing accuracy = 0.8692\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 11579 / 13245\n",
      "training loss = 0.3405\n",
      "training accuracy = 0.8742\n",
      "num_test_corrects / test_total_examples = 11515 / 13245\n",
      "testing accuracy = 0.8694\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 11582 / 13245\n",
      "training loss = 0.3402\n",
      "training accuracy = 0.8744\n",
      "num_test_corrects / test_total_examples = 11519 / 13245\n",
      "testing accuracy = 0.8697\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 11590 / 13245\n",
      "training loss = 0.3400\n",
      "training accuracy = 0.8750\n",
      "num_test_corrects / test_total_examples = 11521 / 13245\n",
      "testing accuracy = 0.8698\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 11596 / 13245\n",
      "training loss = 0.3397\n",
      "training accuracy = 0.8755\n",
      "num_test_corrects / test_total_examples = 11523 / 13245\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 11601 / 13245\n",
      "training loss = 0.3395\n",
      "training accuracy = 0.8759\n",
      "num_test_corrects / test_total_examples = 11523 / 13245\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 11606 / 13245\n",
      "training loss = 0.3392\n",
      "training accuracy = 0.8763\n",
      "num_test_corrects / test_total_examples = 11524 / 13245\n",
      "testing accuracy = 0.8701\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 11609 / 13245\n",
      "training loss = 0.3390\n",
      "training accuracy = 0.8765\n",
      "num_test_corrects / test_total_examples = 11529 / 13245\n",
      "testing accuracy = 0.8704\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 11611 / 13245\n",
      "training loss = 0.3388\n",
      "training accuracy = 0.8766\n",
      "num_test_corrects / test_total_examples = 11535 / 13245\n",
      "testing accuracy = 0.8709\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 11615 / 13245\n",
      "training loss = 0.3385\n",
      "training accuracy = 0.8769\n",
      "num_test_corrects / test_total_examples = 11533 / 13245\n",
      "testing accuracy = 0.8707\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 11618 / 13245\n",
      "training loss = 0.3383\n",
      "training accuracy = 0.8772\n",
      "num_test_corrects / test_total_examples = 11541 / 13245\n",
      "testing accuracy = 0.8713\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 11620 / 13245\n",
      "training loss = 0.3381\n",
      "training accuracy = 0.8773\n",
      "num_test_corrects / test_total_examples = 11547 / 13245\n",
      "testing accuracy = 0.8718\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 11627 / 13245\n",
      "training loss = 0.3379\n",
      "training accuracy = 0.8778\n",
      "num_test_corrects / test_total_examples = 11551 / 13245\n",
      "testing accuracy = 0.8721\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 11629 / 13245\n",
      "training loss = 0.3377\n",
      "training accuracy = 0.8780\n",
      "num_test_corrects / test_total_examples = 11554 / 13245\n",
      "testing accuracy = 0.8723\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 11630 / 13245\n",
      "training loss = 0.3374\n",
      "training accuracy = 0.8781\n",
      "num_test_corrects / test_total_examples = 11562 / 13245\n",
      "testing accuracy = 0.8729\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 11632 / 13245\n",
      "training loss = 0.3372\n",
      "training accuracy = 0.8782\n",
      "num_test_corrects / test_total_examples = 11564 / 13245\n",
      "testing accuracy = 0.8731\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 11632 / 13245\n",
      "training loss = 0.3370\n",
      "training accuracy = 0.8782\n",
      "num_test_corrects / test_total_examples = 11567 / 13245\n",
      "testing accuracy = 0.8733\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3368\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11576 / 13245\n",
      "testing accuracy = 0.8740\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3366\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11575 / 13245\n",
      "testing accuracy = 0.8739\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3364\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11579 / 13245\n",
      "testing accuracy = 0.8742\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 11636 / 13245\n",
      "training loss = 0.3362\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 11580 / 13245\n",
      "testing accuracy = 0.8743\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 11640 / 13245\n",
      "training loss = 0.3360\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 11580 / 13245\n",
      "testing accuracy = 0.8743\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 11643 / 13245\n",
      "training loss = 0.3358\n",
      "training accuracy = 0.8790\n",
      "num_test_corrects / test_total_examples = 11582 / 13245\n",
      "testing accuracy = 0.8744\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 11648 / 13245\n",
      "training loss = 0.3356\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11582 / 13245\n",
      "testing accuracy = 0.8744\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 11647 / 13245\n",
      "training loss = 0.3354\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11584 / 13245\n",
      "testing accuracy = 0.8746\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 11648 / 13245\n",
      "training loss = 0.3352\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11586 / 13245\n",
      "testing accuracy = 0.8747\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 11650 / 13245\n",
      "training loss = 0.3350\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 11586 / 13245\n",
      "testing accuracy = 0.8747\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3348\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11588 / 13245\n",
      "testing accuracy = 0.8749\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3346\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11590 / 13245\n",
      "testing accuracy = 0.8750\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 11650 / 13245\n",
      "training loss = 0.3344\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 11590 / 13245\n",
      "testing accuracy = 0.8750\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3343\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11593 / 13245\n",
      "testing accuracy = 0.8753\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 11657 / 13245\n",
      "training loss = 0.3341\n",
      "training accuracy = 0.8801\n",
      "num_test_corrects / test_total_examples = 11592 / 13245\n",
      "testing accuracy = 0.8752\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 11658 / 13245\n",
      "training loss = 0.3339\n",
      "training accuracy = 0.8802\n",
      "num_test_corrects / test_total_examples = 11594 / 13245\n",
      "testing accuracy = 0.8753\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 11659 / 13245\n",
      "training loss = 0.3337\n",
      "training accuracy = 0.8803\n",
      "num_test_corrects / test_total_examples = 11596 / 13245\n",
      "testing accuracy = 0.8755\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 11664 / 13245\n",
      "training loss = 0.3335\n",
      "training accuracy = 0.8806\n",
      "num_test_corrects / test_total_examples = 11603 / 13245\n",
      "testing accuracy = 0.8760\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 11666 / 13245\n",
      "training loss = 0.3334\n",
      "training accuracy = 0.8808\n",
      "num_test_corrects / test_total_examples = 11609 / 13245\n",
      "testing accuracy = 0.8765\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3332\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11611 / 13245\n",
      "testing accuracy = 0.8766\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3330\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11612 / 13245\n",
      "testing accuracy = 0.8767\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3328\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11613 / 13245\n",
      "testing accuracy = 0.8768\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 11672 / 13245\n",
      "training loss = 0.3327\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11615 / 13245\n",
      "testing accuracy = 0.8769\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 11671 / 13245\n",
      "training loss = 0.3325\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11618 / 13245\n",
      "testing accuracy = 0.8772\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 11673 / 13245\n",
      "training loss = 0.3323\n",
      "training accuracy = 0.8813\n",
      "num_test_corrects / test_total_examples = 11623 / 13245\n",
      "testing accuracy = 0.8775\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 11674 / 13245\n",
      "training loss = 0.3322\n",
      "training accuracy = 0.8814\n",
      "num_test_corrects / test_total_examples = 11627 / 13245\n",
      "testing accuracy = 0.8778\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 11671 / 13245\n",
      "training loss = 0.3320\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11631 / 13245\n",
      "testing accuracy = 0.8781\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 11673 / 13245\n",
      "training loss = 0.3318\n",
      "training accuracy = 0.8813\n",
      "num_test_corrects / test_total_examples = 11632 / 13245\n",
      "testing accuracy = 0.8782\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 11675 / 13245\n",
      "training loss = 0.3317\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 11635 / 13245\n",
      "testing accuracy = 0.8784\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 11676 / 13245\n",
      "training loss = 0.3315\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 11636 / 13245\n",
      "testing accuracy = 0.8785\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 11679 / 13245\n",
      "training loss = 0.3314\n",
      "training accuracy = 0.8818\n",
      "num_test_corrects / test_total_examples = 11640 / 13245\n",
      "testing accuracy = 0.8788\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 11677 / 13245\n",
      "training loss = 0.3312\n",
      "training accuracy = 0.8816\n",
      "num_test_corrects / test_total_examples = 11641 / 13245\n",
      "testing accuracy = 0.8789\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 11678 / 13245\n",
      "training loss = 0.3311\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 11642 / 13245\n",
      "testing accuracy = 0.8790\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 11678 / 13245\n",
      "training loss = 0.3309\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 11642 / 13245\n",
      "testing accuracy = 0.8790\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 11682 / 13245\n",
      "training loss = 0.3308\n",
      "training accuracy = 0.8820\n",
      "num_test_corrects / test_total_examples = 11644 / 13245\n",
      "testing accuracy = 0.8791\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 11684 / 13245\n",
      "training loss = 0.3306\n",
      "training accuracy = 0.8821\n",
      "num_test_corrects / test_total_examples = 11647 / 13245\n",
      "testing accuracy = 0.8794\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 11684 / 13245\n",
      "training loss = 0.3304\n",
      "training accuracy = 0.8821\n",
      "num_test_corrects / test_total_examples = 11651 / 13245\n",
      "testing accuracy = 0.8797\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 11690 / 13245\n",
      "training loss = 0.3303\n",
      "training accuracy = 0.8826\n",
      "num_test_corrects / test_total_examples = 11652 / 13245\n",
      "testing accuracy = 0.8797\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 11691 / 13245\n",
      "training loss = 0.3302\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 11656 / 13245\n",
      "testing accuracy = 0.8800\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 11692 / 13245\n",
      "training loss = 0.3300\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 11660 / 13245\n",
      "testing accuracy = 0.8803\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 11694 / 13245\n",
      "training loss = 0.3299\n",
      "training accuracy = 0.8829\n",
      "num_test_corrects / test_total_examples = 11659 / 13245\n",
      "testing accuracy = 0.8803\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 11697 / 13245\n",
      "training loss = 0.3297\n",
      "training accuracy = 0.8831\n",
      "num_test_corrects / test_total_examples = 11661 / 13245\n",
      "testing accuracy = 0.8804\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 11700 / 13245\n",
      "training loss = 0.3296\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11661 / 13245\n",
      "testing accuracy = 0.8804\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 11701 / 13245\n",
      "training loss = 0.3294\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11662 / 13245\n",
      "testing accuracy = 0.8805\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 11701 / 13245\n",
      "training loss = 0.3293\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11662 / 13245\n",
      "testing accuracy = 0.8805\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 11703 / 13245\n",
      "training loss = 0.3291\n",
      "training accuracy = 0.8836\n",
      "num_test_corrects / test_total_examples = 11663 / 13245\n",
      "testing accuracy = 0.8806\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3290\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11664 / 13245\n",
      "testing accuracy = 0.8806\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 11707 / 13245\n",
      "training loss = 0.3289\n",
      "training accuracy = 0.8839\n",
      "num_test_corrects / test_total_examples = 11665 / 13245\n",
      "testing accuracy = 0.8807\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3287\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11668 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3286\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11671 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3285\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11670 / 13245\n",
      "testing accuracy = 0.8811\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3283\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3282\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11668 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3281\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 11711 / 13245\n",
      "training loss = 0.3279\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 11709 / 13245\n",
      "training loss = 0.3278\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11667 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 11708 / 13245\n",
      "training loss = 0.3277\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 11709 / 13245\n",
      "training loss = 0.3275\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11672 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 11711 / 13245\n",
      "training loss = 0.3274\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 11672 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 11712 / 13245\n",
      "training loss = 0.3273\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 11673 / 13245\n",
      "testing accuracy = 0.8813\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 11714 / 13245\n",
      "training loss = 0.3272\n",
      "training accuracy = 0.8844\n",
      "num_test_corrects / test_total_examples = 11674 / 13245\n",
      "testing accuracy = 0.8814\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 11715 / 13245\n",
      "training loss = 0.3270\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 11675 / 13245\n",
      "testing accuracy = 0.8815\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 11715 / 13245\n",
      "training loss = 0.3269\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 11676 / 13245\n",
      "testing accuracy = 0.8815\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 11716 / 13245\n",
      "training loss = 0.3268\n",
      "training accuracy = 0.8846\n",
      "num_test_corrects / test_total_examples = 11679 / 13245\n",
      "testing accuracy = 0.8818\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 11718 / 13245\n",
      "training loss = 0.3267\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 11680 / 13245\n",
      "testing accuracy = 0.8818\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 11719 / 13245\n",
      "training loss = 0.3266\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 11720 / 13245\n",
      "training loss = 0.3264\n",
      "training accuracy = 0.8849\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 11723 / 13245\n",
      "training loss = 0.3263\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 11724 / 13245\n",
      "training loss = 0.3262\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 11682 / 13245\n",
      "testing accuracy = 0.8820\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 11723 / 13245\n",
      "training loss = 0.3261\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 11684 / 13245\n",
      "testing accuracy = 0.8821\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 11725 / 13245\n",
      "training loss = 0.3260\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 11684 / 13245\n",
      "testing accuracy = 0.8821\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 11726 / 13245\n",
      "training loss = 0.3259\n",
      "training accuracy = 0.8853\n",
      "num_test_corrects / test_total_examples = 11686 / 13245\n",
      "testing accuracy = 0.8823\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3257\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11685 / 13245\n",
      "testing accuracy = 0.8822\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 11730 / 13245\n",
      "training loss = 0.3256\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 11688 / 13245\n",
      "testing accuracy = 0.8824\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 11731 / 13245\n",
      "training loss = 0.3255\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 11688 / 13245\n",
      "testing accuracy = 0.8824\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 11732 / 13245\n",
      "training loss = 0.3254\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 11690 / 13245\n",
      "testing accuracy = 0.8826\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 11731 / 13245\n",
      "training loss = 0.3253\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 11692 / 13245\n",
      "testing accuracy = 0.8827\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3252\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3251\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3250\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 11729 / 13245\n",
      "training loss = 0.3248\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11696 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 11730 / 13245\n",
      "training loss = 0.3247\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 11696 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 11729 / 13245\n",
      "training loss = 0.3246\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 11732 / 13245\n",
      "training loss = 0.3245\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 11734 / 13245\n",
      "training loss = 0.3244\n",
      "training accuracy = 0.8859\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 11737 / 13245\n",
      "training loss = 0.3243\n",
      "training accuracy = 0.8861\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 11738 / 13245\n",
      "training loss = 0.3242\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 11738 / 13245\n",
      "training loss = 0.3241\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 11692 / 13245\n",
      "testing accuracy = 0.8827\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 11739 / 13245\n",
      "training loss = 0.3240\n",
      "training accuracy = 0.8863\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 11740 / 13245\n",
      "training loss = 0.3239\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3238\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11695 / 13245\n",
      "testing accuracy = 0.8830\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3237\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3236\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3235\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 11742 / 13245\n",
      "training loss = 0.3234\n",
      "training accuracy = 0.8865\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3233\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3232\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11699 / 13245\n",
      "testing accuracy = 0.8833\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3231\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11700 / 13245\n",
      "testing accuracy = 0.8834\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 11746 / 13245\n",
      "training loss = 0.3230\n",
      "training accuracy = 0.8868\n",
      "num_test_corrects / test_total_examples = 11702 / 13245\n",
      "testing accuracy = 0.8835\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 11747 / 13245\n",
      "training loss = 0.3229\n",
      "training accuracy = 0.8869\n",
      "num_test_corrects / test_total_examples = 11705 / 13245\n",
      "testing accuracy = 0.8837\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3228\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 11752 / 13245\n",
      "training loss = 0.3227\n",
      "training accuracy = 0.8873\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3226\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3225\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3224\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 11749 / 13245\n",
      "training loss = 0.3223\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3222\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3221\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 11754 / 13245\n",
      "training loss = 0.3220\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 11755 / 13245\n",
      "training loss = 0.3219\n",
      "training accuracy = 0.8875\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 11754 / 13245\n",
      "training loss = 0.3219\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 11756 / 13245\n",
      "training loss = 0.3218\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 11757 / 13245\n",
      "training loss = 0.3217\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 11708 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 11758 / 13245\n",
      "training loss = 0.3216\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 11709 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3215\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11709 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3214\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11710 / 13245\n",
      "testing accuracy = 0.8841\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3213\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11711 / 13245\n",
      "testing accuracy = 0.8842\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3212\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11713 / 13245\n",
      "testing accuracy = 0.8843\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3211\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11716 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3211\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11716 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3210\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11717 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3209\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3208\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3207\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3206\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3205\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11718 / 13245\n",
      "testing accuracy = 0.8847\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3205\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3204\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3203\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11721 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3202\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3201\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3200\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11721 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3200\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11723 / 13245\n",
      "testing accuracy = 0.8851\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3199\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11723 / 13245\n",
      "testing accuracy = 0.8851\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3198\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11724 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3197\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11724 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3196\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11725 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3196\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3195\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3194\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3193\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11728 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3192\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3192\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11728 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3191\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 11765 / 13245\n",
      "training loss = 0.3190\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3189\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3189\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3188\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3187\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 11765 / 13245\n",
      "training loss = 0.3186\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3186\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3185\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3184\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11730 / 13245\n",
      "testing accuracy = 0.8856\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 11767 / 13245\n",
      "training loss = 0.3183\n",
      "training accuracy = 0.8884\n",
      "num_test_corrects / test_total_examples = 11731 / 13245\n",
      "testing accuracy = 0.8857\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 11768 / 13245\n",
      "training loss = 0.3183\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 11734 / 13245\n",
      "testing accuracy = 0.8859\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 11768 / 13245\n",
      "training loss = 0.3182\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 11735 / 13245\n",
      "testing accuracy = 0.8860\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 11769 / 13245\n",
      "training loss = 0.3181\n",
      "training accuracy = 0.8886\n",
      "num_test_corrects / test_total_examples = 11735 / 13245\n",
      "testing accuracy = 0.8860\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 11770 / 13245\n",
      "training loss = 0.3181\n",
      "training accuracy = 0.8886\n",
      "num_test_corrects / test_total_examples = 11734 / 13245\n",
      "testing accuracy = 0.8859\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 11772 / 13245\n",
      "training loss = 0.3180\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 11734 / 13245\n",
      "testing accuracy = 0.8859\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 11772 / 13245\n",
      "training loss = 0.3179\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 11736 / 13245\n",
      "testing accuracy = 0.8861\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 11773 / 13245\n",
      "training loss = 0.3178\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11737 / 13245\n",
      "testing accuracy = 0.8861\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3178\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11738 / 13245\n",
      "testing accuracy = 0.8862\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3177\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11738 / 13245\n",
      "testing accuracy = 0.8862\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3176\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3176\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3175\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 11776 / 13245\n",
      "training loss = 0.3174\n",
      "training accuracy = 0.8891\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3173\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3173\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3172\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3171\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3171\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11745 / 13245\n",
      "testing accuracy = 0.8867\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 11773 / 13245\n",
      "training loss = 0.3170\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11746 / 13245\n",
      "testing accuracy = 0.8868\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3169\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11746 / 13245\n",
      "testing accuracy = 0.8868\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 11777 / 13245\n",
      "training loss = 0.3169\n",
      "training accuracy = 0.8892\n",
      "num_test_corrects / test_total_examples = 11747 / 13245\n",
      "testing accuracy = 0.8869\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 11779 / 13245\n",
      "training loss = 0.3168\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 11748 / 13245\n",
      "testing accuracy = 0.8870\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 11781 / 13245\n",
      "training loss = 0.3167\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11747 / 13245\n",
      "testing accuracy = 0.8869\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 11781 / 13245\n",
      "training loss = 0.3167\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 11782 / 13245\n",
      "training loss = 0.3166\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 11784 / 13245\n",
      "training loss = 0.3165\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3165\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3164\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 11786 / 13245\n",
      "training loss = 0.3163\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 11786 / 13245\n",
      "training loss = 0.3163\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3162\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3162\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3161\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 11784 / 13245\n",
      "training loss = 0.3160\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3160\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3159\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3158\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3158\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3157\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3156\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3156\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11751 / 13245\n",
      "testing accuracy = 0.8872\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3155\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11751 / 13245\n",
      "testing accuracy = 0.8872\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3155\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11752 / 13245\n",
      "testing accuracy = 0.8873\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3154\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11753 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3153\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11753 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3153\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11754 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 11792 / 13245\n",
      "training loss = 0.3152\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 11754 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 11792 / 13245\n",
      "training loss = 0.3152\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 11757 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 11793 / 13245\n",
      "training loss = 0.3151\n",
      "training accuracy = 0.8904\n",
      "num_test_corrects / test_total_examples = 11757 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3150\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11758 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3150\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11758 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3149\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11759 / 13245\n",
      "testing accuracy = 0.8878\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3149\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11761 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3148\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11762 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3147\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11761 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3147\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11760 / 13245\n",
      "testing accuracy = 0.8879\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3146\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11760 / 13245\n",
      "testing accuracy = 0.8879\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3146\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11762 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3145\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11763 / 13245\n",
      "testing accuracy = 0.8881\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3145\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11763 / 13245\n",
      "testing accuracy = 0.8881\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3144\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11764 / 13245\n",
      "testing accuracy = 0.8882\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3143\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11765 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3143\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3142\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3142\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3141\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3141\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3140\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3140\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3139\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3138\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11765 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3138\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3137\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3137\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3136\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3136\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3135\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3135\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3134\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3134\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3133\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3132\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3132\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3131\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3131\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3130\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3130\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3129\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3129\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3128\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3128\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3127\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3127\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3126\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3126\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3125\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3125\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3124\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3124\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3123\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3123\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3122\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11771 / 13245\n",
      "testing accuracy = 0.8887\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3122\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3121\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3121\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3120\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3120\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3119\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3119\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3118\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3118\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3117\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11771 / 13245\n",
      "testing accuracy = 0.8887\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3117\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3116\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3116\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3114\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3114\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3113\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3113\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3112\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 11808 / 13245\n",
      "training loss = 0.3112\n",
      "training accuracy = 0.8915\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 11809 / 13245\n",
      "training loss = 0.3111\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 11809 / 13245\n",
      "training loss = 0.3111\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 11810 / 13245\n",
      "training loss = 0.3110\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 11811 / 13245\n",
      "training loss = 0.3110\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 11812 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8918\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3108\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 11814 / 13245\n",
      "training loss = 0.3108\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 11814 / 13245\n",
      "training loss = 0.3107\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 11815 / 13245\n",
      "training loss = 0.3107\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 11815 / 13245\n",
      "training loss = 0.3106\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 11816 / 13245\n",
      "training loss = 0.3106\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 11817 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 11817 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3104\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3104\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3103\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3103\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3102\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 11819 / 13245\n",
      "training loss = 0.3102\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3102\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3101\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3101\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3100\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3100\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 11823 / 13245\n",
      "training loss = 0.3098\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3098\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3097\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3097\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3095\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3095\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3093\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3093\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3091\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3091\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 11829 / 13245\n",
      "training loss = 0.3089\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 11829 / 13245\n",
      "training loss = 0.3089\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 11830 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 11833 / 13245\n",
      "training loss = 0.3087\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3087\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3085\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3085\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11774 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3082\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 11833 / 13245\n",
      "training loss = 0.3082\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 11774 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 11834 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 11834 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 11836 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3079\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3079\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 11836 / 13245\n",
      "training loss = 0.3074\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3074\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3073\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3073\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n"
     ]
    }
   ],
   "source": [
    "def train(model):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f\"--------- epoch: {e+1} ---------\")\n",
    "        # training\n",
    "        train_loss = 0.0\n",
    "        corrects = 0\n",
    "        total_examples = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # zero the gradients\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # the backward pass\n",
    "            loss = criterion(y_pred, y)  # calculate the loss\n",
    "            loss.backward()  # get the gradients\n",
    "            optimizer.step()  # update the params based on the gradients\n",
    "            # collect training results\n",
    "            train_loss += loss.item()\n",
    "            corrects += torch.sum((y_pred.round() == y))\n",
    "            total_examples += len(y)\n",
    "\n",
    "        train_losses.append(train_loss.item() / len(train_loader))\n",
    "        train_accuracies.append(corrects / total_examples)\n",
    "        print(f\"num_corrects / total_examples = {corrects.item()} / {total_examples}\")\n",
    "        print(f\"training loss = {train_losses[-1]:.4f}\")\n",
    "        print(f\"training accuracy = {train_accuracies[-1]:.4f}\")\n",
    "        # print(total_examples)\n",
    "\n",
    "        # testing\n",
    "        test_corrects = 0\n",
    "        test_total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                # prepare data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).to(torch.float32)\n",
    "                # the forward pass\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.reshape(y.shape)\n",
    "                # collect testing results\n",
    "                test_corrects += torch.sum((y_pred.round() == y))\n",
    "                test_total_examples += len(y)\n",
    "\n",
    "        test_accuracies.append(test_corrects.item() / test_total_examples)\n",
    "        print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "        print(f\"testing accuracy = {test_accuracies[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "train_losses, train_accuracies, test_accuracies = train(lr_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f02ec486f70>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f02592ec5e0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0259202880>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.8936202526092529\n",
      "max test accuracy = 0.889316737651825\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.0805e-01,  2.2928e-01, -1.0808e+00, -1.1814e+00, -1.2394e+00,\n",
      "         -1.0086e+00, -8.0494e-01, -4.2681e-01, -1.4453e-01,  2.6786e-01,\n",
      "          6.0523e-01,  1.0209e+00,  1.2789e+00,  1.3858e+00,  1.3914e+00,\n",
      "          1.4229e+00,  1.3957e+00,  1.2809e+00,  1.0406e+00,  9.8646e-01,\n",
      "          1.0232e+00,  1.0591e+00,  1.0071e+00,  1.2150e+00,  1.2869e+00,\n",
      "          1.4050e+00,  1.5281e+00,  1.4550e+00,  1.4493e+00,  1.1187e+00,\n",
      "          6.0982e-01,  2.4857e-01, -4.4515e-01, -1.1483e+00, -1.9852e+00,\n",
      "         -2.9140e+00, -3.7426e+00, -4.2107e+00, -4.6396e+00, -4.8207e+00,\n",
      "         -4.9198e+00, -5.0579e+00, -4.9247e+00, -4.7342e+00, -4.0362e+00,\n",
      "         -2.5876e+00, -7.4479e-01,  1.1280e+00,  2.7342e+00,  3.7398e+00,\n",
      "          4.4966e+00,  4.8508e+00,  5.3535e+00,  7.6177e+00,  9.5119e+00,\n",
      "          9.2978e+00,  7.7093e+00,  2.8428e+00, -1.6147e+00, -1.7053e+00,\n",
      "         -1.4594e+00, -4.9289e+00, -7.1145e+00, -2.5885e+00,  1.4174e+00,\n",
      "          4.9473e+00,  2.0093e+00, -3.5156e+00, -7.5841e+00,  3.9093e+00,\n",
      "          1.9201e+01,  8.4337e+00, -6.2661e+00, -7.9307e+00, -6.2855e+00,\n",
      "         -3.5558e+00, -6.1914e-01,  2.7728e-01,  3.5840e-01, -1.0087e+00,\n",
      "         -2.7942e+00, -4.0431e+00, -5.2118e+00, -6.2560e+00, -6.1587e+00,\n",
      "         -4.1866e+00, -1.5281e+00,  1.0691e+00,  3.3413e+00,  3.8164e+00,\n",
      "          3.8247e+00,  3.6810e+00,  3.4232e+00,  3.2772e+00,  2.9340e+00,\n",
      "          2.4199e+00,  1.8391e+00,  1.0803e+00,  2.1286e-01, -6.7443e-01,\n",
      "         -1.2695e+00, -1.4264e+00, -1.4219e+00, -1.2639e+00, -1.1704e+00,\n",
      "         -9.8844e-01, -9.3390e-01, -7.2413e-01, -4.5983e-01, -3.6452e-01,\n",
      "         -3.4517e-01, -8.3548e-02,  6.3152e-03,  2.2757e-01,  2.6028e-01,\n",
      "          1.6981e-01,  2.2868e-01,  2.9805e-01,  3.5142e-01,  3.2054e-01,\n",
      "          2.2655e-01,  1.5399e-02, -2.5377e-01, -4.7772e-01, -8.8955e-01,\n",
      "         -9.7964e-01, -9.7394e-01, -1.0518e+00]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([2.3368], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hesplitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399b82d93c603256e32b1f50bab1e44304c1a63f6ab0c291ce7489d4e70c0394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
