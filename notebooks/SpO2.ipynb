{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on the SpO2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "input_path = project_path / 'data' / 'SpO2' / 'SpO2_input.csv'\n",
    "output_path = project_path / 'data' / 'SpO2' / 'SpO2_output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypnogram(Dataset):\n",
    "\n",
    "    def __init__(self, input_path: Path, output_path: Path, train=True, scale=False):\n",
    "        x = np.loadtxt(input_path, dtype=int, delimiter=',')\n",
    "        y = np.loadtxt(output_path, dtype=int, delimiter=',')\n",
    "        if scale:\n",
    "            x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "        split_index = int(x.shape[0] * split_ratio)\n",
    "        if train:\n",
    "            self.x = x[:split_index, :]\n",
    "            self.y = y[:split_index]\n",
    "        else:\n",
    "            self.x = x[split_index:, :]\n",
    "            self.y = y[split_index:]\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float)\n",
    "        self.y = torch.unsqueeze(self.y, dim=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(f\"{self.x.shape = }\")\n",
    "        print(f\"{self.y.shape = }\")\n",
    "        print(f\"{torch.max(self.x) = }\")\n",
    "        print(f\"{torch.min(self.x) = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple 1 FC Layer Torch Model, Train Function & Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TorchLinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000 \n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, save_weight_path = None):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f\"--------- epoch: {e+1} ---------\")\n",
    "        # training\n",
    "        train_loss = 0.0\n",
    "        corrects = 0\n",
    "        total_examples = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # zero the gradients\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # the backward pass\n",
    "            loss = criterion(y_pred, y)  # calculate the loss\n",
    "            loss.backward()  # get the gradients\n",
    "            optimizer.step()  # update the params based on the gradients\n",
    "            # collect training results\n",
    "            train_loss += loss.item()\n",
    "            corrects += torch.sum((y_pred.round() == y))\n",
    "            total_examples += len(y)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(corrects / total_examples)\n",
    "        print(f\"num_corrects / total_examples = {corrects.item()} / {total_examples}\")\n",
    "        print(f\"training loss = {train_losses[-1]:.4f}\")\n",
    "        print(f\"training accuracy = {train_accuracies[-1]:.4f}\")\n",
    "        # print(total_examples)\n",
    "\n",
    "        # testing\n",
    "        test_corrects = 0\n",
    "        test_total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                # prepare data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).to(torch.float32)\n",
    "                # the forward pass\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.reshape(y.shape)\n",
    "                # collect testing results\n",
    "                test_corrects += torch.sum((y_pred.round() == y))\n",
    "                test_total_examples += len(y)\n",
    "\n",
    "        test_acc = test_corrects.item() / test_total_examples\n",
    "        test_accuracies.append(test_acc)\n",
    "        print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "        print(f\"testing accuracy = {test_accuracies[-1]:.4f}\")\n",
    "        if test_acc > best_test_acc:\n",
    "            print(f\"found best test accuracy at epoch {e+1}\")\n",
    "            best_test_acc = test_acc\n",
    "            if save_weight_path is not None:\n",
    "                # save the model\n",
    "                torch.save({\n",
    "                    'epoch': e+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'test_acc': test_acc,\n",
    "                }, save_weight_path)\n",
    "                print(f\"save the model checkpoint to {save_weight_path}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the 1FC Network on Integer Data (No Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchLinearModel(\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the simple nn on CPU is much faster\n",
    "lr_model = TorchLinearModel(300, 1)\n",
    "lr_model.to(device)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([36865, 300])\n",
      "self.y.shape = torch.Size([36865, 1])\n",
      "torch.max(self.x) = tensor(31.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Hypnogram(input_path=input_path, \n",
    "                          output_path=output_path, \n",
    "                          train=True,\n",
    "                          scale=False)\n",
    "\n",
    "train_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([9217, 300])\n",
      "self.y.shape = torch.Size([9217, 1])\n",
      "torch.max(self.x) = tensor(31.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Hypnogram(input_path=input_path, \n",
    "                         output_path=output_path, \n",
    "                         train=False)\n",
    "                         \n",
    "test_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3d88615bb0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, test_accuracies = train(lr_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf068cfb20>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf0db8f520>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    train_accuracies = [acc.cpu() for acc in train_accuracies]\n",
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf057fb790>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.6909534931182861\n",
      "max test accuracy = 0.7175870673755018\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0475,  0.1035, -0.0447, -0.0250, -0.1009,  0.1327, -0.0243,  0.0446,\n",
      "         -0.0705,  0.0117,  0.0269, -0.0262,  0.0708, -0.1628,  0.0670, -0.0204,\n",
      "          0.0022,  0.1177, -0.0419,  0.0290, -0.1351,  0.0856,  0.0096, -0.0676,\n",
      "         -0.0191,  0.0309, -0.0240,  0.0377, -0.0050,  0.0314, -0.0844,  0.0493,\n",
      "          0.0681, -0.0734, -0.0164, -0.0241,  0.0482, -0.0055, -0.0927,  0.0939,\n",
      "          0.0143, -0.0523,  0.0284, -0.0237, -0.0709,  0.0867, -0.0470,  0.0380,\n",
      "         -0.1166,  0.0613,  0.0170, -0.0462,  0.0557, -0.0159,  0.0035, -0.0027,\n",
      "         -0.0035, -0.0178, -0.0328,  0.0420,  0.0055, -0.0318, -0.0012,  0.0088,\n",
      "         -0.0286,  0.1226, -0.0468, -0.1034,  0.0386,  0.0867, -0.0821, -0.0181,\n",
      "         -0.0529,  0.0987, -0.1463,  0.0284, -0.0308,  0.1270, -0.1194,  0.0476,\n",
      "         -0.0543,  0.0328,  0.0013,  0.0659, -0.0322, -0.0188, -0.0099, -0.0390,\n",
      "         -0.0092,  0.0735, -0.0243, -0.0333, -0.0325,  0.0263,  0.0452, -0.0362,\n",
      "         -0.0584,  0.0588, -0.0064, -0.0262, -0.0339,  0.0843, -0.0810,  0.0461,\n",
      "          0.0221, -0.0302,  0.0112, -0.0171,  0.0334,  0.0499, -0.0764, -0.0188,\n",
      "         -0.0597,  0.1065, -0.0143, -0.0409, -0.0046, -0.0251,  0.0148,  0.0549,\n",
      "         -0.0793, -0.0119,  0.1042, -0.0516,  0.0584, -0.0317, -0.0590, -0.0005,\n",
      "          0.0190, -0.0189,  0.0484, -0.0451,  0.0057,  0.0454, -0.1059,  0.0645,\n",
      "         -0.0176,  0.0386, -0.0004,  0.0095, -0.0507,  0.0178,  0.0177, -0.0190,\n",
      "         -0.0491,  0.0525, -0.0531,  0.0209,  0.0842, -0.0046, -0.0751, -0.0178,\n",
      "         -0.0522,  0.0858, -0.0845,  0.0038,  0.0097,  0.0083, -0.0185, -0.0584,\n",
      "          0.0911,  0.0207,  0.0589, -0.0831, -0.0029, -0.0587,  0.0456, -0.0303,\n",
      "         -0.0374,  0.0922, -0.0205, -0.0676,  0.0911, -0.0450, -0.0575,  0.0240,\n",
      "          0.0020,  0.0269, -0.0049, -0.0593,  0.0777, -0.0815,  0.0143, -0.0079,\n",
      "          0.0119,  0.0202, -0.0172,  0.0364,  0.0148, -0.0810, -0.0007, -0.0471,\n",
      "          0.0380,  0.0522, -0.0576,  0.0265,  0.0303, -0.0009, -0.0668, -0.0122,\n",
      "          0.0316, -0.0547,  0.0458, -0.0262,  0.0556, -0.0278,  0.0205,  0.0332,\n",
      "         -0.0131, -0.0726,  0.0068,  0.0048,  0.0240, -0.0211, -0.0126,  0.0622,\n",
      "         -0.0004, -0.0745,  0.0598,  0.0399, -0.0352, -0.0178,  0.0034, -0.0112,\n",
      "         -0.0975,  0.0602, -0.0072,  0.0619, -0.0196, -0.0494,  0.0089, -0.0550,\n",
      "          0.0035,  0.0781, -0.0128,  0.0224, -0.1101,  0.0237,  0.0096,  0.0317,\n",
      "         -0.0258, -0.0367,  0.0379,  0.0221, -0.0622,  0.0149,  0.0433,  0.0016,\n",
      "         -0.0581,  0.0141, -0.0061,  0.0482, -0.0531,  0.0211, -0.0255,  0.0044,\n",
      "         -0.0492,  0.0424,  0.0249, -0.0512,  0.0853, -0.0482, -0.0526,  0.0079,\n",
      "          0.0534, -0.1026, -0.0249,  0.0367,  0.0709, -0.0685, -0.0809,  0.0565,\n",
      "         -0.0171, -0.0130,  0.0617, -0.0134, -0.0315,  0.0750, -0.0636, -0.0023,\n",
      "         -0.0517, -0.0248,  0.0051, -0.0543,  0.0500,  0.0402, -0.1269,  0.1004,\n",
      "         -0.0755,  0.0796,  0.0434, -0.0861, -0.0160, -0.0035, -0.0129,  0.0050,\n",
      "          0.0107,  0.0267, -0.0297, -0.0417]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([16.4042], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the 1FC Network on Float Data (Min-max Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([36865, 300])\n",
      "self.y.shape = torch.Size([36865, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                 output_path=output_path,\n",
    "                                 train=True, \n",
    "                                 scale=True)\n",
    "train_dataset_scaled.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([9217, 300])\n",
      "self.y.shape = torch.Size([9217, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "test_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                output_path=output_path,\n",
    "                                train=False, \n",
    "                                scale=True)\n",
    "test_dataset_scaled.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f68ddaa5640>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset_scaled.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size)\n",
    "test_loader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchLinearModel(\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "lc_model = TorchLinearModel(300, 1)\n",
    "lc_model.to(device)\n",
    "lc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 25736 / 36865\n",
      "training loss = 0.5986\n",
      "training accuracy = 0.6981\n",
      "num_test_corrects / test_total_examples = 6444 / 9217\n",
      "testing accuracy = 0.6991\n",
      "found best test accuracy at epoch 1\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 26079 / 36865\n",
      "training loss = 0.5848\n",
      "training accuracy = 0.7074\n",
      "num_test_corrects / test_total_examples = 6490 / 9217\n",
      "testing accuracy = 0.7041\n",
      "found best test accuracy at epoch 2\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 26325 / 36865\n",
      "training loss = 0.5744\n",
      "training accuracy = 0.7141\n",
      "num_test_corrects / test_total_examples = 6541 / 9217\n",
      "testing accuracy = 0.7097\n",
      "found best test accuracy at epoch 3\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 26514 / 36865\n",
      "training loss = 0.5664\n",
      "training accuracy = 0.7192\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "found best test accuracy at epoch 4\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 26683 / 36865\n",
      "training loss = 0.5601\n",
      "training accuracy = 0.7238\n",
      "num_test_corrects / test_total_examples = 6630 / 9217\n",
      "testing accuracy = 0.7193\n",
      "found best test accuracy at epoch 5\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 26786 / 36865\n",
      "training loss = 0.5550\n",
      "training accuracy = 0.7266\n",
      "num_test_corrects / test_total_examples = 6665 / 9217\n",
      "testing accuracy = 0.7231\n",
      "found best test accuracy at epoch 6\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 26889 / 36865\n",
      "training loss = 0.5508\n",
      "training accuracy = 0.7294\n",
      "num_test_corrects / test_total_examples = 6694 / 9217\n",
      "testing accuracy = 0.7263\n",
      "found best test accuracy at epoch 7\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 26985 / 36865\n",
      "training loss = 0.5474\n",
      "training accuracy = 0.7320\n",
      "num_test_corrects / test_total_examples = 6730 / 9217\n",
      "testing accuracy = 0.7302\n",
      "found best test accuracy at epoch 8\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 27041 / 36865\n",
      "training loss = 0.5446\n",
      "training accuracy = 0.7335\n",
      "num_test_corrects / test_total_examples = 6753 / 9217\n",
      "testing accuracy = 0.7327\n",
      "found best test accuracy at epoch 9\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 27116 / 36865\n",
      "training loss = 0.5422\n",
      "training accuracy = 0.7355\n",
      "num_test_corrects / test_total_examples = 6762 / 9217\n",
      "testing accuracy = 0.7336\n",
      "found best test accuracy at epoch 10\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 27165 / 36865\n",
      "training loss = 0.5402\n",
      "training accuracy = 0.7369\n",
      "num_test_corrects / test_total_examples = 6782 / 9217\n",
      "testing accuracy = 0.7358\n",
      "found best test accuracy at epoch 11\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 27209 / 36865\n",
      "training loss = 0.5384\n",
      "training accuracy = 0.7381\n",
      "num_test_corrects / test_total_examples = 6791 / 9217\n",
      "testing accuracy = 0.7368\n",
      "found best test accuracy at epoch 12\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 27255 / 36865\n",
      "training loss = 0.5370\n",
      "training accuracy = 0.7393\n",
      "num_test_corrects / test_total_examples = 6797 / 9217\n",
      "testing accuracy = 0.7374\n",
      "found best test accuracy at epoch 13\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 27273 / 36865\n",
      "training loss = 0.5357\n",
      "training accuracy = 0.7398\n",
      "num_test_corrects / test_total_examples = 6796 / 9217\n",
      "testing accuracy = 0.7373\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 27318 / 36865\n",
      "training loss = 0.5346\n",
      "training accuracy = 0.7410\n",
      "num_test_corrects / test_total_examples = 6806 / 9217\n",
      "testing accuracy = 0.7384\n",
      "found best test accuracy at epoch 15\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 27348 / 36865\n",
      "training loss = 0.5337\n",
      "training accuracy = 0.7418\n",
      "num_test_corrects / test_total_examples = 6819 / 9217\n",
      "testing accuracy = 0.7398\n",
      "found best test accuracy at epoch 16\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 27387 / 36865\n",
      "training loss = 0.5329\n",
      "training accuracy = 0.7429\n",
      "num_test_corrects / test_total_examples = 6838 / 9217\n",
      "testing accuracy = 0.7419\n",
      "found best test accuracy at epoch 17\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 27420 / 36865\n",
      "training loss = 0.5321\n",
      "training accuracy = 0.7438\n",
      "num_test_corrects / test_total_examples = 6847 / 9217\n",
      "testing accuracy = 0.7429\n",
      "found best test accuracy at epoch 18\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 27436 / 36865\n",
      "training loss = 0.5315\n",
      "training accuracy = 0.7442\n",
      "num_test_corrects / test_total_examples = 6851 / 9217\n",
      "testing accuracy = 0.7433\n",
      "found best test accuracy at epoch 19\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 27466 / 36865\n",
      "training loss = 0.5310\n",
      "training accuracy = 0.7450\n",
      "num_test_corrects / test_total_examples = 6854 / 9217\n",
      "testing accuracy = 0.7436\n",
      "found best test accuracy at epoch 20\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 27498 / 36865\n",
      "training loss = 0.5305\n",
      "training accuracy = 0.7459\n",
      "num_test_corrects / test_total_examples = 6857 / 9217\n",
      "testing accuracy = 0.7440\n",
      "found best test accuracy at epoch 21\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 27492 / 36865\n",
      "training loss = 0.5301\n",
      "training accuracy = 0.7457\n",
      "num_test_corrects / test_total_examples = 6860 / 9217\n",
      "testing accuracy = 0.7443\n",
      "found best test accuracy at epoch 22\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 27500 / 36865\n",
      "training loss = 0.5297\n",
      "training accuracy = 0.7460\n",
      "num_test_corrects / test_total_examples = 6869 / 9217\n",
      "testing accuracy = 0.7453\n",
      "found best test accuracy at epoch 23\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 27515 / 36865\n",
      "training loss = 0.5294\n",
      "training accuracy = 0.7464\n",
      "num_test_corrects / test_total_examples = 6863 / 9217\n",
      "testing accuracy = 0.7446\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 27527 / 36865\n",
      "training loss = 0.5291\n",
      "training accuracy = 0.7467\n",
      "num_test_corrects / test_total_examples = 6868 / 9217\n",
      "testing accuracy = 0.7451\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 27538 / 36865\n",
      "training loss = 0.5288\n",
      "training accuracy = 0.7470\n",
      "num_test_corrects / test_total_examples = 6872 / 9217\n",
      "testing accuracy = 0.7456\n",
      "found best test accuracy at epoch 26\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 27548 / 36865\n",
      "training loss = 0.5286\n",
      "training accuracy = 0.7473\n",
      "num_test_corrects / test_total_examples = 6872 / 9217\n",
      "testing accuracy = 0.7456\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 27557 / 36865\n",
      "training loss = 0.5284\n",
      "training accuracy = 0.7475\n",
      "num_test_corrects / test_total_examples = 6870 / 9217\n",
      "testing accuracy = 0.7454\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 27565 / 36865\n",
      "training loss = 0.5282\n",
      "training accuracy = 0.7477\n",
      "num_test_corrects / test_total_examples = 6865 / 9217\n",
      "testing accuracy = 0.7448\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 27572 / 36865\n",
      "training loss = 0.5280\n",
      "training accuracy = 0.7479\n",
      "num_test_corrects / test_total_examples = 6864 / 9217\n",
      "testing accuracy = 0.7447\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 27576 / 36865\n",
      "training loss = 0.5279\n",
      "training accuracy = 0.7480\n",
      "num_test_corrects / test_total_examples = 6868 / 9217\n",
      "testing accuracy = 0.7451\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 27582 / 36865\n",
      "training loss = 0.5277\n",
      "training accuracy = 0.7482\n",
      "num_test_corrects / test_total_examples = 6871 / 9217\n",
      "testing accuracy = 0.7455\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 27577 / 36865\n",
      "training loss = 0.5276\n",
      "training accuracy = 0.7481\n",
      "num_test_corrects / test_total_examples = 6874 / 9217\n",
      "testing accuracy = 0.7458\n",
      "found best test accuracy at epoch 33\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 27583 / 36865\n",
      "training loss = 0.5275\n",
      "training accuracy = 0.7482\n",
      "num_test_corrects / test_total_examples = 6874 / 9217\n",
      "testing accuracy = 0.7458\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 27600 / 36865\n",
      "training loss = 0.5274\n",
      "training accuracy = 0.7487\n",
      "num_test_corrects / test_total_examples = 6871 / 9217\n",
      "testing accuracy = 0.7455\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 27602 / 36865\n",
      "training loss = 0.5273\n",
      "training accuracy = 0.7487\n",
      "num_test_corrects / test_total_examples = 6875 / 9217\n",
      "testing accuracy = 0.7459\n",
      "found best test accuracy at epoch 36\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 27612 / 36865\n",
      "training loss = 0.5272\n",
      "training accuracy = 0.7490\n",
      "num_test_corrects / test_total_examples = 6877 / 9217\n",
      "testing accuracy = 0.7461\n",
      "found best test accuracy at epoch 37\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 27610 / 36865\n",
      "training loss = 0.5271\n",
      "training accuracy = 0.7489\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "found best test accuracy at epoch 38\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 27615 / 36865\n",
      "training loss = 0.5270\n",
      "training accuracy = 0.7491\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "found best test accuracy at epoch 39\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 27619 / 36865\n",
      "training loss = 0.5270\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 27618 / 36865\n",
      "training loss = 0.5269\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "found best test accuracy at epoch 41\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 27624 / 36865\n",
      "training loss = 0.5268\n",
      "training accuracy = 0.7493\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 27614 / 36865\n",
      "training loss = 0.5268\n",
      "training accuracy = 0.7491\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 27618 / 36865\n",
      "training loss = 0.5267\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 27621 / 36865\n",
      "training loss = 0.5267\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 27620 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6884 / 9217\n",
      "testing accuracy = 0.7469\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 27625 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7494\n",
      "num_test_corrects / test_total_examples = 6881 / 9217\n",
      "testing accuracy = 0.7466\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 27631 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7495\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 27636 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7497\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 27630 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7495\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 27633 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7496\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 27637 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7497\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 27641 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7498\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 27648 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7500\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 27650 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7500\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 27652 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 27654 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 27658 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 27662 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 27661 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 27658 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "found best test accuracy at epoch 66\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "found best test accuracy at epoch 67\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 27661 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 27653 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 27656 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 27656 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 27662 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 27664 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 27667 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "found best test accuracy at epoch 100\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 27673 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "found best test accuracy at epoch 102\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "found best test accuracy at epoch 104\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "found best test accuracy at epoch 135\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 27673 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "found best test accuracy at epoch 147\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "found best test accuracy at epoch 148\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 27687 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 27690 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 27688 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 27687 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 27690 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 27698 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 27700 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 27697 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 27702 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 27702 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 27704 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 27705 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 27704 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 27706 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 27707 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 27708 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 27710 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 27712 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 27712 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 27713 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 27717 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 27719 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 27721 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 27721 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 27723 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 27724 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 27726 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 27727 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 27733 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 27736 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 27737 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 27739 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 27741 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "found best test accuracy at epoch 315\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "found best test accuracy at epoch 317\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "found best test accuracy at epoch 334\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "found best test accuracy at epoch 342\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "found best test accuracy at epoch 355\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "found best test accuracy at epoch 360\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "found best test accuracy at epoch 377\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "found best test accuracy at epoch 381\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "found best test accuracy at epoch 384\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6909 / 9217\n",
      "testing accuracy = 0.7496\n",
      "found best test accuracy at epoch 422\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6909 / 9217\n",
      "testing accuracy = 0.7496\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "found best test accuracy at epoch 424\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "found best test accuracy at epoch 432\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "found best test accuracy at epoch 443\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "found best test accuracy at epoch 474\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "found best test accuracy at epoch 478\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "found best test accuracy at epoch 487\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "found best test accuracy at epoch 492\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "found best test accuracy at epoch 496\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 501 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 502 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 503 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 504 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 505 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 506 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 507 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 508 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 509 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 510 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 511 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 512 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 513 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 514 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 515 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 516 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 517 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 518 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 519 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 520 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 521 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 522 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 523 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 524 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 525 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 526 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 527 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 528 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 529 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 530 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 531 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 532 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 533 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 534 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 535 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 536 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 537 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 538 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 539 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 540 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 541 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 542 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 543 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 544 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 545 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 546 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 547 ---------\n",
      "num_corrects / total_examples = 27759 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 548 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 549 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 550 ---------\n",
      "num_corrects / total_examples = 27759 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 551 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 552 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 553 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 554 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 555 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 556 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 557 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 558 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 559 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 560 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 561 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "found best test accuracy at epoch 561\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 562 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "found best test accuracy at epoch 562\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 563 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 564 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 565 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 566 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 567 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 568 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 569 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 570 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 571 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 572 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 573 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 574 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 575 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 576 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 577 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 578 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 579 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 580 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 581 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 582 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 583 ---------\n",
      "num_corrects / total_examples = 27763 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 584 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 585 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 586 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 587 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 588 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 589 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 590 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 591 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 592 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 593 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 594 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 595 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 596 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 597 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 598 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 599 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 600 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 601 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 602 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 603 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 604 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 605 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 606 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 607 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 608 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 609 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 610 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 611 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 612 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 613 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 614 ---------\n",
      "num_corrects / total_examples = 27758 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 615 ---------\n",
      "num_corrects / total_examples = 27757 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 616 ---------\n",
      "num_corrects / total_examples = 27759 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 617 ---------\n",
      "num_corrects / total_examples = 27759 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 618 ---------\n",
      "num_corrects / total_examples = 27759 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 619 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 620 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 621 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 622 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 623 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 624 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 625 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 626 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 627 ---------\n",
      "num_corrects / total_examples = 27760 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 628 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5233\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 629 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 630 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 631 ---------\n",
      "num_corrects / total_examples = 27761 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7530\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 632 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 633 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 634 ---------\n",
      "num_corrects / total_examples = 27762 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 635 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 636 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 637 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 638 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 639 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 640 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 641 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 642 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 643 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 644 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 645 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 646 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 647 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 648 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 649 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 650 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 651 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 652 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 653 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 654 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 655 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 656 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 657 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 658 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 659 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 660 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 661 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 662 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 663 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 664 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 665 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 666 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 667 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 668 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 669 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 670 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 671 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 672 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 673 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 674 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 675 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 676 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 677 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 678 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 679 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 680 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 681 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 682 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 683 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 684 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 685 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 686 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 687 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 688 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 689 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 690 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 691 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 692 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 693 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 694 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 695 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 696 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 697 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 698 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 699 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 700 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 701 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 702 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 703 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 704 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 705 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 706 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 707 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 708 ---------\n",
      "num_corrects / total_examples = 27764 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7531\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 709 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 710 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 711 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 712 ---------\n",
      "num_corrects / total_examples = 27766 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 713 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 714 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 715 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 716 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 717 ---------\n",
      "num_corrects / total_examples = 27765 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 718 ---------\n",
      "num_corrects / total_examples = 27767 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 719 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 720 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 721 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 722 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 723 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 724 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 725 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 726 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 727 ---------\n",
      "num_corrects / total_examples = 27768 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7532\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 728 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 729 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 730 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 731 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 732 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 733 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 734 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 735 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 736 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 737 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 738 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 739 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 740 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 741 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 742 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 743 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 744 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 745 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 746 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 747 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 748 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 749 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 750 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 751 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 752 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 753 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 754 ---------\n",
      "num_corrects / total_examples = 27769 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 755 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 756 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 757 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 758 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 759 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 760 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 761 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 762 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 763 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 764 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 765 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 766 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 767 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 768 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 769 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 770 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 771 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 772 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 773 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 774 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 775 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 776 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 777 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 778 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 779 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 780 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 781 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 782 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 783 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 784 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 785 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 786 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 787 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 788 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 789 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 790 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 791 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 792 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 793 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 794 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 795 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 796 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 797 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 798 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 799 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 800 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 801 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 802 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 803 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 804 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 805 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 806 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 807 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 808 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 809 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 810 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 811 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 812 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 813 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 814 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 815 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 816 ---------\n",
      "num_corrects / total_examples = 27770 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 817 ---------\n",
      "num_corrects / total_examples = 27771 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 818 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 819 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 820 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 821 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 822 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 823 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 824 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 825 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 826 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 827 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 828 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 829 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 830 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 831 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 832 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 833 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 834 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 835 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 836 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 837 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 838 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 839 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 840 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 841 ---------\n",
      "num_corrects / total_examples = 27772 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7533\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 842 ---------\n",
      "num_corrects / total_examples = 27773 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 843 ---------\n",
      "num_corrects / total_examples = 27774 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 844 ---------\n",
      "num_corrects / total_examples = 27774 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 845 ---------\n",
      "num_corrects / total_examples = 27774 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 846 ---------\n",
      "num_corrects / total_examples = 27775 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 847 ---------\n",
      "num_corrects / total_examples = 27775 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 848 ---------\n",
      "num_corrects / total_examples = 27775 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7534\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 849 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 850 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 851 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 852 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 853 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 854 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 855 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 856 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 857 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 858 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 859 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 860 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 861 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 862 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 863 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 864 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 865 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 866 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 867 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 868 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 869 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 870 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 871 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 872 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 873 ---------\n",
      "num_corrects / total_examples = 27776 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 874 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 875 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 876 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 877 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 878 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 879 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 880 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 881 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 882 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 883 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 884 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 885 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 886 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 887 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 888 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 889 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 890 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 891 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5232\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 892 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 893 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 894 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 895 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 896 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 897 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 898 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 899 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 900 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 901 ---------\n",
      "num_corrects / total_examples = 27777 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 902 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 903 ---------\n",
      "num_corrects / total_examples = 27778 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 904 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 905 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 906 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 907 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 908 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 909 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 910 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 911 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 912 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 913 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 914 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 915 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 916 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 917 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 918 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 919 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 920 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 921 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 922 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 923 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 924 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 925 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 926 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 927 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 928 ---------\n",
      "num_corrects / total_examples = 27779 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7535\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 929 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 930 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 931 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 932 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 933 ---------\n",
      "num_corrects / total_examples = 27780 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 934 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 935 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 936 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 937 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 938 ---------\n",
      "num_corrects / total_examples = 27781 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 939 ---------\n",
      "num_corrects / total_examples = 27782 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 940 ---------\n",
      "num_corrects / total_examples = 27782 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 941 ---------\n",
      "num_corrects / total_examples = 27782 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 942 ---------\n",
      "num_corrects / total_examples = 27782 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 943 ---------\n",
      "num_corrects / total_examples = 27782 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 944 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 945 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 946 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 947 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 948 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 949 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 950 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 951 ---------\n",
      "num_corrects / total_examples = 27783 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7536\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 952 ---------\n",
      "num_corrects / total_examples = 27784 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 953 ---------\n",
      "num_corrects / total_examples = 27784 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 954 ---------\n",
      "num_corrects / total_examples = 27785 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 955 ---------\n",
      "num_corrects / total_examples = 27785 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 956 ---------\n",
      "num_corrects / total_examples = 27785 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 957 ---------\n",
      "num_corrects / total_examples = 27786 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 958 ---------\n",
      "num_corrects / total_examples = 27786 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 959 ---------\n",
      "num_corrects / total_examples = 27786 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 960 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 961 ---------\n",
      "num_corrects / total_examples = 27786 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 962 ---------\n",
      "num_corrects / total_examples = 27786 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7537\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 963 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 964 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 965 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 966 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 967 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 968 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 969 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 970 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 971 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 972 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 973 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 974 ---------\n",
      "num_corrects / total_examples = 27787 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 975 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 976 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6919 / 9217\n",
      "testing accuracy = 0.7507\n",
      "--------- epoch: 977 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 978 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 979 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 980 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 981 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 982 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 983 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 984 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 985 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 986 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 987 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 988 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 989 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 990 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 991 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 992 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 993 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 994 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 995 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 996 ---------\n",
      "num_corrects / total_examples = 27788 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 997 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 998 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 999 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n",
      "--------- epoch: 1000 ---------\n",
      "num_corrects / total_examples = 27789 / 36865\n",
      "training loss = 0.5231\n",
      "training accuracy = 0.7538\n",
      "num_test_corrects / test_total_examples = 6920 / 9217\n",
      "testing accuracy = 0.7508\n"
     ]
    }
   ],
   "source": [
    "save_weight_path = project_path / 'weights' / 'hypnogram' / 'lc_model_scaled_hypnogram_1000epoch.pt'\n",
    "train_losses_2, train_accuracies_2, test_accuracies_2 = train(lc_model, \n",
    "                                                              train_loader_scaled, \n",
    "                                                              test_loader_scaled,\n",
    "                                                              save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f68dc7086d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f68e38073a0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f68dc5fae20>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.7538044452667236\n",
      "max test accuracy = 0.7507865899967452\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies_2)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Square Neural Network (2 FC Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSquareModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TorchSquareModel, self).__init__()\n",
    "        self.linear_1 = torch.nn.Linear(input_size, 128)\n",
    "        self.linear_2 = torch.nn.Linear(128, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.square(x)\n",
    "        x = self.linear_2(x)\n",
    "        y_pred = self.sigmoid(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([36865, 300])\n",
      "self.y.shape = torch.Size([36865, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n",
      "self.x.shape = torch.Size([9217, 300])\n",
      "self.y.shape = torch.Size([9217, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                 output_path=output_path,\n",
    "                                 train=True, \n",
    "                                 scale=True)\n",
    "train_dataset_scaled.print_info()                         \n",
    "test_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                output_path=output_path,\n",
    "                                train=False, \n",
    "                                scale=True)\n",
    "test_dataset_scaled.print_info()\n",
    "\n",
    "batch_size = 4\n",
    "train_loader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size)\n",
    "test_loader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSquareModel(\n",
       "  (linear_1): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (linear_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "square_model = TorchSquareModel(300, 1)\n",
    "square_model.to(device)\n",
    "square_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 26139 / 36865\n",
      "training loss = 0.5715\n",
      "training accuracy = 0.7090\n",
      "num_test_corrects / test_total_examples = 6654 / 9217\n",
      "testing accuracy = 0.7219\n",
      "found best test accuracy at epoch 1\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 27380 / 36865\n",
      "training loss = 0.5322\n",
      "training accuracy = 0.7427\n",
      "num_test_corrects / test_total_examples = 6872 / 9217\n",
      "testing accuracy = 0.7456\n",
      "found best test accuracy at epoch 2\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 28360 / 36865\n",
      "training loss = 0.5025\n",
      "training accuracy = 0.7693\n",
      "num_test_corrects / test_total_examples = 7047 / 9217\n",
      "testing accuracy = 0.7646\n",
      "found best test accuracy at epoch 3\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 28636 / 36865\n",
      "training loss = 0.4926\n",
      "training accuracy = 0.7768\n",
      "num_test_corrects / test_total_examples = 7143 / 9217\n",
      "testing accuracy = 0.7750\n",
      "found best test accuracy at epoch 4\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 28784 / 36865\n",
      "training loss = 0.4877\n",
      "training accuracy = 0.7808\n",
      "num_test_corrects / test_total_examples = 7164 / 9217\n",
      "testing accuracy = 0.7773\n",
      "found best test accuracy at epoch 5\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 28866 / 36865\n",
      "training loss = 0.4843\n",
      "training accuracy = 0.7830\n",
      "num_test_corrects / test_total_examples = 7088 / 9217\n",
      "testing accuracy = 0.7690\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 28990 / 36865\n",
      "training loss = 0.4813\n",
      "training accuracy = 0.7864\n",
      "num_test_corrects / test_total_examples = 7092 / 9217\n",
      "testing accuracy = 0.7694\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 29026 / 36865\n",
      "training loss = 0.4788\n",
      "training accuracy = 0.7874\n",
      "num_test_corrects / test_total_examples = 7268 / 9217\n",
      "testing accuracy = 0.7885\n",
      "found best test accuracy at epoch 8\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 29087 / 36865\n",
      "training loss = 0.4790\n",
      "training accuracy = 0.7890\n",
      "num_test_corrects / test_total_examples = 7212 / 9217\n",
      "testing accuracy = 0.7825\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 29128 / 36865\n",
      "training loss = 0.4777\n",
      "training accuracy = 0.7901\n",
      "num_test_corrects / test_total_examples = 7261 / 9217\n",
      "testing accuracy = 0.7878\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 29157 / 36865\n",
      "training loss = 0.4752\n",
      "training accuracy = 0.7909\n",
      "num_test_corrects / test_total_examples = 7163 / 9217\n",
      "testing accuracy = 0.7772\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 29170 / 36865\n",
      "training loss = 0.4741\n",
      "training accuracy = 0.7913\n",
      "num_test_corrects / test_total_examples = 7292 / 9217\n",
      "testing accuracy = 0.7911\n",
      "found best test accuracy at epoch 12\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 29230 / 36865\n",
      "training loss = 0.4721\n",
      "training accuracy = 0.7929\n",
      "num_test_corrects / test_total_examples = 7286 / 9217\n",
      "testing accuracy = 0.7905\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 29245 / 36865\n",
      "training loss = 0.4709\n",
      "training accuracy = 0.7933\n",
      "num_test_corrects / test_total_examples = 7299 / 9217\n",
      "testing accuracy = 0.7919\n",
      "found best test accuracy at epoch 14\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 29271 / 36865\n",
      "training loss = 0.4699\n",
      "training accuracy = 0.7940\n",
      "num_test_corrects / test_total_examples = 7303 / 9217\n",
      "testing accuracy = 0.7923\n",
      "found best test accuracy at epoch 15\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 29317 / 36865\n",
      "training loss = 0.4682\n",
      "training accuracy = 0.7953\n",
      "num_test_corrects / test_total_examples = 7323 / 9217\n",
      "testing accuracy = 0.7945\n",
      "found best test accuracy at epoch 16\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 29344 / 36865\n",
      "training loss = 0.4672\n",
      "training accuracy = 0.7960\n",
      "num_test_corrects / test_total_examples = 7207 / 9217\n",
      "testing accuracy = 0.7819\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 29348 / 36865\n",
      "training loss = 0.4661\n",
      "training accuracy = 0.7961\n",
      "num_test_corrects / test_total_examples = 7257 / 9217\n",
      "testing accuracy = 0.7873\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 29413 / 36865\n",
      "training loss = 0.4654\n",
      "training accuracy = 0.7979\n",
      "num_test_corrects / test_total_examples = 7268 / 9217\n",
      "testing accuracy = 0.7885\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 29428 / 36865\n",
      "training loss = 0.4647\n",
      "training accuracy = 0.7983\n",
      "num_test_corrects / test_total_examples = 7135 / 9217\n",
      "testing accuracy = 0.7741\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 29466 / 36865\n",
      "training loss = 0.4632\n",
      "training accuracy = 0.7993\n",
      "num_test_corrects / test_total_examples = 7294 / 9217\n",
      "testing accuracy = 0.7914\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 29488 / 36865\n",
      "training loss = 0.4631\n",
      "training accuracy = 0.7999\n",
      "num_test_corrects / test_total_examples = 7260 / 9217\n",
      "testing accuracy = 0.7877\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 29519 / 36865\n",
      "training loss = 0.4615\n",
      "training accuracy = 0.8007\n",
      "num_test_corrects / test_total_examples = 7353 / 9217\n",
      "testing accuracy = 0.7978\n",
      "found best test accuracy at epoch 23\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 29530 / 36865\n",
      "training loss = 0.4611\n",
      "training accuracy = 0.8010\n",
      "num_test_corrects / test_total_examples = 7306 / 9217\n",
      "testing accuracy = 0.7927\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 29540 / 36865\n",
      "training loss = 0.4597\n",
      "training accuracy = 0.8013\n",
      "num_test_corrects / test_total_examples = 7284 / 9217\n",
      "testing accuracy = 0.7903\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 29571 / 36865\n",
      "training loss = 0.4585\n",
      "training accuracy = 0.8021\n",
      "num_test_corrects / test_total_examples = 7302 / 9217\n",
      "testing accuracy = 0.7922\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 29591 / 36865\n",
      "training loss = 0.4584\n",
      "training accuracy = 0.8027\n",
      "num_test_corrects / test_total_examples = 7335 / 9217\n",
      "testing accuracy = 0.7958\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 29597 / 36865\n",
      "training loss = 0.4571\n",
      "training accuracy = 0.8028\n",
      "num_test_corrects / test_total_examples = 7304 / 9217\n",
      "testing accuracy = 0.7924\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 29655 / 36865\n",
      "training loss = 0.4566\n",
      "training accuracy = 0.8044\n",
      "num_test_corrects / test_total_examples = 7368 / 9217\n",
      "testing accuracy = 0.7994\n",
      "found best test accuracy at epoch 29\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 29644 / 36865\n",
      "training loss = 0.4559\n",
      "training accuracy = 0.8041\n",
      "num_test_corrects / test_total_examples = 7309 / 9217\n",
      "testing accuracy = 0.7930\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 29715 / 36865\n",
      "training loss = 0.4549\n",
      "training accuracy = 0.8060\n",
      "num_test_corrects / test_total_examples = 7289 / 9217\n",
      "testing accuracy = 0.7908\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 29727 / 36865\n",
      "training loss = 0.4538\n",
      "training accuracy = 0.8064\n",
      "num_test_corrects / test_total_examples = 7257 / 9217\n",
      "testing accuracy = 0.7873\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 29750 / 36865\n",
      "training loss = 0.4531\n",
      "training accuracy = 0.8070\n",
      "num_test_corrects / test_total_examples = 7319 / 9217\n",
      "testing accuracy = 0.7941\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 29764 / 36865\n",
      "training loss = 0.4521\n",
      "training accuracy = 0.8074\n",
      "num_test_corrects / test_total_examples = 7287 / 9217\n",
      "testing accuracy = 0.7906\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 29796 / 36865\n",
      "training loss = 0.4508\n",
      "training accuracy = 0.8082\n",
      "num_test_corrects / test_total_examples = 7264 / 9217\n",
      "testing accuracy = 0.7881\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 29807 / 36865\n",
      "training loss = 0.4537\n",
      "training accuracy = 0.8085\n",
      "num_test_corrects / test_total_examples = 7319 / 9217\n",
      "testing accuracy = 0.7941\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 29847 / 36865\n",
      "training loss = 0.4521\n",
      "training accuracy = 0.8096\n",
      "num_test_corrects / test_total_examples = 7345 / 9217\n",
      "testing accuracy = 0.7969\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 29816 / 36865\n",
      "training loss = 0.4539\n",
      "training accuracy = 0.8088\n",
      "num_test_corrects / test_total_examples = 7353 / 9217\n",
      "testing accuracy = 0.7978\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 29869 / 36865\n",
      "training loss = 0.4530\n",
      "training accuracy = 0.8102\n",
      "num_test_corrects / test_total_examples = 7288 / 9217\n",
      "testing accuracy = 0.7907\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 29871 / 36865\n",
      "training loss = 0.4521\n",
      "training accuracy = 0.8103\n",
      "num_test_corrects / test_total_examples = 7427 / 9217\n",
      "testing accuracy = 0.8058\n",
      "found best test accuracy at epoch 40\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 29925 / 36865\n",
      "training loss = 0.4513\n",
      "training accuracy = 0.8117\n",
      "num_test_corrects / test_total_examples = 7356 / 9217\n",
      "testing accuracy = 0.7981\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 29926 / 36865\n",
      "training loss = 0.4505\n",
      "training accuracy = 0.8118\n",
      "num_test_corrects / test_total_examples = 7352 / 9217\n",
      "testing accuracy = 0.7977\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 29945 / 36865\n",
      "training loss = 0.4504\n",
      "training accuracy = 0.8123\n",
      "num_test_corrects / test_total_examples = 7356 / 9217\n",
      "testing accuracy = 0.7981\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 29982 / 36865\n",
      "training loss = 0.4493\n",
      "training accuracy = 0.8133\n",
      "num_test_corrects / test_total_examples = 7342 / 9217\n",
      "testing accuracy = 0.7966\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 29972 / 36865\n",
      "training loss = 0.4481\n",
      "training accuracy = 0.8130\n",
      "num_test_corrects / test_total_examples = 7379 / 9217\n",
      "testing accuracy = 0.8006\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 30022 / 36865\n",
      "training loss = 0.4480\n",
      "training accuracy = 0.8144\n",
      "num_test_corrects / test_total_examples = 7317 / 9217\n",
      "testing accuracy = 0.7939\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 30040 / 36865\n",
      "training loss = 0.4497\n",
      "training accuracy = 0.8149\n",
      "num_test_corrects / test_total_examples = 7358 / 9217\n",
      "testing accuracy = 0.7983\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 30088 / 36865\n",
      "training loss = 0.4481\n",
      "training accuracy = 0.8162\n",
      "num_test_corrects / test_total_examples = 7386 / 9217\n",
      "testing accuracy = 0.8013\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 30095 / 36865\n",
      "training loss = 0.4481\n",
      "training accuracy = 0.8164\n",
      "num_test_corrects / test_total_examples = 7356 / 9217\n",
      "testing accuracy = 0.7981\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 30093 / 36865\n",
      "training loss = 0.4470\n",
      "training accuracy = 0.8163\n",
      "num_test_corrects / test_total_examples = 7429 / 9217\n",
      "testing accuracy = 0.8060\n",
      "found best test accuracy at epoch 50\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 30115 / 36865\n",
      "training loss = 0.4467\n",
      "training accuracy = 0.8169\n",
      "num_test_corrects / test_total_examples = 7394 / 9217\n",
      "testing accuracy = 0.8022\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 30181 / 36865\n",
      "training loss = 0.4451\n",
      "training accuracy = 0.8187\n",
      "num_test_corrects / test_total_examples = 7376 / 9217\n",
      "testing accuracy = 0.8003\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 30195 / 36865\n",
      "training loss = 0.4442\n",
      "training accuracy = 0.8191\n",
      "num_test_corrects / test_total_examples = 7397 / 9217\n",
      "testing accuracy = 0.8025\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 30197 / 36865\n",
      "training loss = 0.4436\n",
      "training accuracy = 0.8191\n",
      "num_test_corrects / test_total_examples = 7385 / 9217\n",
      "testing accuracy = 0.8012\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 30222 / 36865\n",
      "training loss = 0.4435\n",
      "training accuracy = 0.8198\n",
      "num_test_corrects / test_total_examples = 7393 / 9217\n",
      "testing accuracy = 0.8021\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 30233 / 36865\n",
      "training loss = 0.4443\n",
      "training accuracy = 0.8201\n",
      "num_test_corrects / test_total_examples = 7311 / 9217\n",
      "testing accuracy = 0.7932\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 30253 / 36865\n",
      "training loss = 0.4457\n",
      "training accuracy = 0.8206\n",
      "num_test_corrects / test_total_examples = 7441 / 9217\n",
      "testing accuracy = 0.8073\n",
      "found best test accuracy at epoch 57\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 30261 / 36865\n",
      "training loss = 0.4465\n",
      "training accuracy = 0.8209\n",
      "num_test_corrects / test_total_examples = 7389 / 9217\n",
      "testing accuracy = 0.8017\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 30275 / 36865\n",
      "training loss = 0.4429\n",
      "training accuracy = 0.8212\n",
      "num_test_corrects / test_total_examples = 7361 / 9217\n",
      "testing accuracy = 0.7986\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 30269 / 36865\n",
      "training loss = 0.4422\n",
      "training accuracy = 0.8211\n",
      "num_test_corrects / test_total_examples = 7393 / 9217\n",
      "testing accuracy = 0.8021\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 30336 / 36865\n",
      "training loss = 0.4428\n",
      "training accuracy = 0.8229\n",
      "num_test_corrects / test_total_examples = 7404 / 9217\n",
      "testing accuracy = 0.8033\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 30336 / 36865\n",
      "training loss = 0.4434\n",
      "training accuracy = 0.8229\n",
      "num_test_corrects / test_total_examples = 7407 / 9217\n",
      "testing accuracy = 0.8036\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 30353 / 36865\n",
      "training loss = 0.4403\n",
      "training accuracy = 0.8234\n",
      "num_test_corrects / test_total_examples = 7438 / 9217\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 30364 / 36865\n",
      "training loss = 0.4420\n",
      "training accuracy = 0.8237\n",
      "num_test_corrects / test_total_examples = 7510 / 9217\n",
      "testing accuracy = 0.8148\n",
      "found best test accuracy at epoch 64\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 30404 / 36865\n",
      "training loss = 0.4410\n",
      "training accuracy = 0.8247\n",
      "num_test_corrects / test_total_examples = 7365 / 9217\n",
      "testing accuracy = 0.7991\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 30419 / 36865\n",
      "training loss = 0.4453\n",
      "training accuracy = 0.8251\n",
      "num_test_corrects / test_total_examples = 7578 / 9217\n",
      "testing accuracy = 0.8222\n",
      "found best test accuracy at epoch 66\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 30393 / 36865\n",
      "training loss = 0.4375\n",
      "training accuracy = 0.8244\n",
      "num_test_corrects / test_total_examples = 7452 / 9217\n",
      "testing accuracy = 0.8085\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 30409 / 36865\n",
      "training loss = 0.4371\n",
      "training accuracy = 0.8249\n",
      "num_test_corrects / test_total_examples = 7383 / 9217\n",
      "testing accuracy = 0.8010\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 30444 / 36865\n",
      "training loss = 0.4379\n",
      "training accuracy = 0.8258\n",
      "num_test_corrects / test_total_examples = 7485 / 9217\n",
      "testing accuracy = 0.8121\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 30441 / 36865\n",
      "training loss = 0.4467\n",
      "training accuracy = 0.8257\n",
      "num_test_corrects / test_total_examples = 7506 / 9217\n",
      "testing accuracy = 0.8144\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 30460 / 36865\n",
      "training loss = 0.4355\n",
      "training accuracy = 0.8263\n",
      "num_test_corrects / test_total_examples = 7521 / 9217\n",
      "testing accuracy = 0.8160\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 30479 / 36865\n",
      "training loss = 0.4385\n",
      "training accuracy = 0.8268\n",
      "num_test_corrects / test_total_examples = 7551 / 9217\n",
      "testing accuracy = 0.8192\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 30480 / 36865\n",
      "training loss = 0.4406\n",
      "training accuracy = 0.8268\n",
      "num_test_corrects / test_total_examples = 7521 / 9217\n",
      "testing accuracy = 0.8160\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 30514 / 36865\n",
      "training loss = 0.4410\n",
      "training accuracy = 0.8277\n",
      "num_test_corrects / test_total_examples = 7602 / 9217\n",
      "testing accuracy = 0.8248\n",
      "found best test accuracy at epoch 74\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 30518 / 36865\n",
      "training loss = 0.4454\n",
      "training accuracy = 0.8278\n",
      "num_test_corrects / test_total_examples = 7557 / 9217\n",
      "testing accuracy = 0.8199\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 30559 / 36865\n",
      "training loss = 0.4436\n",
      "training accuracy = 0.8289\n",
      "num_test_corrects / test_total_examples = 7588 / 9217\n",
      "testing accuracy = 0.8233\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 30578 / 36865\n",
      "training loss = 0.4403\n",
      "training accuracy = 0.8295\n",
      "num_test_corrects / test_total_examples = 7418 / 9217\n",
      "testing accuracy = 0.8048\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 30595 / 36865\n",
      "training loss = 0.4430\n",
      "training accuracy = 0.8299\n",
      "num_test_corrects / test_total_examples = 7635 / 9217\n",
      "testing accuracy = 0.8284\n",
      "found best test accuracy at epoch 78\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 30598 / 36865\n",
      "training loss = 0.4368\n",
      "training accuracy = 0.8300\n",
      "num_test_corrects / test_total_examples = 7600 / 9217\n",
      "testing accuracy = 0.8246\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 30677 / 36865\n",
      "training loss = 0.4409\n",
      "training accuracy = 0.8321\n",
      "num_test_corrects / test_total_examples = 7532 / 9217\n",
      "testing accuracy = 0.8172\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 30636 / 36865\n",
      "training loss = 0.4366\n",
      "training accuracy = 0.8310\n",
      "num_test_corrects / test_total_examples = 7472 / 9217\n",
      "testing accuracy = 0.8107\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 30643 / 36865\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8312\n",
      "num_test_corrects / test_total_examples = 7490 / 9217\n",
      "testing accuracy = 0.8126\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 30665 / 36865\n",
      "training loss = 0.4360\n",
      "training accuracy = 0.8318\n",
      "num_test_corrects / test_total_examples = 7493 / 9217\n",
      "testing accuracy = 0.8130\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 30689 / 36865\n",
      "training loss = 0.4371\n",
      "training accuracy = 0.8325\n",
      "num_test_corrects / test_total_examples = 7532 / 9217\n",
      "testing accuracy = 0.8172\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 30701 / 36865\n",
      "training loss = 0.4355\n",
      "training accuracy = 0.8328\n",
      "num_test_corrects / test_total_examples = 7430 / 9217\n",
      "testing accuracy = 0.8061\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 30783 / 36865\n",
      "training loss = 0.4417\n",
      "training accuracy = 0.8350\n",
      "num_test_corrects / test_total_examples = 7639 / 9217\n",
      "testing accuracy = 0.8288\n",
      "found best test accuracy at epoch 86\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 30771 / 36865\n",
      "training loss = 0.4377\n",
      "training accuracy = 0.8347\n",
      "num_test_corrects / test_total_examples = 7643 / 9217\n",
      "testing accuracy = 0.8292\n",
      "found best test accuracy at epoch 87\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 30763 / 36865\n",
      "training loss = 0.4357\n",
      "training accuracy = 0.8345\n",
      "num_test_corrects / test_total_examples = 7340 / 9217\n",
      "testing accuracy = 0.7964\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 30762 / 36865\n",
      "training loss = 0.4395\n",
      "training accuracy = 0.8345\n",
      "num_test_corrects / test_total_examples = 7643 / 9217\n",
      "testing accuracy = 0.8292\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 30804 / 36865\n",
      "training loss = 0.4389\n",
      "training accuracy = 0.8356\n",
      "num_test_corrects / test_total_examples = 7438 / 9217\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 30859 / 36865\n",
      "training loss = 0.4450\n",
      "training accuracy = 0.8371\n",
      "num_test_corrects / test_total_examples = 7662 / 9217\n",
      "testing accuracy = 0.8313\n",
      "found best test accuracy at epoch 91\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 30850 / 36865\n",
      "training loss = 0.4348\n",
      "training accuracy = 0.8368\n",
      "num_test_corrects / test_total_examples = 7363 / 9217\n",
      "testing accuracy = 0.7988\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 30816 / 36865\n",
      "training loss = 0.4354\n",
      "training accuracy = 0.8359\n",
      "num_test_corrects / test_total_examples = 7525 / 9217\n",
      "testing accuracy = 0.8164\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 30875 / 36865\n",
      "training loss = 0.4389\n",
      "training accuracy = 0.8375\n",
      "num_test_corrects / test_total_examples = 7627 / 9217\n",
      "testing accuracy = 0.8275\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 30887 / 36865\n",
      "training loss = 0.4411\n",
      "training accuracy = 0.8378\n",
      "num_test_corrects / test_total_examples = 7533 / 9217\n",
      "testing accuracy = 0.8173\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 30870 / 36865\n",
      "training loss = 0.4453\n",
      "training accuracy = 0.8374\n",
      "num_test_corrects / test_total_examples = 7633 / 9217\n",
      "testing accuracy = 0.8281\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 30937 / 36865\n",
      "training loss = 0.4358\n",
      "training accuracy = 0.8392\n",
      "num_test_corrects / test_total_examples = 7335 / 9217\n",
      "testing accuracy = 0.7958\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 30912 / 36865\n",
      "training loss = 0.4429\n",
      "training accuracy = 0.8385\n",
      "num_test_corrects / test_total_examples = 7597 / 9217\n",
      "testing accuracy = 0.8242\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 31003 / 36865\n",
      "training loss = 0.4328\n",
      "training accuracy = 0.8410\n",
      "num_test_corrects / test_total_examples = 7407 / 9217\n",
      "testing accuracy = 0.8036\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 30954 / 36865\n",
      "training loss = 0.4474\n",
      "training accuracy = 0.8397\n",
      "num_test_corrects / test_total_examples = 7638 / 9217\n",
      "testing accuracy = 0.8287\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 30944 / 36865\n",
      "training loss = 0.4357\n",
      "training accuracy = 0.8394\n",
      "num_test_corrects / test_total_examples = 7524 / 9217\n",
      "testing accuracy = 0.8163\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 30949 / 36865\n",
      "training loss = 0.4423\n",
      "training accuracy = 0.8395\n",
      "num_test_corrects / test_total_examples = 7662 / 9217\n",
      "testing accuracy = 0.8313\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 31030 / 36865\n",
      "training loss = 0.4351\n",
      "training accuracy = 0.8417\n",
      "num_test_corrects / test_total_examples = 7679 / 9217\n",
      "testing accuracy = 0.8331\n",
      "found best test accuracy at epoch 103\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 31012 / 36865\n",
      "training loss = 0.4387\n",
      "training accuracy = 0.8412\n",
      "num_test_corrects / test_total_examples = 7665 / 9217\n",
      "testing accuracy = 0.8316\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 30988 / 36865\n",
      "training loss = 0.4369\n",
      "training accuracy = 0.8406\n",
      "num_test_corrects / test_total_examples = 7438 / 9217\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 30999 / 36865\n",
      "training loss = 0.4469\n",
      "training accuracy = 0.8409\n",
      "num_test_corrects / test_total_examples = 7413 / 9217\n",
      "testing accuracy = 0.8043\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 31047 / 36865\n",
      "training loss = 0.4476\n",
      "training accuracy = 0.8422\n",
      "num_test_corrects / test_total_examples = 7709 / 9217\n",
      "testing accuracy = 0.8364\n",
      "found best test accuracy at epoch 107\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 31078 / 36865\n",
      "training loss = 0.4284\n",
      "training accuracy = 0.8430\n",
      "num_test_corrects / test_total_examples = 7398 / 9217\n",
      "testing accuracy = 0.8026\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 31107 / 36865\n",
      "training loss = 0.4406\n",
      "training accuracy = 0.8438\n",
      "num_test_corrects / test_total_examples = 7663 / 9217\n",
      "testing accuracy = 0.8314\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 31109 / 36865\n",
      "training loss = 0.4427\n",
      "training accuracy = 0.8439\n",
      "num_test_corrects / test_total_examples = 7527 / 9217\n",
      "testing accuracy = 0.8166\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 31099 / 36865\n",
      "training loss = 0.4425\n",
      "training accuracy = 0.8436\n",
      "num_test_corrects / test_total_examples = 7605 / 9217\n",
      "testing accuracy = 0.8251\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 31083 / 36865\n",
      "training loss = 0.4392\n",
      "training accuracy = 0.8432\n",
      "num_test_corrects / test_total_examples = 7579 / 9217\n",
      "testing accuracy = 0.8223\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 31070 / 36865\n",
      "training loss = 0.4484\n",
      "training accuracy = 0.8428\n",
      "num_test_corrects / test_total_examples = 7801 / 9217\n",
      "testing accuracy = 0.8464\n",
      "found best test accuracy at epoch 113\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 31166 / 36865\n",
      "training loss = 0.4397\n",
      "training accuracy = 0.8454\n",
      "num_test_corrects / test_total_examples = 7629 / 9217\n",
      "testing accuracy = 0.8277\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 31097 / 36865\n",
      "training loss = 0.4394\n",
      "training accuracy = 0.8435\n",
      "num_test_corrects / test_total_examples = 7579 / 9217\n",
      "testing accuracy = 0.8223\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 31144 / 36865\n",
      "training loss = 0.4423\n",
      "training accuracy = 0.8448\n",
      "num_test_corrects / test_total_examples = 7571 / 9217\n",
      "testing accuracy = 0.8214\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 31155 / 36865\n",
      "training loss = 0.4470\n",
      "training accuracy = 0.8451\n",
      "num_test_corrects / test_total_examples = 7679 / 9217\n",
      "testing accuracy = 0.8331\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 31171 / 36865\n",
      "training loss = 0.4441\n",
      "training accuracy = 0.8455\n",
      "num_test_corrects / test_total_examples = 7523 / 9217\n",
      "testing accuracy = 0.8162\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 31177 / 36865\n",
      "training loss = 0.4448\n",
      "training accuracy = 0.8457\n",
      "num_test_corrects / test_total_examples = 7617 / 9217\n",
      "testing accuracy = 0.8264\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 31195 / 36865\n",
      "training loss = 0.4553\n",
      "training accuracy = 0.8462\n",
      "num_test_corrects / test_total_examples = 7738 / 9217\n",
      "testing accuracy = 0.8395\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 31264 / 36865\n",
      "training loss = 0.4428\n",
      "training accuracy = 0.8481\n",
      "num_test_corrects / test_total_examples = 7713 / 9217\n",
      "testing accuracy = 0.8368\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 31263 / 36865\n",
      "training loss = 0.4448\n",
      "training accuracy = 0.8480\n",
      "num_test_corrects / test_total_examples = 7724 / 9217\n",
      "testing accuracy = 0.8380\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 31239 / 36865\n",
      "training loss = 0.4456\n",
      "training accuracy = 0.8474\n",
      "num_test_corrects / test_total_examples = 7489 / 9217\n",
      "testing accuracy = 0.8125\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 31184 / 36865\n",
      "training loss = 0.4471\n",
      "training accuracy = 0.8459\n",
      "num_test_corrects / test_total_examples = 7737 / 9217\n",
      "testing accuracy = 0.8394\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 31263 / 36865\n",
      "training loss = 0.4430\n",
      "training accuracy = 0.8480\n",
      "num_test_corrects / test_total_examples = 7664 / 9217\n",
      "testing accuracy = 0.8315\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 31290 / 36865\n",
      "training loss = 0.4404\n",
      "training accuracy = 0.8488\n",
      "num_test_corrects / test_total_examples = 7716 / 9217\n",
      "testing accuracy = 0.8371\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 31330 / 36865\n",
      "training loss = 0.4403\n",
      "training accuracy = 0.8499\n",
      "num_test_corrects / test_total_examples = 7735 / 9217\n",
      "testing accuracy = 0.8392\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 31230 / 36865\n",
      "training loss = 0.4466\n",
      "training accuracy = 0.8471\n",
      "num_test_corrects / test_total_examples = 7761 / 9217\n",
      "testing accuracy = 0.8420\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 31354 / 36865\n",
      "training loss = 0.4390\n",
      "training accuracy = 0.8505\n",
      "num_test_corrects / test_total_examples = 7669 / 9217\n",
      "testing accuracy = 0.8320\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 31302 / 36865\n",
      "training loss = 0.4418\n",
      "training accuracy = 0.8491\n",
      "num_test_corrects / test_total_examples = 7537 / 9217\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 31347 / 36865\n",
      "training loss = 0.4400\n",
      "training accuracy = 0.8503\n",
      "num_test_corrects / test_total_examples = 7760 / 9217\n",
      "testing accuracy = 0.8419\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 31318 / 36865\n",
      "training loss = 0.4409\n",
      "training accuracy = 0.8495\n",
      "num_test_corrects / test_total_examples = 7649 / 9217\n",
      "testing accuracy = 0.8299\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 31358 / 36865\n",
      "training loss = 0.4417\n",
      "training accuracy = 0.8506\n",
      "num_test_corrects / test_total_examples = 7763 / 9217\n",
      "testing accuracy = 0.8422\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 31340 / 36865\n",
      "training loss = 0.4476\n",
      "training accuracy = 0.8501\n",
      "num_test_corrects / test_total_examples = 7593 / 9217\n",
      "testing accuracy = 0.8238\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 31348 / 36865\n",
      "training loss = 0.4401\n",
      "training accuracy = 0.8503\n",
      "num_test_corrects / test_total_examples = 7756 / 9217\n",
      "testing accuracy = 0.8415\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 31385 / 36865\n",
      "training loss = 0.4366\n",
      "training accuracy = 0.8513\n",
      "num_test_corrects / test_total_examples = 7722 / 9217\n",
      "testing accuracy = 0.8378\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 31407 / 36865\n",
      "training loss = 0.4410\n",
      "training accuracy = 0.8519\n",
      "num_test_corrects / test_total_examples = 7750 / 9217\n",
      "testing accuracy = 0.8408\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 31384 / 36865\n",
      "training loss = 0.4402\n",
      "training accuracy = 0.8513\n",
      "num_test_corrects / test_total_examples = 7768 / 9217\n",
      "testing accuracy = 0.8428\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 31395 / 36865\n",
      "training loss = 0.4413\n",
      "training accuracy = 0.8516\n",
      "num_test_corrects / test_total_examples = 7766 / 9217\n",
      "testing accuracy = 0.8426\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 31416 / 36865\n",
      "training loss = 0.4370\n",
      "training accuracy = 0.8522\n",
      "num_test_corrects / test_total_examples = 7709 / 9217\n",
      "testing accuracy = 0.8364\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 31413 / 36865\n",
      "training loss = 0.4415\n",
      "training accuracy = 0.8521\n",
      "num_test_corrects / test_total_examples = 7845 / 9217\n",
      "testing accuracy = 0.8511\n",
      "found best test accuracy at epoch 141\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 31441 / 36865\n",
      "training loss = 0.4369\n",
      "training accuracy = 0.8529\n",
      "num_test_corrects / test_total_examples = 7770 / 9217\n",
      "testing accuracy = 0.8430\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 31480 / 36865\n",
      "training loss = 0.4395\n",
      "training accuracy = 0.8539\n",
      "num_test_corrects / test_total_examples = 7766 / 9217\n",
      "testing accuracy = 0.8426\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 31437 / 36865\n",
      "training loss = 0.4393\n",
      "training accuracy = 0.8528\n",
      "num_test_corrects / test_total_examples = 7790 / 9217\n",
      "testing accuracy = 0.8452\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 31489 / 36865\n",
      "training loss = 0.4355\n",
      "training accuracy = 0.8542\n",
      "num_test_corrects / test_total_examples = 7813 / 9217\n",
      "testing accuracy = 0.8477\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 31511 / 36865\n",
      "training loss = 0.4315\n",
      "training accuracy = 0.8548\n",
      "num_test_corrects / test_total_examples = 7677 / 9217\n",
      "testing accuracy = 0.8329\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 31455 / 36865\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8532\n",
      "num_test_corrects / test_total_examples = 7810 / 9217\n",
      "testing accuracy = 0.8473\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 31432 / 36865\n",
      "training loss = 0.4388\n",
      "training accuracy = 0.8526\n",
      "num_test_corrects / test_total_examples = 7808 / 9217\n",
      "testing accuracy = 0.8471\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 31473 / 36865\n",
      "training loss = 0.4357\n",
      "training accuracy = 0.8537\n",
      "num_test_corrects / test_total_examples = 7831 / 9217\n",
      "testing accuracy = 0.8496\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 31452 / 36865\n",
      "training loss = 0.4399\n",
      "training accuracy = 0.8532\n",
      "num_test_corrects / test_total_examples = 7793 / 9217\n",
      "testing accuracy = 0.8455\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 31480 / 36865\n",
      "training loss = 0.4413\n",
      "training accuracy = 0.8539\n",
      "num_test_corrects / test_total_examples = 7805 / 9217\n",
      "testing accuracy = 0.8468\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 31543 / 36865\n",
      "training loss = 0.4434\n",
      "training accuracy = 0.8556\n",
      "num_test_corrects / test_total_examples = 7804 / 9217\n",
      "testing accuracy = 0.8467\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 31534 / 36865\n",
      "training loss = 0.4347\n",
      "training accuracy = 0.8554\n",
      "num_test_corrects / test_total_examples = 7821 / 9217\n",
      "testing accuracy = 0.8485\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 31535 / 36865\n",
      "training loss = 0.4360\n",
      "training accuracy = 0.8554\n",
      "num_test_corrects / test_total_examples = 7819 / 9217\n",
      "testing accuracy = 0.8483\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 31550 / 36865\n",
      "training loss = 0.4302\n",
      "training accuracy = 0.8558\n",
      "num_test_corrects / test_total_examples = 7669 / 9217\n",
      "testing accuracy = 0.8320\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 31520 / 36865\n",
      "training loss = 0.4365\n",
      "training accuracy = 0.8550\n",
      "num_test_corrects / test_total_examples = 7707 / 9217\n",
      "testing accuracy = 0.8362\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 31587 / 36865\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8568\n",
      "num_test_corrects / test_total_examples = 7780 / 9217\n",
      "testing accuracy = 0.8441\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 31547 / 36865\n",
      "training loss = 0.4339\n",
      "training accuracy = 0.8557\n",
      "num_test_corrects / test_total_examples = 7844 / 9217\n",
      "testing accuracy = 0.8510\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 31547 / 36865\n",
      "training loss = 0.4319\n",
      "training accuracy = 0.8557\n",
      "num_test_corrects / test_total_examples = 7817 / 9217\n",
      "testing accuracy = 0.8481\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 31594 / 36865\n",
      "training loss = 0.4352\n",
      "training accuracy = 0.8570\n",
      "num_test_corrects / test_total_examples = 7828 / 9217\n",
      "testing accuracy = 0.8493\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 31575 / 36865\n",
      "training loss = 0.4396\n",
      "training accuracy = 0.8565\n",
      "num_test_corrects / test_total_examples = 7824 / 9217\n",
      "testing accuracy = 0.8489\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 31549 / 36865\n",
      "training loss = 0.4350\n",
      "training accuracy = 0.8558\n",
      "num_test_corrects / test_total_examples = 7821 / 9217\n",
      "testing accuracy = 0.8485\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 31639 / 36865\n",
      "training loss = 0.4372\n",
      "training accuracy = 0.8582\n",
      "num_test_corrects / test_total_examples = 7831 / 9217\n",
      "testing accuracy = 0.8496\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 31597 / 36865\n",
      "training loss = 0.4380\n",
      "training accuracy = 0.8571\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "found best test accuracy at epoch 164\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 31635 / 36865\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8581\n",
      "num_test_corrects / test_total_examples = 7857 / 9217\n",
      "testing accuracy = 0.8524\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 31595 / 36865\n",
      "training loss = 0.4362\n",
      "training accuracy = 0.8570\n",
      "num_test_corrects / test_total_examples = 7837 / 9217\n",
      "testing accuracy = 0.8503\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 31624 / 36865\n",
      "training loss = 0.4328\n",
      "training accuracy = 0.8578\n",
      "num_test_corrects / test_total_examples = 7610 / 9217\n",
      "testing accuracy = 0.8256\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 31637 / 36865\n",
      "training loss = 0.4364\n",
      "training accuracy = 0.8582\n",
      "num_test_corrects / test_total_examples = 7845 / 9217\n",
      "testing accuracy = 0.8511\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 31685 / 36865\n",
      "training loss = 0.4293\n",
      "training accuracy = 0.8595\n",
      "num_test_corrects / test_total_examples = 7828 / 9217\n",
      "testing accuracy = 0.8493\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 31683 / 36865\n",
      "training loss = 0.4338\n",
      "training accuracy = 0.8594\n",
      "num_test_corrects / test_total_examples = 7827 / 9217\n",
      "testing accuracy = 0.8492\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 31675 / 36865\n",
      "training loss = 0.4433\n",
      "training accuracy = 0.8592\n",
      "num_test_corrects / test_total_examples = 7846 / 9217\n",
      "testing accuracy = 0.8513\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 31655 / 36865\n",
      "training loss = 0.4366\n",
      "training accuracy = 0.8587\n",
      "num_test_corrects / test_total_examples = 7832 / 9217\n",
      "testing accuracy = 0.8497\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 31632 / 36865\n",
      "training loss = 0.4338\n",
      "training accuracy = 0.8580\n",
      "num_test_corrects / test_total_examples = 7825 / 9217\n",
      "testing accuracy = 0.8490\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 31663 / 36865\n",
      "training loss = 0.4336\n",
      "training accuracy = 0.8589\n",
      "num_test_corrects / test_total_examples = 7849 / 9217\n",
      "testing accuracy = 0.8516\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 31717 / 36865\n",
      "training loss = 0.4305\n",
      "training accuracy = 0.8604\n",
      "num_test_corrects / test_total_examples = 7818 / 9217\n",
      "testing accuracy = 0.8482\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 31685 / 36865\n",
      "training loss = 0.4340\n",
      "training accuracy = 0.8595\n",
      "num_test_corrects / test_total_examples = 7847 / 9217\n",
      "testing accuracy = 0.8514\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 31681 / 36865\n",
      "training loss = 0.4348\n",
      "training accuracy = 0.8594\n",
      "num_test_corrects / test_total_examples = 7871 / 9217\n",
      "testing accuracy = 0.8540\n",
      "found best test accuracy at epoch 177\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 31705 / 36865\n",
      "training loss = 0.4362\n",
      "training accuracy = 0.8600\n",
      "num_test_corrects / test_total_examples = 7842 / 9217\n",
      "testing accuracy = 0.8508\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 31726 / 36865\n",
      "training loss = 0.4300\n",
      "training accuracy = 0.8606\n",
      "num_test_corrects / test_total_examples = 7834 / 9217\n",
      "testing accuracy = 0.8500\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 31715 / 36865\n",
      "training loss = 0.4352\n",
      "training accuracy = 0.8603\n",
      "num_test_corrects / test_total_examples = 7857 / 9217\n",
      "testing accuracy = 0.8524\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 31674 / 36865\n",
      "training loss = 0.4353\n",
      "training accuracy = 0.8592\n",
      "num_test_corrects / test_total_examples = 7850 / 9217\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 31698 / 36865\n",
      "training loss = 0.4323\n",
      "training accuracy = 0.8598\n",
      "num_test_corrects / test_total_examples = 7863 / 9217\n",
      "testing accuracy = 0.8531\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 31749 / 36865\n",
      "training loss = 0.4327\n",
      "training accuracy = 0.8612\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 31712 / 36865\n",
      "training loss = 0.4347\n",
      "training accuracy = 0.8602\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "found best test accuracy at epoch 184\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 31734 / 36865\n",
      "training loss = 0.4289\n",
      "training accuracy = 0.8608\n",
      "num_test_corrects / test_total_examples = 7868 / 9217\n",
      "testing accuracy = 0.8536\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 31783 / 36865\n",
      "training loss = 0.4339\n",
      "training accuracy = 0.8621\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 31779 / 36865\n",
      "training loss = 0.4253\n",
      "training accuracy = 0.8620\n",
      "num_test_corrects / test_total_examples = 7880 / 9217\n",
      "testing accuracy = 0.8549\n",
      "found best test accuracy at epoch 187\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 31755 / 36865\n",
      "training loss = 0.4345\n",
      "training accuracy = 0.8614\n",
      "num_test_corrects / test_total_examples = 7872 / 9217\n",
      "testing accuracy = 0.8541\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 31757 / 36865\n",
      "training loss = 0.4340\n",
      "training accuracy = 0.8614\n",
      "num_test_corrects / test_total_examples = 7895 / 9217\n",
      "testing accuracy = 0.8566\n",
      "found best test accuracy at epoch 189\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 31711 / 36865\n",
      "training loss = 0.4299\n",
      "training accuracy = 0.8602\n",
      "num_test_corrects / test_total_examples = 7862 / 9217\n",
      "testing accuracy = 0.8530\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 31776 / 36865\n",
      "training loss = 0.4338\n",
      "training accuracy = 0.8620\n",
      "num_test_corrects / test_total_examples = 7868 / 9217\n",
      "testing accuracy = 0.8536\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 31774 / 36865\n",
      "training loss = 0.4309\n",
      "training accuracy = 0.8619\n",
      "num_test_corrects / test_total_examples = 7878 / 9217\n",
      "testing accuracy = 0.8547\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 31770 / 36865\n",
      "training loss = 0.4257\n",
      "training accuracy = 0.8618\n",
      "num_test_corrects / test_total_examples = 7882 / 9217\n",
      "testing accuracy = 0.8552\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 31812 / 36865\n",
      "training loss = 0.4243\n",
      "training accuracy = 0.8629\n",
      "num_test_corrects / test_total_examples = 7867 / 9217\n",
      "testing accuracy = 0.8535\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 31788 / 36865\n",
      "training loss = 0.4291\n",
      "training accuracy = 0.8623\n",
      "num_test_corrects / test_total_examples = 7886 / 9217\n",
      "testing accuracy = 0.8556\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 31845 / 36865\n",
      "training loss = 0.4325\n",
      "training accuracy = 0.8638\n",
      "num_test_corrects / test_total_examples = 7884 / 9217\n",
      "testing accuracy = 0.8554\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 31711 / 36865\n",
      "training loss = 0.4300\n",
      "training accuracy = 0.8602\n",
      "num_test_corrects / test_total_examples = 7860 / 9217\n",
      "testing accuracy = 0.8528\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 31823 / 36865\n",
      "training loss = 0.4254\n",
      "training accuracy = 0.8632\n",
      "num_test_corrects / test_total_examples = 7876 / 9217\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 31822 / 36865\n",
      "training loss = 0.4274\n",
      "training accuracy = 0.8632\n",
      "num_test_corrects / test_total_examples = 7897 / 9217\n",
      "testing accuracy = 0.8568\n",
      "found best test accuracy at epoch 199\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 31703 / 36865\n",
      "training loss = 0.4332\n",
      "training accuracy = 0.8600\n",
      "num_test_corrects / test_total_examples = 7655 / 9217\n",
      "testing accuracy = 0.8305\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 31765 / 36865\n",
      "training loss = 0.4393\n",
      "training accuracy = 0.8617\n",
      "num_test_corrects / test_total_examples = 7877 / 9217\n",
      "testing accuracy = 0.8546\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 31811 / 36865\n",
      "training loss = 0.4278\n",
      "training accuracy = 0.8629\n",
      "num_test_corrects / test_total_examples = 7880 / 9217\n",
      "testing accuracy = 0.8549\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 31792 / 36865\n",
      "training loss = 0.4246\n",
      "training accuracy = 0.8624\n",
      "num_test_corrects / test_total_examples = 7883 / 9217\n",
      "testing accuracy = 0.8553\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 31816 / 36865\n",
      "training loss = 0.4291\n",
      "training accuracy = 0.8630\n",
      "num_test_corrects / test_total_examples = 7878 / 9217\n",
      "testing accuracy = 0.8547\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 31853 / 36865\n",
      "training loss = 0.4247\n",
      "training accuracy = 0.8640\n",
      "num_test_corrects / test_total_examples = 7901 / 9217\n",
      "testing accuracy = 0.8572\n",
      "found best test accuracy at epoch 205\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 31823 / 36865\n",
      "training loss = 0.4222\n",
      "training accuracy = 0.8632\n",
      "num_test_corrects / test_total_examples = 7876 / 9217\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 31914 / 36865\n",
      "training loss = 0.4215\n",
      "training accuracy = 0.8657\n",
      "num_test_corrects / test_total_examples = 7872 / 9217\n",
      "testing accuracy = 0.8541\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 31822 / 36865\n",
      "training loss = 0.4249\n",
      "training accuracy = 0.8632\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "found best test accuracy at epoch 208\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 31873 / 36865\n",
      "training loss = 0.4284\n",
      "training accuracy = 0.8646\n",
      "num_test_corrects / test_total_examples = 7920 / 9217\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 31851 / 36865\n",
      "training loss = 0.4262\n",
      "training accuracy = 0.8640\n",
      "num_test_corrects / test_total_examples = 7894 / 9217\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 31877 / 36865\n",
      "training loss = 0.4382\n",
      "training accuracy = 0.8647\n",
      "num_test_corrects / test_total_examples = 7876 / 9217\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 31918 / 36865\n",
      "training loss = 0.4230\n",
      "training accuracy = 0.8658\n",
      "num_test_corrects / test_total_examples = 7844 / 9217\n",
      "testing accuracy = 0.8510\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 31862 / 36865\n",
      "training loss = 0.4214\n",
      "training accuracy = 0.8643\n",
      "num_test_corrects / test_total_examples = 7862 / 9217\n",
      "testing accuracy = 0.8530\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 31885 / 36865\n",
      "training loss = 0.4219\n",
      "training accuracy = 0.8649\n",
      "num_test_corrects / test_total_examples = 7890 / 9217\n",
      "testing accuracy = 0.8560\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 31897 / 36865\n",
      "training loss = 0.4207\n",
      "training accuracy = 0.8652\n",
      "num_test_corrects / test_total_examples = 7907 / 9217\n",
      "testing accuracy = 0.8579\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 31876 / 36865\n",
      "training loss = 0.4203\n",
      "training accuracy = 0.8647\n",
      "num_test_corrects / test_total_examples = 7898 / 9217\n",
      "testing accuracy = 0.8569\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 31907 / 36865\n",
      "training loss = 0.4216\n",
      "training accuracy = 0.8655\n",
      "num_test_corrects / test_total_examples = 7866 / 9217\n",
      "testing accuracy = 0.8534\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 31928 / 36865\n",
      "training loss = 0.4209\n",
      "training accuracy = 0.8661\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 31938 / 36865\n",
      "training loss = 0.4275\n",
      "training accuracy = 0.8664\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "found best test accuracy at epoch 219\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 31881 / 36865\n",
      "training loss = 0.4267\n",
      "training accuracy = 0.8648\n",
      "num_test_corrects / test_total_examples = 7911 / 9217\n",
      "testing accuracy = 0.8583\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 31890 / 36865\n",
      "training loss = 0.4211\n",
      "training accuracy = 0.8650\n",
      "num_test_corrects / test_total_examples = 7891 / 9217\n",
      "testing accuracy = 0.8561\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 31944 / 36865\n",
      "training loss = 0.4195\n",
      "training accuracy = 0.8665\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 31968 / 36865\n",
      "training loss = 0.4170\n",
      "training accuracy = 0.8672\n",
      "num_test_corrects / test_total_examples = 7917 / 9217\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 31887 / 36865\n",
      "training loss = 0.4243\n",
      "training accuracy = 0.8650\n",
      "num_test_corrects / test_total_examples = 7917 / 9217\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 31948 / 36865\n",
      "training loss = 0.4313\n",
      "training accuracy = 0.8666\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "found best test accuracy at epoch 225\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 31930 / 36865\n",
      "training loss = 0.4235\n",
      "training accuracy = 0.8661\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 31968 / 36865\n",
      "training loss = 0.4185\n",
      "training accuracy = 0.8672\n",
      "num_test_corrects / test_total_examples = 7922 / 9217\n",
      "testing accuracy = 0.8595\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 31910 / 36865\n",
      "training loss = 0.4184\n",
      "training accuracy = 0.8656\n",
      "num_test_corrects / test_total_examples = 7755 / 9217\n",
      "testing accuracy = 0.8414\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 31945 / 36865\n",
      "training loss = 0.4289\n",
      "training accuracy = 0.8665\n",
      "num_test_corrects / test_total_examples = 7891 / 9217\n",
      "testing accuracy = 0.8561\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 31904 / 36865\n",
      "training loss = 0.4239\n",
      "training accuracy = 0.8654\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 31964 / 36865\n",
      "training loss = 0.4196\n",
      "training accuracy = 0.8671\n",
      "num_test_corrects / test_total_examples = 7903 / 9217\n",
      "testing accuracy = 0.8574\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 31932 / 36865\n",
      "training loss = 0.4256\n",
      "training accuracy = 0.8662\n",
      "num_test_corrects / test_total_examples = 7919 / 9217\n",
      "testing accuracy = 0.8592\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 32010 / 36865\n",
      "training loss = 0.4230\n",
      "training accuracy = 0.8683\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 31988 / 36865\n",
      "training loss = 0.4282\n",
      "training accuracy = 0.8677\n",
      "num_test_corrects / test_total_examples = 7835 / 9217\n",
      "testing accuracy = 0.8501\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 31914 / 36865\n",
      "training loss = 0.4209\n",
      "training accuracy = 0.8657\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 31948 / 36865\n",
      "training loss = 0.4151\n",
      "training accuracy = 0.8666\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 32026 / 36865\n",
      "training loss = 0.4252\n",
      "training accuracy = 0.8687\n",
      "num_test_corrects / test_total_examples = 7925 / 9217\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 31994 / 36865\n",
      "training loss = 0.4208\n",
      "training accuracy = 0.8679\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "found best test accuracy at epoch 238\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 31989 / 36865\n",
      "training loss = 0.4149\n",
      "training accuracy = 0.8677\n",
      "num_test_corrects / test_total_examples = 7887 / 9217\n",
      "testing accuracy = 0.8557\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 32020 / 36865\n",
      "training loss = 0.4188\n",
      "training accuracy = 0.8686\n",
      "num_test_corrects / test_total_examples = 7890 / 9217\n",
      "testing accuracy = 0.8560\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 32034 / 36865\n",
      "training loss = 0.4229\n",
      "training accuracy = 0.8690\n",
      "num_test_corrects / test_total_examples = 7913 / 9217\n",
      "testing accuracy = 0.8585\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 32018 / 36865\n",
      "training loss = 0.4200\n",
      "training accuracy = 0.8685\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 32034 / 36865\n",
      "training loss = 0.4149\n",
      "training accuracy = 0.8690\n",
      "num_test_corrects / test_total_examples = 7915 / 9217\n",
      "testing accuracy = 0.8587\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 32036 / 36865\n",
      "training loss = 0.4182\n",
      "training accuracy = 0.8690\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 31985 / 36865\n",
      "training loss = 0.4239\n",
      "training accuracy = 0.8676\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 31960 / 36865\n",
      "training loss = 0.4198\n",
      "training accuracy = 0.8669\n",
      "num_test_corrects / test_total_examples = 7918 / 9217\n",
      "testing accuracy = 0.8591\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 31964 / 36865\n",
      "training loss = 0.4291\n",
      "training accuracy = 0.8671\n",
      "num_test_corrects / test_total_examples = 7859 / 9217\n",
      "testing accuracy = 0.8527\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 32027 / 36865\n",
      "training loss = 0.4250\n",
      "training accuracy = 0.8688\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 32027 / 36865\n",
      "training loss = 0.4246\n",
      "training accuracy = 0.8688\n",
      "num_test_corrects / test_total_examples = 7895 / 9217\n",
      "testing accuracy = 0.8566\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 32014 / 36865\n",
      "training loss = 0.4156\n",
      "training accuracy = 0.8684\n",
      "num_test_corrects / test_total_examples = 7893 / 9217\n",
      "testing accuracy = 0.8564\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 31994 / 36865\n",
      "training loss = 0.4191\n",
      "training accuracy = 0.8679\n",
      "num_test_corrects / test_total_examples = 7826 / 9217\n",
      "testing accuracy = 0.8491\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 32091 / 36865\n",
      "training loss = 0.4133\n",
      "training accuracy = 0.8705\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 31990 / 36865\n",
      "training loss = 0.4225\n",
      "training accuracy = 0.8678\n",
      "num_test_corrects / test_total_examples = 7921 / 9217\n",
      "testing accuracy = 0.8594\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 31955 / 36865\n",
      "training loss = 0.4281\n",
      "training accuracy = 0.8668\n",
      "num_test_corrects / test_total_examples = 7904 / 9217\n",
      "testing accuracy = 0.8575\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 32011 / 36865\n",
      "training loss = 0.4177\n",
      "training accuracy = 0.8683\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 32041 / 36865\n",
      "training loss = 0.4215\n",
      "training accuracy = 0.8691\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 32053 / 36865\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8695\n",
      "num_test_corrects / test_total_examples = 7929 / 9217\n",
      "testing accuracy = 0.8603\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 32052 / 36865\n",
      "training loss = 0.4155\n",
      "training accuracy = 0.8694\n",
      "num_test_corrects / test_total_examples = 7930 / 9217\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 32065 / 36865\n",
      "training loss = 0.4161\n",
      "training accuracy = 0.8698\n",
      "num_test_corrects / test_total_examples = 7875 / 9217\n",
      "testing accuracy = 0.8544\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 32083 / 36865\n",
      "training loss = 0.4174\n",
      "training accuracy = 0.8703\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "found best test accuracy at epoch 260\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 32040 / 36865\n",
      "training loss = 0.4177\n",
      "training accuracy = 0.8691\n",
      "num_test_corrects / test_total_examples = 7938 / 9217\n",
      "testing accuracy = 0.8612\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 32074 / 36865\n",
      "training loss = 0.4268\n",
      "training accuracy = 0.8700\n",
      "num_test_corrects / test_total_examples = 7916 / 9217\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 32098 / 36865\n",
      "training loss = 0.4142\n",
      "training accuracy = 0.8707\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 32011 / 36865\n",
      "training loss = 0.4146\n",
      "training accuracy = 0.8683\n",
      "num_test_corrects / test_total_examples = 7929 / 9217\n",
      "testing accuracy = 0.8603\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 32103 / 36865\n",
      "training loss = 0.4199\n",
      "training accuracy = 0.8708\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 32042 / 36865\n",
      "training loss = 0.4117\n",
      "training accuracy = 0.8692\n",
      "num_test_corrects / test_total_examples = 7938 / 9217\n",
      "testing accuracy = 0.8612\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 32034 / 36865\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8690\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 32073 / 36865\n",
      "training loss = 0.4227\n",
      "training accuracy = 0.8700\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "found best test accuracy at epoch 268\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 32111 / 36865\n",
      "training loss = 0.4175\n",
      "training accuracy = 0.8710\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "found best test accuracy at epoch 269\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 32092 / 36865\n",
      "training loss = 0.4197\n",
      "training accuracy = 0.8705\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "found best test accuracy at epoch 270\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 32103 / 36865\n",
      "training loss = 0.4177\n",
      "training accuracy = 0.8708\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 32128 / 36865\n",
      "training loss = 0.4097\n",
      "training accuracy = 0.8715\n",
      "num_test_corrects / test_total_examples = 7920 / 9217\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 32094 / 36865\n",
      "training loss = 0.4227\n",
      "training accuracy = 0.8706\n",
      "num_test_corrects / test_total_examples = 7863 / 9217\n",
      "testing accuracy = 0.8531\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 32111 / 36865\n",
      "training loss = 0.4171\n",
      "training accuracy = 0.8710\n",
      "num_test_corrects / test_total_examples = 7924 / 9217\n",
      "testing accuracy = 0.8597\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 32142 / 36865\n",
      "training loss = 0.4123\n",
      "training accuracy = 0.8719\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "found best test accuracy at epoch 275\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 32214 / 36865\n",
      "training loss = 0.4106\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 7948 / 9217\n",
      "testing accuracy = 0.8623\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 32169 / 36865\n",
      "training loss = 0.4148\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 7940 / 9217\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 32174 / 36865\n",
      "training loss = 0.4156\n",
      "training accuracy = 0.8728\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 32103 / 36865\n",
      "training loss = 0.4147\n",
      "training accuracy = 0.8708\n",
      "num_test_corrects / test_total_examples = 7938 / 9217\n",
      "testing accuracy = 0.8612\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 32098 / 36865\n",
      "training loss = 0.4155\n",
      "training accuracy = 0.8707\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 32060 / 36865\n",
      "training loss = 0.4190\n",
      "training accuracy = 0.8697\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 32184 / 36865\n",
      "training loss = 0.4136\n",
      "training accuracy = 0.8730\n",
      "num_test_corrects / test_total_examples = 7916 / 9217\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 32086 / 36865\n",
      "training loss = 0.4209\n",
      "training accuracy = 0.8704\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 32125 / 36865\n",
      "training loss = 0.4182\n",
      "training accuracy = 0.8714\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 32106 / 36865\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8709\n",
      "num_test_corrects / test_total_examples = 7924 / 9217\n",
      "testing accuracy = 0.8597\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 32211 / 36865\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 32115 / 36865\n",
      "training loss = 0.4181\n",
      "training accuracy = 0.8712\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 32161 / 36865\n",
      "training loss = 0.4108\n",
      "training accuracy = 0.8724\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 32196 / 36865\n",
      "training loss = 0.4106\n",
      "training accuracy = 0.8733\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 32146 / 36865\n",
      "training loss = 0.4125\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 7903 / 9217\n",
      "testing accuracy = 0.8574\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 32182 / 36865\n",
      "training loss = 0.4115\n",
      "training accuracy = 0.8730\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 32183 / 36865\n",
      "training loss = 0.4167\n",
      "training accuracy = 0.8730\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 32208 / 36865\n",
      "training loss = 0.4108\n",
      "training accuracy = 0.8737\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 32129 / 36865\n",
      "training loss = 0.4204\n",
      "training accuracy = 0.8715\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 32169 / 36865\n",
      "training loss = 0.4133\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 7934 / 9217\n",
      "testing accuracy = 0.8608\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 32142 / 36865\n",
      "training loss = 0.4094\n",
      "training accuracy = 0.8719\n",
      "num_test_corrects / test_total_examples = 7913 / 9217\n",
      "testing accuracy = 0.8585\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 32145 / 36865\n",
      "training loss = 0.4141\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 7908 / 9217\n",
      "testing accuracy = 0.8580\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 32168 / 36865\n",
      "training loss = 0.4115\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 7882 / 9217\n",
      "testing accuracy = 0.8552\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 32203 / 36865\n",
      "training loss = 0.4125\n",
      "training accuracy = 0.8735\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 32198 / 36865\n",
      "training loss = 0.4051\n",
      "training accuracy = 0.8734\n",
      "num_test_corrects / test_total_examples = 7939 / 9217\n",
      "testing accuracy = 0.8613\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 32163 / 36865\n",
      "training loss = 0.4141\n",
      "training accuracy = 0.8725\n",
      "num_test_corrects / test_total_examples = 7911 / 9217\n",
      "testing accuracy = 0.8583\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 32199 / 36865\n",
      "training loss = 0.4066\n",
      "training accuracy = 0.8734\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 32196 / 36865\n",
      "training loss = 0.4090\n",
      "training accuracy = 0.8733\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 32219 / 36865\n",
      "training loss = 0.4218\n",
      "training accuracy = 0.8740\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 32195 / 36865\n",
      "training loss = 0.4140\n",
      "training accuracy = 0.8733\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 32212 / 36865\n",
      "training loss = 0.4146\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 7920 / 9217\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 32232 / 36865\n",
      "training loss = 0.4114\n",
      "training accuracy = 0.8743\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 32225 / 36865\n",
      "training loss = 0.4136\n",
      "training accuracy = 0.8741\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 32239 / 36865\n",
      "training loss = 0.4124\n",
      "training accuracy = 0.8745\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "found best test accuracy at epoch 309\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 32212 / 36865\n",
      "training loss = 0.4121\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 7915 / 9217\n",
      "testing accuracy = 0.8587\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 32258 / 36865\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8750\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 32123 / 36865\n",
      "training loss = 0.4172\n",
      "training accuracy = 0.8714\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "found best test accuracy at epoch 312\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 32245 / 36865\n",
      "training loss = 0.4114\n",
      "training accuracy = 0.8747\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 32258 / 36865\n",
      "training loss = 0.4125\n",
      "training accuracy = 0.8750\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 32245 / 36865\n",
      "training loss = 0.4146\n",
      "training accuracy = 0.8747\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 32171 / 36865\n",
      "training loss = 0.4262\n",
      "training accuracy = 0.8727\n",
      "num_test_corrects / test_total_examples = 7790 / 9217\n",
      "testing accuracy = 0.8452\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 32266 / 36865\n",
      "training loss = 0.4134\n",
      "training accuracy = 0.8752\n",
      "num_test_corrects / test_total_examples = 7922 / 9217\n",
      "testing accuracy = 0.8595\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 32227 / 36865\n",
      "training loss = 0.4160\n",
      "training accuracy = 0.8742\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 32220 / 36865\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8740\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 32250 / 36865\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8748\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 32288 / 36865\n",
      "training loss = 0.4138\n",
      "training accuracy = 0.8758\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 32274 / 36865\n",
      "training loss = 0.4076\n",
      "training accuracy = 0.8755\n",
      "num_test_corrects / test_total_examples = 7938 / 9217\n",
      "testing accuracy = 0.8612\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 32285 / 36865\n",
      "training loss = 0.4142\n",
      "training accuracy = 0.8758\n",
      "num_test_corrects / test_total_examples = 7920 / 9217\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 32312 / 36865\n",
      "training loss = 0.4125\n",
      "training accuracy = 0.8765\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 32199 / 36865\n",
      "training loss = 0.4265\n",
      "training accuracy = 0.8734\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 32238 / 36865\n",
      "training loss = 0.4150\n",
      "training accuracy = 0.8745\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 32257 / 36865\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8750\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 32306 / 36865\n",
      "training loss = 0.4118\n",
      "training accuracy = 0.8763\n",
      "num_test_corrects / test_total_examples = 7945 / 9217\n",
      "testing accuracy = 0.8620\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 32142 / 36865\n",
      "training loss = 0.4137\n",
      "training accuracy = 0.8719\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 32285 / 36865\n",
      "training loss = 0.4037\n",
      "training accuracy = 0.8758\n",
      "num_test_corrects / test_total_examples = 7891 / 9217\n",
      "testing accuracy = 0.8561\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 32245 / 36865\n",
      "training loss = 0.4156\n",
      "training accuracy = 0.8747\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 32243 / 36865\n",
      "training loss = 0.4121\n",
      "training accuracy = 0.8746\n",
      "num_test_corrects / test_total_examples = 7921 / 9217\n",
      "testing accuracy = 0.8594\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 32240 / 36865\n",
      "training loss = 0.4141\n",
      "training accuracy = 0.8745\n",
      "num_test_corrects / test_total_examples = 7899 / 9217\n",
      "testing accuracy = 0.8570\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 32329 / 36865\n",
      "training loss = 0.4031\n",
      "training accuracy = 0.8770\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 32248 / 36865\n",
      "training loss = 0.4159\n",
      "training accuracy = 0.8748\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 32264 / 36865\n",
      "training loss = 0.4131\n",
      "training accuracy = 0.8752\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 32280 / 36865\n",
      "training loss = 0.4184\n",
      "training accuracy = 0.8756\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 32287 / 36865\n",
      "training loss = 0.4080\n",
      "training accuracy = 0.8758\n",
      "num_test_corrects / test_total_examples = 7987 / 9217\n",
      "testing accuracy = 0.8666\n",
      "found best test accuracy at epoch 338\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 32298 / 36865\n",
      "training loss = 0.4046\n",
      "training accuracy = 0.8761\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 32329 / 36865\n",
      "training loss = 0.4139\n",
      "training accuracy = 0.8770\n",
      "num_test_corrects / test_total_examples = 7876 / 9217\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 32302 / 36865\n",
      "training loss = 0.4134\n",
      "training accuracy = 0.8762\n",
      "num_test_corrects / test_total_examples = 7948 / 9217\n",
      "testing accuracy = 0.8623\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 32337 / 36865\n",
      "training loss = 0.4089\n",
      "training accuracy = 0.8772\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 32226 / 36865\n",
      "training loss = 0.4131\n",
      "training accuracy = 0.8742\n",
      "num_test_corrects / test_total_examples = 7853 / 9217\n",
      "testing accuracy = 0.8520\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 32300 / 36865\n",
      "training loss = 0.4214\n",
      "training accuracy = 0.8762\n",
      "num_test_corrects / test_total_examples = 7878 / 9217\n",
      "testing accuracy = 0.8547\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 32290 / 36865\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8759\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 32296 / 36865\n",
      "training loss = 0.4051\n",
      "training accuracy = 0.8761\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 32187 / 36865\n",
      "training loss = 0.4288\n",
      "training accuracy = 0.8731\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 32318 / 36865\n",
      "training loss = 0.4099\n",
      "training accuracy = 0.8767\n",
      "num_test_corrects / test_total_examples = 7945 / 9217\n",
      "testing accuracy = 0.8620\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 32311 / 36865\n",
      "training loss = 0.4049\n",
      "training accuracy = 0.8765\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 32309 / 36865\n",
      "training loss = 0.4109\n",
      "training accuracy = 0.8764\n",
      "num_test_corrects / test_total_examples = 7904 / 9217\n",
      "testing accuracy = 0.8575\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 32330 / 36865\n",
      "training loss = 0.4051\n",
      "training accuracy = 0.8770\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 32247 / 36865\n",
      "training loss = 0.4088\n",
      "training accuracy = 0.8747\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 32320 / 36865\n",
      "training loss = 0.4115\n",
      "training accuracy = 0.8767\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 32302 / 36865\n",
      "training loss = 0.4070\n",
      "training accuracy = 0.8762\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 32324 / 36865\n",
      "training loss = 0.4094\n",
      "training accuracy = 0.8768\n",
      "num_test_corrects / test_total_examples = 7944 / 9217\n",
      "testing accuracy = 0.8619\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 32364 / 36865\n",
      "training loss = 0.4012\n",
      "training accuracy = 0.8779\n",
      "num_test_corrects / test_total_examples = 7906 / 9217\n",
      "testing accuracy = 0.8578\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 32346 / 36865\n",
      "training loss = 0.4028\n",
      "training accuracy = 0.8774\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 32391 / 36865\n",
      "training loss = 0.4071\n",
      "training accuracy = 0.8786\n",
      "num_test_corrects / test_total_examples = 7909 / 9217\n",
      "testing accuracy = 0.8581\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 32277 / 36865\n",
      "training loss = 0.4060\n",
      "training accuracy = 0.8755\n",
      "num_test_corrects / test_total_examples = 7896 / 9217\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 32365 / 36865\n",
      "training loss = 0.4061\n",
      "training accuracy = 0.8779\n",
      "num_test_corrects / test_total_examples = 7988 / 9217\n",
      "testing accuracy = 0.8667\n",
      "found best test accuracy at epoch 360\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 32330 / 36865\n",
      "training loss = 0.4068\n",
      "training accuracy = 0.8770\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 32295 / 36865\n",
      "training loss = 0.4164\n",
      "training accuracy = 0.8760\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 32331 / 36865\n",
      "training loss = 0.4039\n",
      "training accuracy = 0.8770\n",
      "num_test_corrects / test_total_examples = 7884 / 9217\n",
      "testing accuracy = 0.8554\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 32376 / 36865\n",
      "training loss = 0.4047\n",
      "training accuracy = 0.8782\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 32311 / 36865\n",
      "training loss = 0.4137\n",
      "training accuracy = 0.8765\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 32338 / 36865\n",
      "training loss = 0.4025\n",
      "training accuracy = 0.8772\n",
      "num_test_corrects / test_total_examples = 7916 / 9217\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 32400 / 36865\n",
      "training loss = 0.4004\n",
      "training accuracy = 0.8789\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 32367 / 36865\n",
      "training loss = 0.4078\n",
      "training accuracy = 0.8780\n",
      "num_test_corrects / test_total_examples = 7887 / 9217\n",
      "testing accuracy = 0.8557\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 32361 / 36865\n",
      "training loss = 0.4016\n",
      "training accuracy = 0.8778\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 32360 / 36865\n",
      "training loss = 0.4072\n",
      "training accuracy = 0.8778\n",
      "num_test_corrects / test_total_examples = 7882 / 9217\n",
      "testing accuracy = 0.8552\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 32399 / 36865\n",
      "training loss = 0.4051\n",
      "training accuracy = 0.8789\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 32387 / 36865\n",
      "training loss = 0.4040\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 32345 / 36865\n",
      "training loss = 0.3983\n",
      "training accuracy = 0.8774\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 32346 / 36865\n",
      "training loss = 0.4047\n",
      "training accuracy = 0.8774\n",
      "num_test_corrects / test_total_examples = 7858 / 9217\n",
      "testing accuracy = 0.8526\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 32383 / 36865\n",
      "training loss = 0.4053\n",
      "training accuracy = 0.8784\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 32321 / 36865\n",
      "training loss = 0.4065\n",
      "training accuracy = 0.8767\n",
      "num_test_corrects / test_total_examples = 7890 / 9217\n",
      "testing accuracy = 0.8560\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 32407 / 36865\n",
      "training loss = 0.3998\n",
      "training accuracy = 0.8791\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 32351 / 36865\n",
      "training loss = 0.4163\n",
      "training accuracy = 0.8776\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 32422 / 36865\n",
      "training loss = 0.4051\n",
      "training accuracy = 0.8795\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 32412 / 36865\n",
      "training loss = 0.4045\n",
      "training accuracy = 0.8792\n",
      "num_test_corrects / test_total_examples = 7862 / 9217\n",
      "testing accuracy = 0.8530\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 32305 / 36865\n",
      "training loss = 0.4110\n",
      "training accuracy = 0.8763\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 32385 / 36865\n",
      "training loss = 0.4066\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 32393 / 36865\n",
      "training loss = 0.4075\n",
      "training accuracy = 0.8787\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 32346 / 36865\n",
      "training loss = 0.4163\n",
      "training accuracy = 0.8774\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 32409 / 36865\n",
      "training loss = 0.4025\n",
      "training accuracy = 0.8791\n",
      "num_test_corrects / test_total_examples = 7993 / 9217\n",
      "testing accuracy = 0.8672\n",
      "found best test accuracy at epoch 385\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 32412 / 36865\n",
      "training loss = 0.4104\n",
      "training accuracy = 0.8792\n",
      "num_test_corrects / test_total_examples = 7899 / 9217\n",
      "testing accuracy = 0.8570\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 32400 / 36865\n",
      "training loss = 0.4093\n",
      "training accuracy = 0.8789\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 32381 / 36865\n",
      "training loss = 0.4074\n",
      "training accuracy = 0.8784\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 32396 / 36865\n",
      "training loss = 0.4132\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 32437 / 36865\n",
      "training loss = 0.4061\n",
      "training accuracy = 0.8799\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 32425 / 36865\n",
      "training loss = 0.4034\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 7894 / 9217\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 32342 / 36865\n",
      "training loss = 0.4090\n",
      "training accuracy = 0.8773\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 32382 / 36865\n",
      "training loss = 0.4120\n",
      "training accuracy = 0.8784\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 32456 / 36865\n",
      "training loss = 0.4039\n",
      "training accuracy = 0.8804\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 32398 / 36865\n",
      "training loss = 0.4084\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 32387 / 36865\n",
      "training loss = 0.4080\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 32346 / 36865\n",
      "training loss = 0.4088\n",
      "training accuracy = 0.8774\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 32463 / 36865\n",
      "training loss = 0.4041\n",
      "training accuracy = 0.8806\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 32427 / 36865\n",
      "training loss = 0.4018\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 32411 / 36865\n",
      "training loss = 0.4074\n",
      "training accuracy = 0.8792\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 32395 / 36865\n",
      "training loss = 0.4103\n",
      "training accuracy = 0.8787\n",
      "num_test_corrects / test_total_examples = 7934 / 9217\n",
      "testing accuracy = 0.8608\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 32445 / 36865\n",
      "training loss = 0.4138\n",
      "training accuracy = 0.8801\n",
      "num_test_corrects / test_total_examples = 7917 / 9217\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 32401 / 36865\n",
      "training loss = 0.4160\n",
      "training accuracy = 0.8789\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 32451 / 36865\n",
      "training loss = 0.4040\n",
      "training accuracy = 0.8803\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 32397 / 36865\n",
      "training loss = 0.4036\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 32459 / 36865\n",
      "training loss = 0.4010\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 32475 / 36865\n",
      "training loss = 0.4022\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 32408 / 36865\n",
      "training loss = 0.4059\n",
      "training accuracy = 0.8791\n",
      "num_test_corrects / test_total_examples = 7974 / 9217\n",
      "testing accuracy = 0.8651\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 32479 / 36865\n",
      "training loss = 0.3993\n",
      "training accuracy = 0.8810\n",
      "num_test_corrects / test_total_examples = 7974 / 9217\n",
      "testing accuracy = 0.8651\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 32476 / 36865\n",
      "training loss = 0.3957\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 7908 / 9217\n",
      "testing accuracy = 0.8580\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 32459 / 36865\n",
      "training loss = 0.4028\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7945 / 9217\n",
      "testing accuracy = 0.8620\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 32386 / 36865\n",
      "training loss = 0.4011\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 32458 / 36865\n",
      "training loss = 0.4046\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7892 / 9217\n",
      "testing accuracy = 0.8562\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 32441 / 36865\n",
      "training loss = 0.4028\n",
      "training accuracy = 0.8800\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 32398 / 36865\n",
      "training loss = 0.3984\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 32459 / 36865\n",
      "training loss = 0.3979\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 32426 / 36865\n",
      "training loss = 0.4024\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 32423 / 36865\n",
      "training loss = 0.4011\n",
      "training accuracy = 0.8795\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 32442 / 36865\n",
      "training loss = 0.4050\n",
      "training accuracy = 0.8800\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 32485 / 36865\n",
      "training loss = 0.4088\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 32498 / 36865\n",
      "training loss = 0.4025\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 32434 / 36865\n",
      "training loss = 0.3988\n",
      "training accuracy = 0.8798\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 32412 / 36865\n",
      "training loss = 0.4003\n",
      "training accuracy = 0.8792\n",
      "num_test_corrects / test_total_examples = 7909 / 9217\n",
      "testing accuracy = 0.8581\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 32486 / 36865\n",
      "training loss = 0.4044\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 7896 / 9217\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 32426 / 36865\n",
      "training loss = 0.3983\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 7914 / 9217\n",
      "testing accuracy = 0.8586\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 32501 / 36865\n",
      "training loss = 0.3966\n",
      "training accuracy = 0.8816\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 32479 / 36865\n",
      "training loss = 0.3960\n",
      "training accuracy = 0.8810\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 32524 / 36865\n",
      "training loss = 0.3982\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7889 / 9217\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 32460 / 36865\n",
      "training loss = 0.4107\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 32485 / 36865\n",
      "training loss = 0.4055\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 32452 / 36865\n",
      "training loss = 0.3979\n",
      "training accuracy = 0.8803\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 32497 / 36865\n",
      "training loss = 0.4044\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 7974 / 9217\n",
      "testing accuracy = 0.8651\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 32558 / 36865\n",
      "training loss = 0.3955\n",
      "training accuracy = 0.8832\n",
      "num_test_corrects / test_total_examples = 7899 / 9217\n",
      "testing accuracy = 0.8570\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 32521 / 36865\n",
      "training loss = 0.4084\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 32516 / 36865\n",
      "training loss = 0.4021\n",
      "training accuracy = 0.8820\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 32504 / 36865\n",
      "training loss = 0.3951\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 7944 / 9217\n",
      "testing accuracy = 0.8619\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 32533 / 36865\n",
      "training loss = 0.4043\n",
      "training accuracy = 0.8825\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 32506 / 36865\n",
      "training loss = 0.3977\n",
      "training accuracy = 0.8818\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 32491 / 36865\n",
      "training loss = 0.4004\n",
      "training accuracy = 0.8814\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 32510 / 36865\n",
      "training loss = 0.4028\n",
      "training accuracy = 0.8819\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 32538 / 36865\n",
      "training loss = 0.3929\n",
      "training accuracy = 0.8826\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 32483 / 36865\n",
      "training loss = 0.3932\n",
      "training accuracy = 0.8811\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 32483 / 36865\n",
      "training loss = 0.3946\n",
      "training accuracy = 0.8811\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "found best test accuracy at epoch 443\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 32480 / 36865\n",
      "training loss = 0.4060\n",
      "training accuracy = 0.8811\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 32492 / 36865\n",
      "training loss = 0.3923\n",
      "training accuracy = 0.8814\n",
      "num_test_corrects / test_total_examples = 7938 / 9217\n",
      "testing accuracy = 0.8612\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 32466 / 36865\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8807\n",
      "num_test_corrects / test_total_examples = 7992 / 9217\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 32523 / 36865\n",
      "training loss = 0.3974\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 32521 / 36865\n",
      "training loss = 0.3997\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 32465 / 36865\n",
      "training loss = 0.4013\n",
      "training accuracy = 0.8806\n",
      "num_test_corrects / test_total_examples = 7822 / 9217\n",
      "testing accuracy = 0.8486\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 32540 / 36865\n",
      "training loss = 0.3998\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 32472 / 36865\n",
      "training loss = 0.4044\n",
      "training accuracy = 0.8808\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 32524 / 36865\n",
      "training loss = 0.4011\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 32469 / 36865\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8808\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 32533 / 36865\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8825\n",
      "num_test_corrects / test_total_examples = 7738 / 9217\n",
      "testing accuracy = 0.8395\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 32501 / 36865\n",
      "training loss = 0.4004\n",
      "training accuracy = 0.8816\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 32546 / 36865\n",
      "training loss = 0.3920\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 32461 / 36865\n",
      "training loss = 0.3967\n",
      "training accuracy = 0.8805\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 32487 / 36865\n",
      "training loss = 0.4075\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 32553 / 36865\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8830\n",
      "num_test_corrects / test_total_examples = 7976 / 9217\n",
      "testing accuracy = 0.8654\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 32520 / 36865\n",
      "training loss = 0.3950\n",
      "training accuracy = 0.8821\n",
      "num_test_corrects / test_total_examples = 7924 / 9217\n",
      "testing accuracy = 0.8597\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 32525 / 36865\n",
      "training loss = 0.3966\n",
      "training accuracy = 0.8823\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 32550 / 36865\n",
      "training loss = 0.3951\n",
      "training accuracy = 0.8830\n",
      "num_test_corrects / test_total_examples = 7889 / 9217\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 32555 / 36865\n",
      "training loss = 0.3923\n",
      "training accuracy = 0.8831\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 32482 / 36865\n",
      "training loss = 0.4105\n",
      "training accuracy = 0.8811\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 32545 / 36865\n",
      "training loss = 0.3979\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 32548 / 36865\n",
      "training loss = 0.3991\n",
      "training accuracy = 0.8829\n",
      "num_test_corrects / test_total_examples = 7820 / 9217\n",
      "testing accuracy = 0.8484\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 32542 / 36865\n",
      "training loss = 0.3959\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 32562 / 36865\n",
      "training loss = 0.3975\n",
      "training accuracy = 0.8833\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 32535 / 36865\n",
      "training loss = 0.3882\n",
      "training accuracy = 0.8825\n",
      "num_test_corrects / test_total_examples = 7833 / 9217\n",
      "testing accuracy = 0.8498\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 32504 / 36865\n",
      "training loss = 0.3954\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 32544 / 36865\n",
      "training loss = 0.4001\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 32596 / 36865\n",
      "training loss = 0.4047\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 7994 / 9217\n",
      "testing accuracy = 0.8673\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 32577 / 36865\n",
      "training loss = 0.3909\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "found best test accuracy at epoch 473\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 32530 / 36865\n",
      "training loss = 0.4063\n",
      "training accuracy = 0.8824\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 32530 / 36865\n",
      "training loss = 0.3990\n",
      "training accuracy = 0.8824\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 32556 / 36865\n",
      "training loss = 0.4015\n",
      "training accuracy = 0.8831\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 32570 / 36865\n",
      "training loss = 0.4026\n",
      "training accuracy = 0.8835\n",
      "num_test_corrects / test_total_examples = 7871 / 9217\n",
      "testing accuracy = 0.8540\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 32600 / 36865\n",
      "training loss = 0.3958\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 32541 / 36865\n",
      "training loss = 0.3989\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 7745 / 9217\n",
      "testing accuracy = 0.8403\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 32562 / 36865\n",
      "training loss = 0.4046\n",
      "training accuracy = 0.8833\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 32529 / 36865\n",
      "training loss = 0.3973\n",
      "training accuracy = 0.8824\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 32576 / 36865\n",
      "training loss = 0.3966\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 7676 / 9217\n",
      "testing accuracy = 0.8328\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 32551 / 36865\n",
      "training loss = 0.3975\n",
      "training accuracy = 0.8830\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 32535 / 36865\n",
      "training loss = 0.3876\n",
      "training accuracy = 0.8825\n",
      "num_test_corrects / test_total_examples = 7887 / 9217\n",
      "testing accuracy = 0.8557\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 32536 / 36865\n",
      "training loss = 0.4012\n",
      "training accuracy = 0.8826\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 32586 / 36865\n",
      "training loss = 0.3982\n",
      "training accuracy = 0.8839\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 32613 / 36865\n",
      "training loss = 0.3908\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 32614 / 36865\n",
      "training loss = 0.3909\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 32545 / 36865\n",
      "training loss = 0.4066\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 32625 / 36865\n",
      "training loss = 0.3912\n",
      "training accuracy = 0.8850\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 32544 / 36865\n",
      "training loss = 0.4028\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7976 / 9217\n",
      "testing accuracy = 0.8654\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 32522 / 36865\n",
      "training loss = 0.4048\n",
      "training accuracy = 0.8822\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 32566 / 36865\n",
      "training loss = 0.3939\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 32615 / 36865\n",
      "training loss = 0.3938\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 7987 / 9217\n",
      "testing accuracy = 0.8666\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 32594 / 36865\n",
      "training loss = 0.3915\n",
      "training accuracy = 0.8841\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 32498 / 36865\n",
      "training loss = 0.4127\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 8021 / 9217\n",
      "testing accuracy = 0.8702\n",
      "found best test accuracy at epoch 496\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 32573 / 36865\n",
      "training loss = 0.3917\n",
      "training accuracy = 0.8836\n",
      "num_test_corrects / test_total_examples = 7844 / 9217\n",
      "testing accuracy = 0.8510\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 32559 / 36865\n",
      "training loss = 0.3914\n",
      "training accuracy = 0.8832\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 32590 / 36865\n",
      "training loss = 0.3963\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 32615 / 36865\n",
      "training loss = 0.3953\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 501 ---------\n",
      "num_corrects / total_examples = 32577 / 36865\n",
      "training loss = 0.3982\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 502 ---------\n",
      "num_corrects / total_examples = 32631 / 36865\n",
      "training loss = 0.3850\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 7847 / 9217\n",
      "testing accuracy = 0.8514\n",
      "--------- epoch: 503 ---------\n",
      "num_corrects / total_examples = 32559 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8832\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 504 ---------\n",
      "num_corrects / total_examples = 32617 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 505 ---------\n",
      "num_corrects / total_examples = 32566 / 36865\n",
      "training loss = 0.3910\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 7915 / 9217\n",
      "testing accuracy = 0.8587\n",
      "--------- epoch: 506 ---------\n",
      "num_corrects / total_examples = 32568 / 36865\n",
      "training loss = 0.4006\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 507 ---------\n",
      "num_corrects / total_examples = 32588 / 36865\n",
      "training loss = 0.4016\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 7793 / 9217\n",
      "testing accuracy = 0.8455\n",
      "--------- epoch: 508 ---------\n",
      "num_corrects / total_examples = 32623 / 36865\n",
      "training loss = 0.3906\n",
      "training accuracy = 0.8849\n",
      "num_test_corrects / test_total_examples = 8004 / 9217\n",
      "testing accuracy = 0.8684\n",
      "--------- epoch: 509 ---------\n",
      "num_corrects / total_examples = 32600 / 36865\n",
      "training loss = 0.3999\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 510 ---------\n",
      "num_corrects / total_examples = 32622 / 36865\n",
      "training loss = 0.3980\n",
      "training accuracy = 0.8849\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 511 ---------\n",
      "num_corrects / total_examples = 32663 / 36865\n",
      "training loss = 0.3969\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 512 ---------\n",
      "num_corrects / total_examples = 32593 / 36865\n",
      "training loss = 0.3930\n",
      "training accuracy = 0.8841\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 513 ---------\n",
      "num_corrects / total_examples = 32567 / 36865\n",
      "training loss = 0.4052\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 7942 / 9217\n",
      "testing accuracy = 0.8617\n",
      "--------- epoch: 514 ---------\n",
      "num_corrects / total_examples = 32563 / 36865\n",
      "training loss = 0.3936\n",
      "training accuracy = 0.8833\n",
      "num_test_corrects / test_total_examples = 7861 / 9217\n",
      "testing accuracy = 0.8529\n",
      "--------- epoch: 515 ---------\n",
      "num_corrects / total_examples = 32600 / 36865\n",
      "training loss = 0.3916\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 516 ---------\n",
      "num_corrects / total_examples = 32653 / 36865\n",
      "training loss = 0.3944\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 517 ---------\n",
      "num_corrects / total_examples = 32662 / 36865\n",
      "training loss = 0.3876\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 8009 / 9217\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 518 ---------\n",
      "num_corrects / total_examples = 32663 / 36865\n",
      "training loss = 0.3922\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7944 / 9217\n",
      "testing accuracy = 0.8619\n",
      "--------- epoch: 519 ---------\n",
      "num_corrects / total_examples = 32629 / 36865\n",
      "training loss = 0.3900\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 520 ---------\n",
      "num_corrects / total_examples = 32617 / 36865\n",
      "training loss = 0.4018\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7891 / 9217\n",
      "testing accuracy = 0.8561\n",
      "--------- epoch: 521 ---------\n",
      "num_corrects / total_examples = 32670 / 36865\n",
      "training loss = 0.3935\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 522 ---------\n",
      "num_corrects / total_examples = 32608 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 523 ---------\n",
      "num_corrects / total_examples = 32651 / 36865\n",
      "training loss = 0.3938\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 524 ---------\n",
      "num_corrects / total_examples = 32612 / 36865\n",
      "training loss = 0.3939\n",
      "training accuracy = 0.8846\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 525 ---------\n",
      "num_corrects / total_examples = 32617 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 526 ---------\n",
      "num_corrects / total_examples = 32623 / 36865\n",
      "training loss = 0.4042\n",
      "training accuracy = 0.8849\n",
      "num_test_corrects / test_total_examples = 7987 / 9217\n",
      "testing accuracy = 0.8666\n",
      "--------- epoch: 527 ---------\n",
      "num_corrects / total_examples = 32634 / 36865\n",
      "training loss = 0.3923\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 528 ---------\n",
      "num_corrects / total_examples = 32585 / 36865\n",
      "training loss = 0.4046\n",
      "training accuracy = 0.8839\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 529 ---------\n",
      "num_corrects / total_examples = 32595 / 36865\n",
      "training loss = 0.3949\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 530 ---------\n",
      "num_corrects / total_examples = 32634 / 36865\n",
      "training loss = 0.3989\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 531 ---------\n",
      "num_corrects / total_examples = 32673 / 36865\n",
      "training loss = 0.3951\n",
      "training accuracy = 0.8863\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 532 ---------\n",
      "num_corrects / total_examples = 32661 / 36865\n",
      "training loss = 0.3869\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7988 / 9217\n",
      "testing accuracy = 0.8667\n",
      "--------- epoch: 533 ---------\n",
      "num_corrects / total_examples = 32614 / 36865\n",
      "training loss = 0.3901\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 7993 / 9217\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 534 ---------\n",
      "num_corrects / total_examples = 32618 / 36865\n",
      "training loss = 0.3949\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 535 ---------\n",
      "num_corrects / total_examples = 32698 / 36865\n",
      "training loss = 0.3915\n",
      "training accuracy = 0.8870\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 536 ---------\n",
      "num_corrects / total_examples = 32624 / 36865\n",
      "training loss = 0.3959\n",
      "training accuracy = 0.8850\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 537 ---------\n",
      "num_corrects / total_examples = 32640 / 36865\n",
      "training loss = 0.3933\n",
      "training accuracy = 0.8854\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 538 ---------\n",
      "num_corrects / total_examples = 32624 / 36865\n",
      "training loss = 0.3905\n",
      "training accuracy = 0.8850\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 539 ---------\n",
      "num_corrects / total_examples = 32603 / 36865\n",
      "training loss = 0.3967\n",
      "training accuracy = 0.8844\n",
      "num_test_corrects / test_total_examples = 7901 / 9217\n",
      "testing accuracy = 0.8572\n",
      "--------- epoch: 540 ---------\n",
      "num_corrects / total_examples = 32606 / 36865\n",
      "training loss = 0.3981\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 541 ---------\n",
      "num_corrects / total_examples = 32686 / 36865\n",
      "training loss = 0.3949\n",
      "training accuracy = 0.8866\n",
      "num_test_corrects / test_total_examples = 7537 / 9217\n",
      "testing accuracy = 0.8177\n",
      "--------- epoch: 542 ---------\n",
      "num_corrects / total_examples = 32639 / 36865\n",
      "training loss = 0.3857\n",
      "training accuracy = 0.8854\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 543 ---------\n",
      "num_corrects / total_examples = 32660 / 36865\n",
      "training loss = 0.3974\n",
      "training accuracy = 0.8859\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 544 ---------\n",
      "num_corrects / total_examples = 32619 / 36865\n",
      "training loss = 0.4081\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "--------- epoch: 545 ---------\n",
      "num_corrects / total_examples = 32683 / 36865\n",
      "training loss = 0.4038\n",
      "training accuracy = 0.8866\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 546 ---------\n",
      "num_corrects / total_examples = 32664 / 36865\n",
      "training loss = 0.3905\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7996 / 9217\n",
      "testing accuracy = 0.8675\n",
      "--------- epoch: 547 ---------\n",
      "num_corrects / total_examples = 32655 / 36865\n",
      "training loss = 0.4026\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 7757 / 9217\n",
      "testing accuracy = 0.8416\n",
      "--------- epoch: 548 ---------\n",
      "num_corrects / total_examples = 32652 / 36865\n",
      "training loss = 0.4001\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "--------- epoch: 549 ---------\n",
      "num_corrects / total_examples = 32652 / 36865\n",
      "training loss = 0.3912\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 550 ---------\n",
      "num_corrects / total_examples = 32701 / 36865\n",
      "training loss = 0.3921\n",
      "training accuracy = 0.8870\n",
      "num_test_corrects / test_total_examples = 7952 / 9217\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 551 ---------\n",
      "num_corrects / total_examples = 32664 / 36865\n",
      "training loss = 0.3972\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 552 ---------\n",
      "num_corrects / total_examples = 32634 / 36865\n",
      "training loss = 0.4065\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 553 ---------\n",
      "num_corrects / total_examples = 32617 / 36865\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 554 ---------\n",
      "num_corrects / total_examples = 32714 / 36865\n",
      "training loss = 0.3885\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 7996 / 9217\n",
      "testing accuracy = 0.8675\n",
      "--------- epoch: 555 ---------\n",
      "num_corrects / total_examples = 32612 / 36865\n",
      "training loss = 0.4101\n",
      "training accuracy = 0.8846\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 556 ---------\n",
      "num_corrects / total_examples = 32665 / 36865\n",
      "training loss = 0.3897\n",
      "training accuracy = 0.8861\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 557 ---------\n",
      "num_corrects / total_examples = 32675 / 36865\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8863\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 558 ---------\n",
      "num_corrects / total_examples = 32545 / 36865\n",
      "training loss = 0.4013\n",
      "training accuracy = 0.8828\n",
      "num_test_corrects / test_total_examples = 7896 / 9217\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 559 ---------\n",
      "num_corrects / total_examples = 32639 / 36865\n",
      "training loss = 0.3917\n",
      "training accuracy = 0.8854\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 560 ---------\n",
      "num_corrects / total_examples = 32609 / 36865\n",
      "training loss = 0.4092\n",
      "training accuracy = 0.8846\n",
      "num_test_corrects / test_total_examples = 7996 / 9217\n",
      "testing accuracy = 0.8675\n",
      "--------- epoch: 561 ---------\n",
      "num_corrects / total_examples = 32664 / 36865\n",
      "training loss = 0.3955\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 562 ---------\n",
      "num_corrects / total_examples = 32766 / 36865\n",
      "training loss = 0.3806\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7757 / 9217\n",
      "testing accuracy = 0.8416\n",
      "--------- epoch: 563 ---------\n",
      "num_corrects / total_examples = 32633 / 36865\n",
      "training loss = 0.4045\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 564 ---------\n",
      "num_corrects / total_examples = 32725 / 36865\n",
      "training loss = 0.3941\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 565 ---------\n",
      "num_corrects / total_examples = 32693 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8868\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 566 ---------\n",
      "num_corrects / total_examples = 32598 / 36865\n",
      "training loss = 0.4098\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 567 ---------\n",
      "num_corrects / total_examples = 32687 / 36865\n",
      "training loss = 0.3939\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 568 ---------\n",
      "num_corrects / total_examples = 32792 / 36865\n",
      "training loss = 0.3868\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7411 / 9217\n",
      "testing accuracy = 0.8041\n",
      "--------- epoch: 569 ---------\n",
      "num_corrects / total_examples = 32635 / 36865\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8853\n",
      "num_test_corrects / test_total_examples = 7924 / 9217\n",
      "testing accuracy = 0.8597\n",
      "--------- epoch: 570 ---------\n",
      "num_corrects / total_examples = 32687 / 36865\n",
      "training loss = 0.3920\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7794 / 9217\n",
      "testing accuracy = 0.8456\n",
      "--------- epoch: 571 ---------\n",
      "num_corrects / total_examples = 32675 / 36865\n",
      "training loss = 0.3902\n",
      "training accuracy = 0.8863\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 572 ---------\n",
      "num_corrects / total_examples = 32688 / 36865\n",
      "training loss = 0.3969\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7696 / 9217\n",
      "testing accuracy = 0.8350\n",
      "--------- epoch: 573 ---------\n",
      "num_corrects / total_examples = 32649 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 574 ---------\n",
      "num_corrects / total_examples = 32689 / 36865\n",
      "training loss = 0.3878\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 575 ---------\n",
      "num_corrects / total_examples = 32655 / 36865\n",
      "training loss = 0.4011\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 576 ---------\n",
      "num_corrects / total_examples = 32748 / 36865\n",
      "training loss = 0.3882\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 7942 / 9217\n",
      "testing accuracy = 0.8617\n",
      "--------- epoch: 577 ---------\n",
      "num_corrects / total_examples = 32677 / 36865\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 7934 / 9217\n",
      "testing accuracy = 0.8608\n",
      "--------- epoch: 578 ---------\n",
      "num_corrects / total_examples = 32722 / 36865\n",
      "training loss = 0.3971\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 579 ---------\n",
      "num_corrects / total_examples = 32690 / 36865\n",
      "training loss = 0.4037\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 580 ---------\n",
      "num_corrects / total_examples = 32703 / 36865\n",
      "training loss = 0.3968\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 7678 / 9217\n",
      "testing accuracy = 0.8330\n",
      "--------- epoch: 581 ---------\n",
      "num_corrects / total_examples = 32655 / 36865\n",
      "training loss = 0.3904\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 582 ---------\n",
      "num_corrects / total_examples = 32663 / 36865\n",
      "training loss = 0.3998\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 583 ---------\n",
      "num_corrects / total_examples = 32671 / 36865\n",
      "training loss = 0.4030\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 584 ---------\n",
      "num_corrects / total_examples = 32766 / 36865\n",
      "training loss = 0.3906\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7940 / 9217\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 585 ---------\n",
      "num_corrects / total_examples = 32689 / 36865\n",
      "training loss = 0.3955\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 586 ---------\n",
      "num_corrects / total_examples = 32740 / 36865\n",
      "training loss = 0.3978\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 587 ---------\n",
      "num_corrects / total_examples = 32690 / 36865\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 588 ---------\n",
      "num_corrects / total_examples = 32691 / 36865\n",
      "training loss = 0.3934\n",
      "training accuracy = 0.8868\n",
      "num_test_corrects / test_total_examples = 7976 / 9217\n",
      "testing accuracy = 0.8654\n",
      "--------- epoch: 589 ---------\n",
      "num_corrects / total_examples = 32783 / 36865\n",
      "training loss = 0.3938\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 590 ---------\n",
      "num_corrects / total_examples = 32718 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8875\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 591 ---------\n",
      "num_corrects / total_examples = 32662 / 36865\n",
      "training loss = 0.4012\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 592 ---------\n",
      "num_corrects / total_examples = 32714 / 36865\n",
      "training loss = 0.3855\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 593 ---------\n",
      "num_corrects / total_examples = 32713 / 36865\n",
      "training loss = 0.3935\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 7873 / 9217\n",
      "testing accuracy = 0.8542\n",
      "--------- epoch: 594 ---------\n",
      "num_corrects / total_examples = 32749 / 36865\n",
      "training loss = 0.3915\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 595 ---------\n",
      "num_corrects / total_examples = 32658 / 36865\n",
      "training loss = 0.4020\n",
      "training accuracy = 0.8859\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 596 ---------\n",
      "num_corrects / total_examples = 32696 / 36865\n",
      "training loss = 0.3904\n",
      "training accuracy = 0.8869\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 597 ---------\n",
      "num_corrects / total_examples = 32687 / 36865\n",
      "training loss = 0.3886\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7764 / 9217\n",
      "testing accuracy = 0.8424\n",
      "--------- epoch: 598 ---------\n",
      "num_corrects / total_examples = 32721 / 36865\n",
      "training loss = 0.3945\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7918 / 9217\n",
      "testing accuracy = 0.8591\n",
      "--------- epoch: 599 ---------\n",
      "num_corrects / total_examples = 32709 / 36865\n",
      "training loss = 0.3962\n",
      "training accuracy = 0.8873\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 600 ---------\n",
      "num_corrects / total_examples = 32725 / 36865\n",
      "training loss = 0.4045\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 601 ---------\n",
      "num_corrects / total_examples = 32732 / 36865\n",
      "training loss = 0.3881\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 7940 / 9217\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 602 ---------\n",
      "num_corrects / total_examples = 32698 / 36865\n",
      "training loss = 0.3979\n",
      "training accuracy = 0.8870\n",
      "num_test_corrects / test_total_examples = 7867 / 9217\n",
      "testing accuracy = 0.8535\n",
      "--------- epoch: 603 ---------\n",
      "num_corrects / total_examples = 32642 / 36865\n",
      "training loss = 0.3968\n",
      "training accuracy = 0.8854\n",
      "num_test_corrects / test_total_examples = 7852 / 9217\n",
      "testing accuracy = 0.8519\n",
      "--------- epoch: 604 ---------\n",
      "num_corrects / total_examples = 32664 / 36865\n",
      "training loss = 0.3904\n",
      "training accuracy = 0.8860\n",
      "num_test_corrects / test_total_examples = 7940 / 9217\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 605 ---------\n",
      "num_corrects / total_examples = 32693 / 36865\n",
      "training loss = 0.4083\n",
      "training accuracy = 0.8868\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 606 ---------\n",
      "num_corrects / total_examples = 32723 / 36865\n",
      "training loss = 0.3872\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 607 ---------\n",
      "num_corrects / total_examples = 32754 / 36865\n",
      "training loss = 0.4026\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 7890 / 9217\n",
      "testing accuracy = 0.8560\n",
      "--------- epoch: 608 ---------\n",
      "num_corrects / total_examples = 32711 / 36865\n",
      "training loss = 0.3962\n",
      "training accuracy = 0.8873\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 609 ---------\n",
      "num_corrects / total_examples = 32708 / 36865\n",
      "training loss = 0.4008\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 610 ---------\n",
      "num_corrects / total_examples = 32704 / 36865\n",
      "training loss = 0.4003\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 611 ---------\n",
      "num_corrects / total_examples = 32687 / 36865\n",
      "training loss = 0.3958\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 612 ---------\n",
      "num_corrects / total_examples = 32746 / 36865\n",
      "training loss = 0.3960\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 7456 / 9217\n",
      "testing accuracy = 0.8089\n",
      "--------- epoch: 613 ---------\n",
      "num_corrects / total_examples = 32722 / 36865\n",
      "training loss = 0.3869\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 614 ---------\n",
      "num_corrects / total_examples = 32793 / 36865\n",
      "training loss = 0.3886\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 615 ---------\n",
      "num_corrects / total_examples = 32820 / 36865\n",
      "training loss = 0.3894\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 616 ---------\n",
      "num_corrects / total_examples = 32648 / 36865\n",
      "training loss = 0.4013\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 617 ---------\n",
      "num_corrects / total_examples = 32745 / 36865\n",
      "training loss = 0.3845\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 618 ---------\n",
      "num_corrects / total_examples = 32761 / 36865\n",
      "training loss = 0.3867\n",
      "training accuracy = 0.8887\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 619 ---------\n",
      "num_corrects / total_examples = 32738 / 36865\n",
      "training loss = 0.3963\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 620 ---------\n",
      "num_corrects / total_examples = 32756 / 36865\n",
      "training loss = 0.3975\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 621 ---------\n",
      "num_corrects / total_examples = 32771 / 36865\n",
      "training loss = 0.3995\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 622 ---------\n",
      "num_corrects / total_examples = 32774 / 36865\n",
      "training loss = 0.3884\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 7865 / 9217\n",
      "testing accuracy = 0.8533\n",
      "--------- epoch: 623 ---------\n",
      "num_corrects / total_examples = 32760 / 36865\n",
      "training loss = 0.3982\n",
      "training accuracy = 0.8886\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 624 ---------\n",
      "num_corrects / total_examples = 32740 / 36865\n",
      "training loss = 0.3893\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 7993 / 9217\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 625 ---------\n",
      "num_corrects / total_examples = 32771 / 36865\n",
      "training loss = 0.3930\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 626 ---------\n",
      "num_corrects / total_examples = 32732 / 36865\n",
      "training loss = 0.3961\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 627 ---------\n",
      "num_corrects / total_examples = 32744 / 36865\n",
      "training loss = 0.3900\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 7507 / 9217\n",
      "testing accuracy = 0.8145\n",
      "--------- epoch: 628 ---------\n",
      "num_corrects / total_examples = 32723 / 36865\n",
      "training loss = 0.3975\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 629 ---------\n",
      "num_corrects / total_examples = 32677 / 36865\n",
      "training loss = 0.3956\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 630 ---------\n",
      "num_corrects / total_examples = 32767 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 631 ---------\n",
      "num_corrects / total_examples = 32764 / 36865\n",
      "training loss = 0.4003\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 632 ---------\n",
      "num_corrects / total_examples = 32798 / 36865\n",
      "training loss = 0.3809\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 7895 / 9217\n",
      "testing accuracy = 0.8566\n",
      "--------- epoch: 633 ---------\n",
      "num_corrects / total_examples = 32775 / 36865\n",
      "training loss = 0.4020\n",
      "training accuracy = 0.8891\n",
      "num_test_corrects / test_total_examples = 7866 / 9217\n",
      "testing accuracy = 0.8534\n",
      "--------- epoch: 634 ---------\n",
      "num_corrects / total_examples = 32737 / 36865\n",
      "training loss = 0.3854\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 7888 / 9217\n",
      "testing accuracy = 0.8558\n",
      "--------- epoch: 635 ---------\n",
      "num_corrects / total_examples = 32746 / 36865\n",
      "training loss = 0.4072\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 7952 / 9217\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 636 ---------\n",
      "num_corrects / total_examples = 32798 / 36865\n",
      "training loss = 0.3875\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 7916 / 9217\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 637 ---------\n",
      "num_corrects / total_examples = 32737 / 36865\n",
      "training loss = 0.3855\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 638 ---------\n",
      "num_corrects / total_examples = 32768 / 36865\n",
      "training loss = 0.3927\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 7907 / 9217\n",
      "testing accuracy = 0.8579\n",
      "--------- epoch: 639 ---------\n",
      "num_corrects / total_examples = 32736 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 640 ---------\n",
      "num_corrects / total_examples = 32785 / 36865\n",
      "training loss = 0.3805\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 7910 / 9217\n",
      "testing accuracy = 0.8582\n",
      "--------- epoch: 641 ---------\n",
      "num_corrects / total_examples = 32701 / 36865\n",
      "training loss = 0.3976\n",
      "training accuracy = 0.8870\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 642 ---------\n",
      "num_corrects / total_examples = 32821 / 36865\n",
      "training loss = 0.3958\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 643 ---------\n",
      "num_corrects / total_examples = 32812 / 36865\n",
      "training loss = 0.3982\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7862 / 9217\n",
      "testing accuracy = 0.8530\n",
      "--------- epoch: 644 ---------\n",
      "num_corrects / total_examples = 32732 / 36865\n",
      "training loss = 0.3897\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 645 ---------\n",
      "num_corrects / total_examples = 32802 / 36865\n",
      "training loss = 0.3903\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 7609 / 9217\n",
      "testing accuracy = 0.8255\n",
      "--------- epoch: 646 ---------\n",
      "num_corrects / total_examples = 32729 / 36865\n",
      "training loss = 0.4022\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 7885 / 9217\n",
      "testing accuracy = 0.8555\n",
      "--------- epoch: 647 ---------\n",
      "num_corrects / total_examples = 32825 / 36865\n",
      "training loss = 0.3953\n",
      "training accuracy = 0.8904\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 648 ---------\n",
      "num_corrects / total_examples = 32756 / 36865\n",
      "training loss = 0.3942\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 649 ---------\n",
      "num_corrects / total_examples = 32700 / 36865\n",
      "training loss = 0.4022\n",
      "training accuracy = 0.8870\n",
      "num_test_corrects / test_total_examples = 7899 / 9217\n",
      "testing accuracy = 0.8570\n",
      "--------- epoch: 650 ---------\n",
      "num_corrects / total_examples = 32795 / 36865\n",
      "training loss = 0.3928\n",
      "training accuracy = 0.8896\n",
      "num_test_corrects / test_total_examples = 7945 / 9217\n",
      "testing accuracy = 0.8620\n",
      "--------- epoch: 651 ---------\n",
      "num_corrects / total_examples = 32785 / 36865\n",
      "training loss = 0.3860\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 7908 / 9217\n",
      "testing accuracy = 0.8580\n",
      "--------- epoch: 652 ---------\n",
      "num_corrects / total_examples = 32793 / 36865\n",
      "training loss = 0.3895\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7885 / 9217\n",
      "testing accuracy = 0.8555\n",
      "--------- epoch: 653 ---------\n",
      "num_corrects / total_examples = 32766 / 36865\n",
      "training loss = 0.3995\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7764 / 9217\n",
      "testing accuracy = 0.8424\n",
      "--------- epoch: 654 ---------\n",
      "num_corrects / total_examples = 32785 / 36865\n",
      "training loss = 0.3902\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 7987 / 9217\n",
      "testing accuracy = 0.8666\n",
      "--------- epoch: 655 ---------\n",
      "num_corrects / total_examples = 32750 / 36865\n",
      "training loss = 0.3893\n",
      "training accuracy = 0.8884\n",
      "num_test_corrects / test_total_examples = 7981 / 9217\n",
      "testing accuracy = 0.8659\n",
      "--------- epoch: 656 ---------\n",
      "num_corrects / total_examples = 32713 / 36865\n",
      "training loss = 0.3877\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 7936 / 9217\n",
      "testing accuracy = 0.8610\n",
      "--------- epoch: 657 ---------\n",
      "num_corrects / total_examples = 32830 / 36865\n",
      "training loss = 0.3895\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 7716 / 9217\n",
      "testing accuracy = 0.8371\n",
      "--------- epoch: 658 ---------\n",
      "num_corrects / total_examples = 32840 / 36865\n",
      "training loss = 0.3866\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 659 ---------\n",
      "num_corrects / total_examples = 32744 / 36865\n",
      "training loss = 0.3930\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 7952 / 9217\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 660 ---------\n",
      "num_corrects / total_examples = 32720 / 36865\n",
      "training loss = 0.3938\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 661 ---------\n",
      "num_corrects / total_examples = 32812 / 36865\n",
      "training loss = 0.4001\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7857 / 9217\n",
      "testing accuracy = 0.8524\n",
      "--------- epoch: 662 ---------\n",
      "num_corrects / total_examples = 32798 / 36865\n",
      "training loss = 0.3916\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 663 ---------\n",
      "num_corrects / total_examples = 32775 / 36865\n",
      "training loss = 0.3871\n",
      "training accuracy = 0.8891\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 664 ---------\n",
      "num_corrects / total_examples = 32812 / 36865\n",
      "training loss = 0.3859\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7934 / 9217\n",
      "testing accuracy = 0.8608\n",
      "--------- epoch: 665 ---------\n",
      "num_corrects / total_examples = 32818 / 36865\n",
      "training loss = 0.3834\n",
      "training accuracy = 0.8902\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 666 ---------\n",
      "num_corrects / total_examples = 32791 / 36865\n",
      "training loss = 0.3959\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7850 / 9217\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 667 ---------\n",
      "num_corrects / total_examples = 32793 / 36865\n",
      "training loss = 0.3931\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7930 / 9217\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 668 ---------\n",
      "num_corrects / total_examples = 32739 / 36865\n",
      "training loss = 0.3915\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 7996 / 9217\n",
      "testing accuracy = 0.8675\n",
      "--------- epoch: 669 ---------\n",
      "num_corrects / total_examples = 32705 / 36865\n",
      "training loss = 0.4053\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 670 ---------\n",
      "num_corrects / total_examples = 32813 / 36865\n",
      "training loss = 0.3881\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 671 ---------\n",
      "num_corrects / total_examples = 32780 / 36865\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8892\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 672 ---------\n",
      "num_corrects / total_examples = 32765 / 36865\n",
      "training loss = 0.3904\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 673 ---------\n",
      "num_corrects / total_examples = 32810 / 36865\n",
      "training loss = 0.3909\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 674 ---------\n",
      "num_corrects / total_examples = 32815 / 36865\n",
      "training loss = 0.3883\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7491 / 9217\n",
      "testing accuracy = 0.8127\n",
      "--------- epoch: 675 ---------\n",
      "num_corrects / total_examples = 32710 / 36865\n",
      "training loss = 0.4024\n",
      "training accuracy = 0.8873\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 676 ---------\n",
      "num_corrects / total_examples = 32808 / 36865\n",
      "training loss = 0.3866\n",
      "training accuracy = 0.8899\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 677 ---------\n",
      "num_corrects / total_examples = 32804 / 36865\n",
      "training loss = 0.3849\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 7976 / 9217\n",
      "testing accuracy = 0.8654\n",
      "--------- epoch: 678 ---------\n",
      "num_corrects / total_examples = 32746 / 36865\n",
      "training loss = 0.4030\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 679 ---------\n",
      "num_corrects / total_examples = 32761 / 36865\n",
      "training loss = 0.3897\n",
      "training accuracy = 0.8887\n",
      "num_test_corrects / test_total_examples = 7442 / 9217\n",
      "testing accuracy = 0.8074\n",
      "--------- epoch: 680 ---------\n",
      "num_corrects / total_examples = 32761 / 36865\n",
      "training loss = 0.3913\n",
      "training accuracy = 0.8887\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 681 ---------\n",
      "num_corrects / total_examples = 32820 / 36865\n",
      "training loss = 0.3851\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 682 ---------\n",
      "num_corrects / total_examples = 32804 / 36865\n",
      "training loss = 0.3940\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 683 ---------\n",
      "num_corrects / total_examples = 32857 / 36865\n",
      "training loss = 0.3935\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 684 ---------\n",
      "num_corrects / total_examples = 32777 / 36865\n",
      "training loss = 0.4057\n",
      "training accuracy = 0.8891\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 685 ---------\n",
      "num_corrects / total_examples = 32807 / 36865\n",
      "training loss = 0.3813\n",
      "training accuracy = 0.8899\n",
      "num_test_corrects / test_total_examples = 7588 / 9217\n",
      "testing accuracy = 0.8233\n",
      "--------- epoch: 686 ---------\n",
      "num_corrects / total_examples = 32812 / 36865\n",
      "training loss = 0.3889\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 687 ---------\n",
      "num_corrects / total_examples = 32848 / 36865\n",
      "training loss = 0.3952\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 688 ---------\n",
      "num_corrects / total_examples = 32773 / 36865\n",
      "training loss = 0.4026\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 7919 / 9217\n",
      "testing accuracy = 0.8592\n",
      "--------- epoch: 689 ---------\n",
      "num_corrects / total_examples = 32869 / 36865\n",
      "training loss = 0.3853\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 690 ---------\n",
      "num_corrects / total_examples = 32801 / 36865\n",
      "training loss = 0.4001\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 691 ---------\n",
      "num_corrects / total_examples = 32848 / 36865\n",
      "training loss = 0.3895\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 692 ---------\n",
      "num_corrects / total_examples = 32832 / 36865\n",
      "training loss = 0.4018\n",
      "training accuracy = 0.8906\n",
      "num_test_corrects / test_total_examples = 7715 / 9217\n",
      "testing accuracy = 0.8370\n",
      "--------- epoch: 693 ---------\n",
      "num_corrects / total_examples = 32802 / 36865\n",
      "training loss = 0.3889\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 694 ---------\n",
      "num_corrects / total_examples = 32788 / 36865\n",
      "training loss = 0.3941\n",
      "training accuracy = 0.8894\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 695 ---------\n",
      "num_corrects / total_examples = 32862 / 36865\n",
      "training loss = 0.3796\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 696 ---------\n",
      "num_corrects / total_examples = 32842 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 7762 / 9217\n",
      "testing accuracy = 0.8421\n",
      "--------- epoch: 697 ---------\n",
      "num_corrects / total_examples = 32814 / 36865\n",
      "training loss = 0.3905\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7952 / 9217\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 698 ---------\n",
      "num_corrects / total_examples = 32800 / 36865\n",
      "training loss = 0.3876\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 699 ---------\n",
      "num_corrects / total_examples = 32843 / 36865\n",
      "training loss = 0.3858\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 7974 / 9217\n",
      "testing accuracy = 0.8651\n",
      "--------- epoch: 700 ---------\n",
      "num_corrects / total_examples = 32814 / 36865\n",
      "training loss = 0.3936\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 8016 / 9217\n",
      "testing accuracy = 0.8697\n",
      "--------- epoch: 701 ---------\n",
      "num_corrects / total_examples = 32892 / 36865\n",
      "training loss = 0.3893\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "--------- epoch: 702 ---------\n",
      "num_corrects / total_examples = 32880 / 36865\n",
      "training loss = 0.3820\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 703 ---------\n",
      "num_corrects / total_examples = 32853 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 704 ---------\n",
      "num_corrects / total_examples = 32858 / 36865\n",
      "training loss = 0.3876\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 7790 / 9217\n",
      "testing accuracy = 0.8452\n",
      "--------- epoch: 705 ---------\n",
      "num_corrects / total_examples = 32820 / 36865\n",
      "training loss = 0.3903\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 706 ---------\n",
      "num_corrects / total_examples = 32862 / 36865\n",
      "training loss = 0.3856\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7722 / 9217\n",
      "testing accuracy = 0.8378\n",
      "--------- epoch: 707 ---------\n",
      "num_corrects / total_examples = 32815 / 36865\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 708 ---------\n",
      "num_corrects / total_examples = 32823 / 36865\n",
      "training loss = 0.3865\n",
      "training accuracy = 0.8904\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 709 ---------\n",
      "num_corrects / total_examples = 32800 / 36865\n",
      "training loss = 0.3875\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 710 ---------\n",
      "num_corrects / total_examples = 32860 / 36865\n",
      "training loss = 0.3831\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7860 / 9217\n",
      "testing accuracy = 0.8528\n",
      "--------- epoch: 711 ---------\n",
      "num_corrects / total_examples = 32795 / 36865\n",
      "training loss = 0.3831\n",
      "training accuracy = 0.8896\n",
      "num_test_corrects / test_total_examples = 7912 / 9217\n",
      "testing accuracy = 0.8584\n",
      "--------- epoch: 712 ---------\n",
      "num_corrects / total_examples = 32878 / 36865\n",
      "training loss = 0.3878\n",
      "training accuracy = 0.8918\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 713 ---------\n",
      "num_corrects / total_examples = 32774 / 36865\n",
      "training loss = 0.3898\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 714 ---------\n",
      "num_corrects / total_examples = 32783 / 36865\n",
      "training loss = 0.3902\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 7870 / 9217\n",
      "testing accuracy = 0.8539\n",
      "--------- epoch: 715 ---------\n",
      "num_corrects / total_examples = 32853 / 36865\n",
      "training loss = 0.3850\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 716 ---------\n",
      "num_corrects / total_examples = 32841 / 36865\n",
      "training loss = 0.3824\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 717 ---------\n",
      "num_corrects / total_examples = 32750 / 36865\n",
      "training loss = 0.3985\n",
      "training accuracy = 0.8884\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 718 ---------\n",
      "num_corrects / total_examples = 32931 / 36865\n",
      "training loss = 0.3845\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 719 ---------\n",
      "num_corrects / total_examples = 32861 / 36865\n",
      "training loss = 0.3784\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 720 ---------\n",
      "num_corrects / total_examples = 32882 / 36865\n",
      "training loss = 0.3925\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 8004 / 9217\n",
      "testing accuracy = 0.8684\n",
      "--------- epoch: 721 ---------\n",
      "num_corrects / total_examples = 32852 / 36865\n",
      "training loss = 0.3813\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 722 ---------\n",
      "num_corrects / total_examples = 32874 / 36865\n",
      "training loss = 0.3818\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 723 ---------\n",
      "num_corrects / total_examples = 32887 / 36865\n",
      "training loss = 0.3975\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7872 / 9217\n",
      "testing accuracy = 0.8541\n",
      "--------- epoch: 724 ---------\n",
      "num_corrects / total_examples = 32813 / 36865\n",
      "training loss = 0.3903\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7853 / 9217\n",
      "testing accuracy = 0.8520\n",
      "--------- epoch: 725 ---------\n",
      "num_corrects / total_examples = 32846 / 36865\n",
      "training loss = 0.3825\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 726 ---------\n",
      "num_corrects / total_examples = 32831 / 36865\n",
      "training loss = 0.3889\n",
      "training accuracy = 0.8906\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 727 ---------\n",
      "num_corrects / total_examples = 32855 / 36865\n",
      "training loss = 0.3810\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 728 ---------\n",
      "num_corrects / total_examples = 32806 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8899\n",
      "num_test_corrects / test_total_examples = 7952 / 9217\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 729 ---------\n",
      "num_corrects / total_examples = 32851 / 36865\n",
      "training loss = 0.3796\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 730 ---------\n",
      "num_corrects / total_examples = 32858 / 36865\n",
      "training loss = 0.3866\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 731 ---------\n",
      "num_corrects / total_examples = 32903 / 36865\n",
      "training loss = 0.3853\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 7837 / 9217\n",
      "testing accuracy = 0.8503\n",
      "--------- epoch: 732 ---------\n",
      "num_corrects / total_examples = 32831 / 36865\n",
      "training loss = 0.3889\n",
      "training accuracy = 0.8906\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 733 ---------\n",
      "num_corrects / total_examples = 32848 / 36865\n",
      "training loss = 0.3972\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 734 ---------\n",
      "num_corrects / total_examples = 32814 / 36865\n",
      "training loss = 0.3926\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 735 ---------\n",
      "num_corrects / total_examples = 32910 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7924 / 9217\n",
      "testing accuracy = 0.8597\n",
      "--------- epoch: 736 ---------\n",
      "num_corrects / total_examples = 32850 / 36865\n",
      "training loss = 0.3880\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7907 / 9217\n",
      "testing accuracy = 0.8579\n",
      "--------- epoch: 737 ---------\n",
      "num_corrects / total_examples = 32791 / 36865\n",
      "training loss = 0.4014\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7991 / 9217\n",
      "testing accuracy = 0.8670\n",
      "--------- epoch: 738 ---------\n",
      "num_corrects / total_examples = 32893 / 36865\n",
      "training loss = 0.3758\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 7951 / 9217\n",
      "testing accuracy = 0.8626\n",
      "--------- epoch: 739 ---------\n",
      "num_corrects / total_examples = 32842 / 36865\n",
      "training loss = 0.3979\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 740 ---------\n",
      "num_corrects / total_examples = 32770 / 36865\n",
      "training loss = 0.3902\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 741 ---------\n",
      "num_corrects / total_examples = 32790 / 36865\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 742 ---------\n",
      "num_corrects / total_examples = 32886 / 36865\n",
      "training loss = 0.3751\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7968 / 9217\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 743 ---------\n",
      "num_corrects / total_examples = 32905 / 36865\n",
      "training loss = 0.3817\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 7992 / 9217\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 744 ---------\n",
      "num_corrects / total_examples = 32852 / 36865\n",
      "training loss = 0.3885\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 745 ---------\n",
      "num_corrects / total_examples = 32856 / 36865\n",
      "training loss = 0.3851\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 746 ---------\n",
      "num_corrects / total_examples = 32874 / 36865\n",
      "training loss = 0.3932\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 747 ---------\n",
      "num_corrects / total_examples = 32903 / 36865\n",
      "training loss = 0.3803\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 7854 / 9217\n",
      "testing accuracy = 0.8521\n",
      "--------- epoch: 748 ---------\n",
      "num_corrects / total_examples = 32874 / 36865\n",
      "training loss = 0.3817\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7976 / 9217\n",
      "testing accuracy = 0.8654\n",
      "--------- epoch: 749 ---------\n",
      "num_corrects / total_examples = 32878 / 36865\n",
      "training loss = 0.3821\n",
      "training accuracy = 0.8918\n",
      "num_test_corrects / test_total_examples = 7914 / 9217\n",
      "testing accuracy = 0.8586\n",
      "--------- epoch: 750 ---------\n",
      "num_corrects / total_examples = 32850 / 36865\n",
      "training loss = 0.3919\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 751 ---------\n",
      "num_corrects / total_examples = 32859 / 36865\n",
      "training loss = 0.3882\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 752 ---------\n",
      "num_corrects / total_examples = 32848 / 36865\n",
      "training loss = 0.3828\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7837 / 9217\n",
      "testing accuracy = 0.8503\n",
      "--------- epoch: 753 ---------\n",
      "num_corrects / total_examples = 32887 / 36865\n",
      "training loss = 0.3901\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7917 / 9217\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 754 ---------\n",
      "num_corrects / total_examples = 32881 / 36865\n",
      "training loss = 0.3783\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 755 ---------\n",
      "num_corrects / total_examples = 32926 / 36865\n",
      "training loss = 0.3799\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 756 ---------\n",
      "num_corrects / total_examples = 32811 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 757 ---------\n",
      "num_corrects / total_examples = 32887 / 36865\n",
      "training loss = 0.3791\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7891 / 9217\n",
      "testing accuracy = 0.8561\n",
      "--------- epoch: 758 ---------\n",
      "num_corrects / total_examples = 32850 / 36865\n",
      "training loss = 0.3924\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 759 ---------\n",
      "num_corrects / total_examples = 32873 / 36865\n",
      "training loss = 0.3843\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 760 ---------\n",
      "num_corrects / total_examples = 32841 / 36865\n",
      "training loss = 0.3859\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 761 ---------\n",
      "num_corrects / total_examples = 32845 / 36865\n",
      "training loss = 0.3870\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 7992 / 9217\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 762 ---------\n",
      "num_corrects / total_examples = 32931 / 36865\n",
      "training loss = 0.3855\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7999 / 9217\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 763 ---------\n",
      "num_corrects / total_examples = 32912 / 36865\n",
      "training loss = 0.3931\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 7972 / 9217\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 764 ---------\n",
      "num_corrects / total_examples = 32810 / 36865\n",
      "training loss = 0.4104\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 765 ---------\n",
      "num_corrects / total_examples = 32885 / 36865\n",
      "training loss = 0.3828\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 766 ---------\n",
      "num_corrects / total_examples = 32900 / 36865\n",
      "training loss = 0.3755\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 767 ---------\n",
      "num_corrects / total_examples = 32888 / 36865\n",
      "training loss = 0.3854\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 768 ---------\n",
      "num_corrects / total_examples = 32869 / 36865\n",
      "training loss = 0.3813\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 8017 / 9217\n",
      "testing accuracy = 0.8698\n",
      "--------- epoch: 769 ---------\n",
      "num_corrects / total_examples = 32835 / 36865\n",
      "training loss = 0.3950\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 770 ---------\n",
      "num_corrects / total_examples = 32869 / 36865\n",
      "training loss = 0.3820\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 7992 / 9217\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 771 ---------\n",
      "num_corrects / total_examples = 32874 / 36865\n",
      "training loss = 0.3850\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 772 ---------\n",
      "num_corrects / total_examples = 32921 / 36865\n",
      "training loss = 0.3782\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 773 ---------\n",
      "num_corrects / total_examples = 32906 / 36865\n",
      "training loss = 0.3759\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 774 ---------\n",
      "num_corrects / total_examples = 32876 / 36865\n",
      "training loss = 0.3908\n",
      "training accuracy = 0.8918\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 775 ---------\n",
      "num_corrects / total_examples = 32909 / 36865\n",
      "training loss = 0.3928\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 776 ---------\n",
      "num_corrects / total_examples = 32945 / 36865\n",
      "training loss = 0.3953\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7875 / 9217\n",
      "testing accuracy = 0.8544\n",
      "--------- epoch: 777 ---------\n",
      "num_corrects / total_examples = 32791 / 36865\n",
      "training loss = 0.3903\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 8003 / 9217\n",
      "testing accuracy = 0.8683\n",
      "--------- epoch: 778 ---------\n",
      "num_corrects / total_examples = 32892 / 36865\n",
      "training loss = 0.3844\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 7879 / 9217\n",
      "testing accuracy = 0.8548\n",
      "--------- epoch: 779 ---------\n",
      "num_corrects / total_examples = 32935 / 36865\n",
      "training loss = 0.3840\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 780 ---------\n",
      "num_corrects / total_examples = 32881 / 36865\n",
      "training loss = 0.3792\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 781 ---------\n",
      "num_corrects / total_examples = 32916 / 36865\n",
      "training loss = 0.3804\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 8005 / 9217\n",
      "testing accuracy = 0.8685\n",
      "--------- epoch: 782 ---------\n",
      "num_corrects / total_examples = 32961 / 36865\n",
      "training loss = 0.3890\n",
      "training accuracy = 0.8941\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 783 ---------\n",
      "num_corrects / total_examples = 32911 / 36865\n",
      "training loss = 0.3832\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 784 ---------\n",
      "num_corrects / total_examples = 32829 / 36865\n",
      "training loss = 0.4013\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 8003 / 9217\n",
      "testing accuracy = 0.8683\n",
      "--------- epoch: 785 ---------\n",
      "num_corrects / total_examples = 32908 / 36865\n",
      "training loss = 0.3849\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 786 ---------\n",
      "num_corrects / total_examples = 32951 / 36865\n",
      "training loss = 0.3860\n",
      "training accuracy = 0.8938\n",
      "num_test_corrects / test_total_examples = 7971 / 9217\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 787 ---------\n",
      "num_corrects / total_examples = 32924 / 36865\n",
      "training loss = 0.3838\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 788 ---------\n",
      "num_corrects / total_examples = 32921 / 36865\n",
      "training loss = 0.3860\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7999 / 9217\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 789 ---------\n",
      "num_corrects / total_examples = 32931 / 36865\n",
      "training loss = 0.3862\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 790 ---------\n",
      "num_corrects / total_examples = 32873 / 36865\n",
      "training loss = 0.3933\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 7907 / 9217\n",
      "testing accuracy = 0.8579\n",
      "--------- epoch: 791 ---------\n",
      "num_corrects / total_examples = 32910 / 36865\n",
      "training loss = 0.3723\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7765 / 9217\n",
      "testing accuracy = 0.8425\n",
      "--------- epoch: 792 ---------\n",
      "num_corrects / total_examples = 32855 / 36865\n",
      "training loss = 0.3847\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 7926 / 9217\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 793 ---------\n",
      "num_corrects / total_examples = 32943 / 36865\n",
      "training loss = 0.3862\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 7987 / 9217\n",
      "testing accuracy = 0.8666\n",
      "--------- epoch: 794 ---------\n",
      "num_corrects / total_examples = 32925 / 36865\n",
      "training loss = 0.3895\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 795 ---------\n",
      "num_corrects / total_examples = 32879 / 36865\n",
      "training loss = 0.3910\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 7911 / 9217\n",
      "testing accuracy = 0.8583\n",
      "--------- epoch: 796 ---------\n",
      "num_corrects / total_examples = 32950 / 36865\n",
      "training loss = 0.3727\n",
      "training accuracy = 0.8938\n",
      "num_test_corrects / test_total_examples = 8010 / 9217\n",
      "testing accuracy = 0.8690\n",
      "--------- epoch: 797 ---------\n",
      "num_corrects / total_examples = 32861 / 36865\n",
      "training loss = 0.3858\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7830 / 9217\n",
      "testing accuracy = 0.8495\n",
      "--------- epoch: 798 ---------\n",
      "num_corrects / total_examples = 32929 / 36865\n",
      "training loss = 0.3745\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 799 ---------\n",
      "num_corrects / total_examples = 32855 / 36865\n",
      "training loss = 0.4035\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 7951 / 9217\n",
      "testing accuracy = 0.8626\n",
      "--------- epoch: 800 ---------\n",
      "num_corrects / total_examples = 32964 / 36865\n",
      "training loss = 0.3776\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7967 / 9217\n",
      "testing accuracy = 0.8644\n",
      "--------- epoch: 801 ---------\n",
      "num_corrects / total_examples = 32885 / 36865\n",
      "training loss = 0.3905\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 7850 / 9217\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 802 ---------\n",
      "num_corrects / total_examples = 32846 / 36865\n",
      "training loss = 0.3907\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 8023 / 9217\n",
      "testing accuracy = 0.8705\n",
      "found best test accuracy at epoch 802\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 803 ---------\n",
      "num_corrects / total_examples = 32929 / 36865\n",
      "training loss = 0.3758\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 804 ---------\n",
      "num_corrects / total_examples = 32965 / 36865\n",
      "training loss = 0.3840\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 805 ---------\n",
      "num_corrects / total_examples = 32943 / 36865\n",
      "training loss = 0.3867\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 8001 / 9217\n",
      "testing accuracy = 0.8681\n",
      "--------- epoch: 806 ---------\n",
      "num_corrects / total_examples = 32921 / 36865\n",
      "training loss = 0.3815\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7988 / 9217\n",
      "testing accuracy = 0.8667\n",
      "--------- epoch: 807 ---------\n",
      "num_corrects / total_examples = 32862 / 36865\n",
      "training loss = 0.3822\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 8012 / 9217\n",
      "testing accuracy = 0.8693\n",
      "--------- epoch: 808 ---------\n",
      "num_corrects / total_examples = 32981 / 36865\n",
      "training loss = 0.3823\n",
      "training accuracy = 0.8946\n",
      "num_test_corrects / test_total_examples = 7945 / 9217\n",
      "testing accuracy = 0.8620\n",
      "--------- epoch: 809 ---------\n",
      "num_corrects / total_examples = 32853 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 8018 / 9217\n",
      "testing accuracy = 0.8699\n",
      "--------- epoch: 810 ---------\n",
      "num_corrects / total_examples = 32922 / 36865\n",
      "training loss = 0.3833\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 8011 / 9217\n",
      "testing accuracy = 0.8692\n",
      "--------- epoch: 811 ---------\n",
      "num_corrects / total_examples = 32936 / 36865\n",
      "training loss = 0.3777\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 8031 / 9217\n",
      "testing accuracy = 0.8713\n",
      "found best test accuracy at epoch 811\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/square_model_scaled_hypnogram_1000epoch.pt\n",
      "--------- epoch: 812 ---------\n",
      "num_corrects / total_examples = 32938 / 36865\n",
      "training loss = 0.3832\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 813 ---------\n",
      "num_corrects / total_examples = 32836 / 36865\n",
      "training loss = 0.3896\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 814 ---------\n",
      "num_corrects / total_examples = 32890 / 36865\n",
      "training loss = 0.3801\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 815 ---------\n",
      "num_corrects / total_examples = 32926 / 36865\n",
      "training loss = 0.3976\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 8007 / 9217\n",
      "testing accuracy = 0.8687\n",
      "--------- epoch: 816 ---------\n",
      "num_corrects / total_examples = 32913 / 36865\n",
      "training loss = 0.3861\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 817 ---------\n",
      "num_corrects / total_examples = 32986 / 36865\n",
      "training loss = 0.3781\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7929 / 9217\n",
      "testing accuracy = 0.8603\n",
      "--------- epoch: 818 ---------\n",
      "num_corrects / total_examples = 32988 / 36865\n",
      "training loss = 0.3712\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7954 / 9217\n",
      "testing accuracy = 0.8630\n",
      "--------- epoch: 819 ---------\n",
      "num_corrects / total_examples = 32944 / 36865\n",
      "training loss = 0.3810\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 820 ---------\n",
      "num_corrects / total_examples = 32905 / 36865\n",
      "training loss = 0.3868\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 7866 / 9217\n",
      "testing accuracy = 0.8534\n",
      "--------- epoch: 821 ---------\n",
      "num_corrects / total_examples = 32968 / 36865\n",
      "training loss = 0.3840\n",
      "training accuracy = 0.8943\n",
      "num_test_corrects / test_total_examples = 7583 / 9217\n",
      "testing accuracy = 0.8227\n",
      "--------- epoch: 822 ---------\n",
      "num_corrects / total_examples = 32934 / 36865\n",
      "training loss = 0.3848\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 823 ---------\n",
      "num_corrects / total_examples = 32862 / 36865\n",
      "training loss = 0.3884\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 824 ---------\n",
      "num_corrects / total_examples = 32965 / 36865\n",
      "training loss = 0.3783\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 825 ---------\n",
      "num_corrects / total_examples = 32885 / 36865\n",
      "training loss = 0.3861\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 7895 / 9217\n",
      "testing accuracy = 0.8566\n",
      "--------- epoch: 826 ---------\n",
      "num_corrects / total_examples = 32947 / 36865\n",
      "training loss = 0.3791\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 827 ---------\n",
      "num_corrects / total_examples = 32917 / 36865\n",
      "training loss = 0.3824\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 7850 / 9217\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 828 ---------\n",
      "num_corrects / total_examples = 32967 / 36865\n",
      "training loss = 0.3767\n",
      "training accuracy = 0.8943\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 829 ---------\n",
      "num_corrects / total_examples = 32924 / 36865\n",
      "training loss = 0.3821\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 7861 / 9217\n",
      "testing accuracy = 0.8529\n",
      "--------- epoch: 830 ---------\n",
      "num_corrects / total_examples = 32939 / 36865\n",
      "training loss = 0.3879\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 831 ---------\n",
      "num_corrects / total_examples = 32900 / 36865\n",
      "training loss = 0.3843\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 7826 / 9217\n",
      "testing accuracy = 0.8491\n",
      "--------- epoch: 832 ---------\n",
      "num_corrects / total_examples = 32922 / 36865\n",
      "training loss = 0.3882\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 833 ---------\n",
      "num_corrects / total_examples = 32956 / 36865\n",
      "training loss = 0.3807\n",
      "training accuracy = 0.8940\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 834 ---------\n",
      "num_corrects / total_examples = 32934 / 36865\n",
      "training loss = 0.3860\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 8019 / 9217\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 835 ---------\n",
      "num_corrects / total_examples = 32955 / 36865\n",
      "training loss = 0.3837\n",
      "training accuracy = 0.8939\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 836 ---------\n",
      "num_corrects / total_examples = 32946 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 837 ---------\n",
      "num_corrects / total_examples = 32972 / 36865\n",
      "training loss = 0.3824\n",
      "training accuracy = 0.8944\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 838 ---------\n",
      "num_corrects / total_examples = 32972 / 36865\n",
      "training loss = 0.3750\n",
      "training accuracy = 0.8944\n",
      "num_test_corrects / test_total_examples = 7793 / 9217\n",
      "testing accuracy = 0.8455\n",
      "--------- epoch: 839 ---------\n",
      "num_corrects / total_examples = 32919 / 36865\n",
      "training loss = 0.3749\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7913 / 9217\n",
      "testing accuracy = 0.8585\n",
      "--------- epoch: 840 ---------\n",
      "num_corrects / total_examples = 32914 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 7912 / 9217\n",
      "testing accuracy = 0.8584\n",
      "--------- epoch: 841 ---------\n",
      "num_corrects / total_examples = 32911 / 36865\n",
      "training loss = 0.3846\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 842 ---------\n",
      "num_corrects / total_examples = 32886 / 36865\n",
      "training loss = 0.3834\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 843 ---------\n",
      "num_corrects / total_examples = 32936 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 844 ---------\n",
      "num_corrects / total_examples = 32974 / 36865\n",
      "training loss = 0.3807\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 845 ---------\n",
      "num_corrects / total_examples = 32981 / 36865\n",
      "training loss = 0.3802\n",
      "training accuracy = 0.8946\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 846 ---------\n",
      "num_corrects / total_examples = 32978 / 36865\n",
      "training loss = 0.3866\n",
      "training accuracy = 0.8946\n",
      "num_test_corrects / test_total_examples = 7989 / 9217\n",
      "testing accuracy = 0.8668\n",
      "--------- epoch: 847 ---------\n",
      "num_corrects / total_examples = 32927 / 36865\n",
      "training loss = 0.3755\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 848 ---------\n",
      "num_corrects / total_examples = 32923 / 36865\n",
      "training loss = 0.3761\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 7993 / 9217\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 849 ---------\n",
      "num_corrects / total_examples = 32938 / 36865\n",
      "training loss = 0.3785\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 7963 / 9217\n",
      "testing accuracy = 0.8639\n",
      "--------- epoch: 850 ---------\n",
      "num_corrects / total_examples = 32930 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7854 / 9217\n",
      "testing accuracy = 0.8521\n",
      "--------- epoch: 851 ---------\n",
      "num_corrects / total_examples = 32908 / 36865\n",
      "training loss = 0.3787\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 8005 / 9217\n",
      "testing accuracy = 0.8685\n",
      "--------- epoch: 852 ---------\n",
      "num_corrects / total_examples = 32954 / 36865\n",
      "training loss = 0.3815\n",
      "training accuracy = 0.8939\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 853 ---------\n",
      "num_corrects / total_examples = 32956 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8940\n",
      "num_test_corrects / test_total_examples = 8005 / 9217\n",
      "testing accuracy = 0.8685\n",
      "--------- epoch: 854 ---------\n",
      "num_corrects / total_examples = 32910 / 36865\n",
      "training loss = 0.3772\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 7906 / 9217\n",
      "testing accuracy = 0.8578\n",
      "--------- epoch: 855 ---------\n",
      "num_corrects / total_examples = 32948 / 36865\n",
      "training loss = 0.3853\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 856 ---------\n",
      "num_corrects / total_examples = 33000 / 36865\n",
      "training loss = 0.3784\n",
      "training accuracy = 0.8952\n",
      "num_test_corrects / test_total_examples = 7903 / 9217\n",
      "testing accuracy = 0.8574\n",
      "--------- epoch: 857 ---------\n",
      "num_corrects / total_examples = 32953 / 36865\n",
      "training loss = 0.3902\n",
      "training accuracy = 0.8939\n",
      "num_test_corrects / test_total_examples = 7903 / 9217\n",
      "testing accuracy = 0.8574\n",
      "--------- epoch: 858 ---------\n",
      "num_corrects / total_examples = 32985 / 36865\n",
      "training loss = 0.3806\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7990 / 9217\n",
      "testing accuracy = 0.8669\n",
      "--------- epoch: 859 ---------\n",
      "num_corrects / total_examples = 33006 / 36865\n",
      "training loss = 0.3752\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 7980 / 9217\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 860 ---------\n",
      "num_corrects / total_examples = 32912 / 36865\n",
      "training loss = 0.3850\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 7839 / 9217\n",
      "testing accuracy = 0.8505\n",
      "--------- epoch: 861 ---------\n",
      "num_corrects / total_examples = 32929 / 36865\n",
      "training loss = 0.3779\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 862 ---------\n",
      "num_corrects / total_examples = 32981 / 36865\n",
      "training loss = 0.3843\n",
      "training accuracy = 0.8946\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 863 ---------\n",
      "num_corrects / total_examples = 32966 / 36865\n",
      "training loss = 0.3768\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7888 / 9217\n",
      "testing accuracy = 0.8558\n",
      "--------- epoch: 864 ---------\n",
      "num_corrects / total_examples = 33001 / 36865\n",
      "training loss = 0.3743\n",
      "training accuracy = 0.8952\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 865 ---------\n",
      "num_corrects / total_examples = 32907 / 36865\n",
      "training loss = 0.3825\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 866 ---------\n",
      "num_corrects / total_examples = 33015 / 36865\n",
      "training loss = 0.3783\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7902 / 9217\n",
      "testing accuracy = 0.8573\n",
      "--------- epoch: 867 ---------\n",
      "num_corrects / total_examples = 32947 / 36865\n",
      "training loss = 0.3717\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 868 ---------\n",
      "num_corrects / total_examples = 32933 / 36865\n",
      "training loss = 0.3739\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7923 / 9217\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 869 ---------\n",
      "num_corrects / total_examples = 33011 / 36865\n",
      "training loss = 0.3753\n",
      "training accuracy = 0.8955\n",
      "num_test_corrects / test_total_examples = 7948 / 9217\n",
      "testing accuracy = 0.8623\n",
      "--------- epoch: 870 ---------\n",
      "num_corrects / total_examples = 32975 / 36865\n",
      "training loss = 0.3806\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 871 ---------\n",
      "num_corrects / total_examples = 32898 / 36865\n",
      "training loss = 0.3870\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 7922 / 9217\n",
      "testing accuracy = 0.8595\n",
      "--------- epoch: 872 ---------\n",
      "num_corrects / total_examples = 32945 / 36865\n",
      "training loss = 0.3961\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 873 ---------\n",
      "num_corrects / total_examples = 32921 / 36865\n",
      "training loss = 0.3861\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 7942 / 9217\n",
      "testing accuracy = 0.8617\n",
      "--------- epoch: 874 ---------\n",
      "num_corrects / total_examples = 32992 / 36865\n",
      "training loss = 0.3760\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 875 ---------\n",
      "num_corrects / total_examples = 32960 / 36865\n",
      "training loss = 0.3797\n",
      "training accuracy = 0.8941\n",
      "num_test_corrects / test_total_examples = 7973 / 9217\n",
      "testing accuracy = 0.8650\n",
      "--------- epoch: 876 ---------\n",
      "num_corrects / total_examples = 32931 / 36865\n",
      "training loss = 0.3932\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 877 ---------\n",
      "num_corrects / total_examples = 32997 / 36865\n",
      "training loss = 0.3692\n",
      "training accuracy = 0.8951\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 878 ---------\n",
      "num_corrects / total_examples = 32991 / 36865\n",
      "training loss = 0.3815\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 8013 / 9217\n",
      "testing accuracy = 0.8694\n",
      "--------- epoch: 879 ---------\n",
      "num_corrects / total_examples = 32974 / 36865\n",
      "training loss = 0.3813\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 8029 / 9217\n",
      "testing accuracy = 0.8711\n",
      "--------- epoch: 880 ---------\n",
      "num_corrects / total_examples = 32918 / 36865\n",
      "training loss = 0.3774\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 881 ---------\n",
      "num_corrects / total_examples = 32970 / 36865\n",
      "training loss = 0.3838\n",
      "training accuracy = 0.8943\n",
      "num_test_corrects / test_total_examples = 7930 / 9217\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 882 ---------\n",
      "num_corrects / total_examples = 32952 / 36865\n",
      "training loss = 0.3834\n",
      "training accuracy = 0.8939\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 883 ---------\n",
      "num_corrects / total_examples = 33024 / 36865\n",
      "training loss = 0.3685\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 884 ---------\n",
      "num_corrects / total_examples = 32951 / 36865\n",
      "training loss = 0.3889\n",
      "training accuracy = 0.8938\n",
      "num_test_corrects / test_total_examples = 7612 / 9217\n",
      "testing accuracy = 0.8259\n",
      "--------- epoch: 885 ---------\n",
      "num_corrects / total_examples = 32989 / 36865\n",
      "training loss = 0.3741\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 7763 / 9217\n",
      "testing accuracy = 0.8422\n",
      "--------- epoch: 886 ---------\n",
      "num_corrects / total_examples = 32965 / 36865\n",
      "training loss = 0.3790\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7890 / 9217\n",
      "testing accuracy = 0.8560\n",
      "--------- epoch: 887 ---------\n",
      "num_corrects / total_examples = 33002 / 36865\n",
      "training loss = 0.3769\n",
      "training accuracy = 0.8952\n",
      "num_test_corrects / test_total_examples = 7999 / 9217\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 888 ---------\n",
      "num_corrects / total_examples = 33020 / 36865\n",
      "training loss = 0.3716\n",
      "training accuracy = 0.8957\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 889 ---------\n",
      "num_corrects / total_examples = 32980 / 36865\n",
      "training loss = 0.3897\n",
      "training accuracy = 0.8946\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 890 ---------\n",
      "num_corrects / total_examples = 33037 / 36865\n",
      "training loss = 0.3730\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 891 ---------\n",
      "num_corrects / total_examples = 33075 / 36865\n",
      "training loss = 0.3738\n",
      "training accuracy = 0.8972\n",
      "num_test_corrects / test_total_examples = 8000 / 9217\n",
      "testing accuracy = 0.8680\n",
      "--------- epoch: 892 ---------\n",
      "num_corrects / total_examples = 32991 / 36865\n",
      "training loss = 0.3860\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 8009 / 9217\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 893 ---------\n",
      "num_corrects / total_examples = 33068 / 36865\n",
      "training loss = 0.3705\n",
      "training accuracy = 0.8970\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 894 ---------\n",
      "num_corrects / total_examples = 33046 / 36865\n",
      "training loss = 0.3798\n",
      "training accuracy = 0.8964\n",
      "num_test_corrects / test_total_examples = 7964 / 9217\n",
      "testing accuracy = 0.8641\n",
      "--------- epoch: 895 ---------\n",
      "num_corrects / total_examples = 32967 / 36865\n",
      "training loss = 0.3810\n",
      "training accuracy = 0.8943\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 896 ---------\n",
      "num_corrects / total_examples = 32964 / 36865\n",
      "training loss = 0.3827\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7994 / 9217\n",
      "testing accuracy = 0.8673\n",
      "--------- epoch: 897 ---------\n",
      "num_corrects / total_examples = 32922 / 36865\n",
      "training loss = 0.3910\n",
      "training accuracy = 0.8930\n",
      "num_test_corrects / test_total_examples = 8001 / 9217\n",
      "testing accuracy = 0.8681\n",
      "--------- epoch: 898 ---------\n",
      "num_corrects / total_examples = 32982 / 36865\n",
      "training loss = 0.3726\n",
      "training accuracy = 0.8947\n",
      "num_test_corrects / test_total_examples = 7830 / 9217\n",
      "testing accuracy = 0.8495\n",
      "--------- epoch: 899 ---------\n",
      "num_corrects / total_examples = 32977 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 900 ---------\n",
      "num_corrects / total_examples = 32962 / 36865\n",
      "training loss = 0.3737\n",
      "training accuracy = 0.8941\n",
      "num_test_corrects / test_total_examples = 8024 / 9217\n",
      "testing accuracy = 0.8706\n",
      "--------- epoch: 901 ---------\n",
      "num_corrects / total_examples = 33028 / 36865\n",
      "training loss = 0.3804\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 902 ---------\n",
      "num_corrects / total_examples = 33027 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 903 ---------\n",
      "num_corrects / total_examples = 33016 / 36865\n",
      "training loss = 0.3736\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7937 / 9217\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 904 ---------\n",
      "num_corrects / total_examples = 32947 / 36865\n",
      "training loss = 0.3753\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 905 ---------\n",
      "num_corrects / total_examples = 32971 / 36865\n",
      "training loss = 0.3914\n",
      "training accuracy = 0.8944\n",
      "num_test_corrects / test_total_examples = 7978 / 9217\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 906 ---------\n",
      "num_corrects / total_examples = 32958 / 36865\n",
      "training loss = 0.3721\n",
      "training accuracy = 0.8940\n",
      "num_test_corrects / test_total_examples = 7998 / 9217\n",
      "testing accuracy = 0.8677\n",
      "--------- epoch: 907 ---------\n",
      "num_corrects / total_examples = 33061 / 36865\n",
      "training loss = 0.3732\n",
      "training accuracy = 0.8968\n",
      "num_test_corrects / test_total_examples = 7977 / 9217\n",
      "testing accuracy = 0.8655\n",
      "--------- epoch: 908 ---------\n",
      "num_corrects / total_examples = 32987 / 36865\n",
      "training loss = 0.3881\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 909 ---------\n",
      "num_corrects / total_examples = 33058 / 36865\n",
      "training loss = 0.3809\n",
      "training accuracy = 0.8967\n",
      "num_test_corrects / test_total_examples = 7916 / 9217\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 910 ---------\n",
      "num_corrects / total_examples = 32987 / 36865\n",
      "training loss = 0.3763\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7932 / 9217\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 911 ---------\n",
      "num_corrects / total_examples = 33006 / 36865\n",
      "training loss = 0.3884\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 7873 / 9217\n",
      "testing accuracy = 0.8542\n",
      "--------- epoch: 912 ---------\n",
      "num_corrects / total_examples = 33021 / 36865\n",
      "training loss = 0.3728\n",
      "training accuracy = 0.8957\n",
      "num_test_corrects / test_total_examples = 8017 / 9217\n",
      "testing accuracy = 0.8698\n",
      "--------- epoch: 913 ---------\n",
      "num_corrects / total_examples = 32963 / 36865\n",
      "training loss = 0.3866\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7819 / 9217\n",
      "testing accuracy = 0.8483\n",
      "--------- epoch: 914 ---------\n",
      "num_corrects / total_examples = 33038 / 36865\n",
      "training loss = 0.3703\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7909 / 9217\n",
      "testing accuracy = 0.8581\n",
      "--------- epoch: 915 ---------\n",
      "num_corrects / total_examples = 32989 / 36865\n",
      "training loss = 0.3853\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 916 ---------\n",
      "num_corrects / total_examples = 33004 / 36865\n",
      "training loss = 0.3842\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 7882 / 9217\n",
      "testing accuracy = 0.8552\n",
      "--------- epoch: 917 ---------\n",
      "num_corrects / total_examples = 33056 / 36865\n",
      "training loss = 0.3799\n",
      "training accuracy = 0.8967\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 918 ---------\n",
      "num_corrects / total_examples = 32975 / 36865\n",
      "training loss = 0.3885\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 7984 / 9217\n",
      "testing accuracy = 0.8662\n",
      "--------- epoch: 919 ---------\n",
      "num_corrects / total_examples = 32970 / 36865\n",
      "training loss = 0.3729\n",
      "training accuracy = 0.8943\n",
      "num_test_corrects / test_total_examples = 7957 / 9217\n",
      "testing accuracy = 0.8633\n",
      "--------- epoch: 920 ---------\n",
      "num_corrects / total_examples = 32997 / 36865\n",
      "training loss = 0.3737\n",
      "training accuracy = 0.8951\n",
      "num_test_corrects / test_total_examples = 7946 / 9217\n",
      "testing accuracy = 0.8621\n",
      "--------- epoch: 921 ---------\n",
      "num_corrects / total_examples = 32989 / 36865\n",
      "training loss = 0.3814\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 8001 / 9217\n",
      "testing accuracy = 0.8681\n",
      "--------- epoch: 922 ---------\n",
      "num_corrects / total_examples = 33018 / 36865\n",
      "training loss = 0.3716\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7961 / 9217\n",
      "testing accuracy = 0.8637\n",
      "--------- epoch: 923 ---------\n",
      "num_corrects / total_examples = 33022 / 36865\n",
      "training loss = 0.3786\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 924 ---------\n",
      "num_corrects / total_examples = 33016 / 36865\n",
      "training loss = 0.3796\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7982 / 9217\n",
      "testing accuracy = 0.8660\n",
      "--------- epoch: 925 ---------\n",
      "num_corrects / total_examples = 32986 / 36865\n",
      "training loss = 0.3735\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 926 ---------\n",
      "num_corrects / total_examples = 33068 / 36865\n",
      "training loss = 0.3810\n",
      "training accuracy = 0.8970\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 927 ---------\n",
      "num_corrects / total_examples = 32929 / 36865\n",
      "training loss = 0.3878\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 7853 / 9217\n",
      "testing accuracy = 0.8520\n",
      "--------- epoch: 928 ---------\n",
      "num_corrects / total_examples = 32982 / 36865\n",
      "training loss = 0.4013\n",
      "training accuracy = 0.8947\n",
      "num_test_corrects / test_total_examples = 7955 / 9217\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 929 ---------\n",
      "num_corrects / total_examples = 32963 / 36865\n",
      "training loss = 0.3876\n",
      "training accuracy = 0.8942\n",
      "num_test_corrects / test_total_examples = 7900 / 9217\n",
      "testing accuracy = 0.8571\n",
      "--------- epoch: 930 ---------\n",
      "num_corrects / total_examples = 33038 / 36865\n",
      "training loss = 0.3751\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7934 / 9217\n",
      "testing accuracy = 0.8608\n",
      "--------- epoch: 931 ---------\n",
      "num_corrects / total_examples = 33024 / 36865\n",
      "training loss = 0.3744\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 8023 / 9217\n",
      "testing accuracy = 0.8705\n",
      "--------- epoch: 932 ---------\n",
      "num_corrects / total_examples = 33046 / 36865\n",
      "training loss = 0.3786\n",
      "training accuracy = 0.8964\n",
      "num_test_corrects / test_total_examples = 7992 / 9217\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 933 ---------\n",
      "num_corrects / total_examples = 33013 / 36865\n",
      "training loss = 0.3791\n",
      "training accuracy = 0.8955\n",
      "num_test_corrects / test_total_examples = 7927 / 9217\n",
      "testing accuracy = 0.8600\n",
      "--------- epoch: 934 ---------\n",
      "num_corrects / total_examples = 33056 / 36865\n",
      "training loss = 0.3798\n",
      "training accuracy = 0.8967\n",
      "num_test_corrects / test_total_examples = 7985 / 9217\n",
      "testing accuracy = 0.8663\n",
      "--------- epoch: 935 ---------\n",
      "num_corrects / total_examples = 33025 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 936 ---------\n",
      "num_corrects / total_examples = 32986 / 36865\n",
      "training loss = 0.3804\n",
      "training accuracy = 0.8948\n",
      "num_test_corrects / test_total_examples = 7841 / 9217\n",
      "testing accuracy = 0.8507\n",
      "--------- epoch: 937 ---------\n",
      "num_corrects / total_examples = 33054 / 36865\n",
      "training loss = 0.3904\n",
      "training accuracy = 0.8966\n",
      "num_test_corrects / test_total_examples = 7871 / 9217\n",
      "testing accuracy = 0.8540\n",
      "--------- epoch: 938 ---------\n",
      "num_corrects / total_examples = 33095 / 36865\n",
      "training loss = 0.3717\n",
      "training accuracy = 0.8977\n",
      "num_test_corrects / test_total_examples = 7896 / 9217\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 939 ---------\n",
      "num_corrects / total_examples = 33043 / 36865\n",
      "training loss = 0.3684\n",
      "training accuracy = 0.8963\n",
      "num_test_corrects / test_total_examples = 7724 / 9217\n",
      "testing accuracy = 0.8380\n",
      "--------- epoch: 940 ---------\n",
      "num_corrects / total_examples = 33004 / 36865\n",
      "training loss = 0.3793\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 7925 / 9217\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 941 ---------\n",
      "num_corrects / total_examples = 33000 / 36865\n",
      "training loss = 0.3760\n",
      "training accuracy = 0.8952\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 942 ---------\n",
      "num_corrects / total_examples = 32973 / 36865\n",
      "training loss = 0.3803\n",
      "training accuracy = 0.8944\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 943 ---------\n",
      "num_corrects / total_examples = 33022 / 36865\n",
      "training loss = 0.3784\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 944 ---------\n",
      "num_corrects / total_examples = 32951 / 36865\n",
      "training loss = 0.3918\n",
      "training accuracy = 0.8938\n",
      "num_test_corrects / test_total_examples = 7887 / 9217\n",
      "testing accuracy = 0.8557\n",
      "--------- epoch: 945 ---------\n",
      "num_corrects / total_examples = 33104 / 36865\n",
      "training loss = 0.3740\n",
      "training accuracy = 0.8980\n",
      "num_test_corrects / test_total_examples = 7966 / 9217\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 946 ---------\n",
      "num_corrects / total_examples = 33006 / 36865\n",
      "training loss = 0.3825\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 8005 / 9217\n",
      "testing accuracy = 0.8685\n",
      "--------- epoch: 947 ---------\n",
      "num_corrects / total_examples = 32984 / 36865\n",
      "training loss = 0.3814\n",
      "training accuracy = 0.8947\n",
      "num_test_corrects / test_total_examples = 7960 / 9217\n",
      "testing accuracy = 0.8636\n",
      "--------- epoch: 948 ---------\n",
      "num_corrects / total_examples = 33059 / 36865\n",
      "training loss = 0.3812\n",
      "training accuracy = 0.8968\n",
      "num_test_corrects / test_total_examples = 8020 / 9217\n",
      "testing accuracy = 0.8701\n",
      "--------- epoch: 949 ---------\n",
      "num_corrects / total_examples = 33065 / 36865\n",
      "training loss = 0.3682\n",
      "training accuracy = 0.8969\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 950 ---------\n",
      "num_corrects / total_examples = 33024 / 36865\n",
      "training loss = 0.3794\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 8019 / 9217\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 951 ---------\n",
      "num_corrects / total_examples = 32997 / 36865\n",
      "training loss = 0.3852\n",
      "training accuracy = 0.8951\n",
      "num_test_corrects / test_total_examples = 7917 / 9217\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 952 ---------\n",
      "num_corrects / total_examples = 33064 / 36865\n",
      "training loss = 0.3858\n",
      "training accuracy = 0.8969\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 953 ---------\n",
      "num_corrects / total_examples = 33071 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8971\n",
      "num_test_corrects / test_total_examples = 7951 / 9217\n",
      "testing accuracy = 0.8626\n",
      "--------- epoch: 954 ---------\n",
      "num_corrects / total_examples = 33015 / 36865\n",
      "training loss = 0.3827\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7478 / 9217\n",
      "testing accuracy = 0.8113\n",
      "--------- epoch: 955 ---------\n",
      "num_corrects / total_examples = 33060 / 36865\n",
      "training loss = 0.3807\n",
      "training accuracy = 0.8968\n",
      "num_test_corrects / test_total_examples = 7931 / 9217\n",
      "testing accuracy = 0.8605\n",
      "--------- epoch: 956 ---------\n",
      "num_corrects / total_examples = 33042 / 36865\n",
      "training loss = 0.3760\n",
      "training accuracy = 0.8963\n",
      "num_test_corrects / test_total_examples = 7886 / 9217\n",
      "testing accuracy = 0.8556\n",
      "--------- epoch: 957 ---------\n",
      "num_corrects / total_examples = 33040 / 36865\n",
      "training loss = 0.3816\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7953 / 9217\n",
      "testing accuracy = 0.8629\n",
      "--------- epoch: 958 ---------\n",
      "num_corrects / total_examples = 32951 / 36865\n",
      "training loss = 0.4018\n",
      "training accuracy = 0.8938\n",
      "num_test_corrects / test_total_examples = 7949 / 9217\n",
      "testing accuracy = 0.8624\n",
      "--------- epoch: 959 ---------\n",
      "num_corrects / total_examples = 33026 / 36865\n",
      "training loss = 0.3770\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7777 / 9217\n",
      "testing accuracy = 0.8438\n",
      "--------- epoch: 960 ---------\n",
      "num_corrects / total_examples = 33026 / 36865\n",
      "training loss = 0.3760\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7983 / 9217\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 961 ---------\n",
      "num_corrects / total_examples = 33034 / 36865\n",
      "training loss = 0.3851\n",
      "training accuracy = 0.8961\n",
      "num_test_corrects / test_total_examples = 7935 / 9217\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 962 ---------\n",
      "num_corrects / total_examples = 32992 / 36865\n",
      "training loss = 0.3750\n",
      "training accuracy = 0.8949\n",
      "num_test_corrects / test_total_examples = 7865 / 9217\n",
      "testing accuracy = 0.8533\n",
      "--------- epoch: 963 ---------\n",
      "num_corrects / total_examples = 33025 / 36865\n",
      "training loss = 0.3914\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7789 / 9217\n",
      "testing accuracy = 0.8451\n",
      "--------- epoch: 964 ---------\n",
      "num_corrects / total_examples = 33072 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8971\n",
      "num_test_corrects / test_total_examples = 7928 / 9217\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 965 ---------\n",
      "num_corrects / total_examples = 33027 / 36865\n",
      "training loss = 0.3845\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7965 / 9217\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 966 ---------\n",
      "num_corrects / total_examples = 33069 / 36865\n",
      "training loss = 0.3708\n",
      "training accuracy = 0.8970\n",
      "num_test_corrects / test_total_examples = 7943 / 9217\n",
      "testing accuracy = 0.8618\n",
      "--------- epoch: 967 ---------\n",
      "num_corrects / total_examples = 33045 / 36865\n",
      "training loss = 0.3785\n",
      "training accuracy = 0.8964\n",
      "num_test_corrects / test_total_examples = 7947 / 9217\n",
      "testing accuracy = 0.8622\n",
      "--------- epoch: 968 ---------\n",
      "num_corrects / total_examples = 33033 / 36865\n",
      "training loss = 0.3752\n",
      "training accuracy = 0.8961\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 969 ---------\n",
      "num_corrects / total_examples = 33042 / 36865\n",
      "training loss = 0.3714\n",
      "training accuracy = 0.8963\n",
      "num_test_corrects / test_total_examples = 7885 / 9217\n",
      "testing accuracy = 0.8555\n",
      "--------- epoch: 970 ---------\n",
      "num_corrects / total_examples = 33107 / 36865\n",
      "training loss = 0.3788\n",
      "training accuracy = 0.8981\n",
      "num_test_corrects / test_total_examples = 7962 / 9217\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 971 ---------\n",
      "num_corrects / total_examples = 33025 / 36865\n",
      "training loss = 0.3842\n",
      "training accuracy = 0.8958\n",
      "num_test_corrects / test_total_examples = 7898 / 9217\n",
      "testing accuracy = 0.8569\n",
      "--------- epoch: 972 ---------\n",
      "num_corrects / total_examples = 33045 / 36865\n",
      "training loss = 0.3778\n",
      "training accuracy = 0.8964\n",
      "num_test_corrects / test_total_examples = 7969 / 9217\n",
      "testing accuracy = 0.8646\n",
      "--------- epoch: 973 ---------\n",
      "num_corrects / total_examples = 33041 / 36865\n",
      "training loss = 0.3744\n",
      "training accuracy = 0.8963\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 974 ---------\n",
      "num_corrects / total_examples = 33028 / 36865\n",
      "training loss = 0.3848\n",
      "training accuracy = 0.8959\n",
      "num_test_corrects / test_total_examples = 7933 / 9217\n",
      "testing accuracy = 0.8607\n",
      "--------- epoch: 975 ---------\n",
      "num_corrects / total_examples = 33042 / 36865\n",
      "training loss = 0.3792\n",
      "training accuracy = 0.8963\n",
      "num_test_corrects / test_total_examples = 7970 / 9217\n",
      "testing accuracy = 0.8647\n",
      "--------- epoch: 976 ---------\n",
      "num_corrects / total_examples = 32947 / 36865\n",
      "training loss = 0.3882\n",
      "training accuracy = 0.8937\n",
      "num_test_corrects / test_total_examples = 7993 / 9217\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 977 ---------\n",
      "num_corrects / total_examples = 33035 / 36865\n",
      "training loss = 0.3821\n",
      "training accuracy = 0.8961\n",
      "num_test_corrects / test_total_examples = 7893 / 9217\n",
      "testing accuracy = 0.8564\n",
      "--------- epoch: 978 ---------\n",
      "num_corrects / total_examples = 33039 / 36865\n",
      "training loss = 0.3795\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7930 / 9217\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 979 ---------\n",
      "num_corrects / total_examples = 33013 / 36865\n",
      "training loss = 0.3776\n",
      "training accuracy = 0.8955\n",
      "num_test_corrects / test_total_examples = 7941 / 9217\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 980 ---------\n",
      "num_corrects / total_examples = 33037 / 36865\n",
      "training loss = 0.3782\n",
      "training accuracy = 0.8962\n",
      "num_test_corrects / test_total_examples = 7999 / 9217\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 981 ---------\n",
      "num_corrects / total_examples = 33001 / 36865\n",
      "training loss = 0.3841\n",
      "training accuracy = 0.8952\n",
      "num_test_corrects / test_total_examples = 7956 / 9217\n",
      "testing accuracy = 0.8632\n",
      "--------- epoch: 982 ---------\n",
      "num_corrects / total_examples = 33062 / 36865\n",
      "training loss = 0.3699\n",
      "training accuracy = 0.8968\n",
      "num_test_corrects / test_total_examples = 7815 / 9217\n",
      "testing accuracy = 0.8479\n",
      "--------- epoch: 983 ---------\n",
      "num_corrects / total_examples = 32999 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8951\n",
      "num_test_corrects / test_total_examples = 7779 / 9217\n",
      "testing accuracy = 0.8440\n",
      "--------- epoch: 984 ---------\n",
      "num_corrects / total_examples = 33019 / 36865\n",
      "training loss = 0.3874\n",
      "training accuracy = 0.8957\n",
      "num_test_corrects / test_total_examples = 7950 / 9217\n",
      "testing accuracy = 0.8625\n",
      "--------- epoch: 985 ---------\n",
      "num_corrects / total_examples = 33065 / 36865\n",
      "training loss = 0.3801\n",
      "training accuracy = 0.8969\n",
      "num_test_corrects / test_total_examples = 7940 / 9217\n",
      "testing accuracy = 0.8615\n",
      "--------- epoch: 986 ---------\n",
      "num_corrects / total_examples = 33066 / 36865\n",
      "training loss = 0.3714\n",
      "training accuracy = 0.8969\n",
      "num_test_corrects / test_total_examples = 7979 / 9217\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 987 ---------\n",
      "num_corrects / total_examples = 33018 / 36865\n",
      "training loss = 0.3736\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 7997 / 9217\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 988 ---------\n",
      "num_corrects / total_examples = 33061 / 36865\n",
      "training loss = 0.3663\n",
      "training accuracy = 0.8968\n",
      "num_test_corrects / test_total_examples = 7914 / 9217\n",
      "testing accuracy = 0.8586\n",
      "--------- epoch: 989 ---------\n",
      "num_corrects / total_examples = 33047 / 36865\n",
      "training loss = 0.3777\n",
      "training accuracy = 0.8964\n",
      "num_test_corrects / test_total_examples = 7986 / 9217\n",
      "testing accuracy = 0.8664\n",
      "--------- epoch: 990 ---------\n",
      "num_corrects / total_examples = 33063 / 36865\n",
      "training loss = 0.3784\n",
      "training accuracy = 0.8969\n",
      "num_test_corrects / test_total_examples = 7854 / 9217\n",
      "testing accuracy = 0.8521\n",
      "--------- epoch: 991 ---------\n",
      "num_corrects / total_examples = 33018 / 36865\n",
      "training loss = 0.3842\n",
      "training accuracy = 0.8956\n",
      "num_test_corrects / test_total_examples = 8008 / 9217\n",
      "testing accuracy = 0.8688\n",
      "--------- epoch: 992 ---------\n",
      "num_corrects / total_examples = 33057 / 36865\n",
      "training loss = 0.3692\n",
      "training accuracy = 0.8967\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 993 ---------\n",
      "num_corrects / total_examples = 33081 / 36865\n",
      "training loss = 0.3674\n",
      "training accuracy = 0.8974\n",
      "num_test_corrects / test_total_examples = 7892 / 9217\n",
      "testing accuracy = 0.8562\n",
      "--------- epoch: 994 ---------\n",
      "num_corrects / total_examples = 32977 / 36865\n",
      "training loss = 0.3837\n",
      "training accuracy = 0.8945\n",
      "num_test_corrects / test_total_examples = 7975 / 9217\n",
      "testing accuracy = 0.8652\n",
      "--------- epoch: 995 ---------\n",
      "num_corrects / total_examples = 33007 / 36865\n",
      "training loss = 0.3800\n",
      "training accuracy = 0.8953\n",
      "num_test_corrects / test_total_examples = 7811 / 9217\n",
      "testing accuracy = 0.8475\n",
      "--------- epoch: 996 ---------\n",
      "num_corrects / total_examples = 33030 / 36865\n",
      "training loss = 0.3706\n",
      "training accuracy = 0.8960\n",
      "num_test_corrects / test_total_examples = 7958 / 9217\n",
      "testing accuracy = 0.8634\n",
      "--------- epoch: 997 ---------\n",
      "num_corrects / total_examples = 33124 / 36865\n",
      "training loss = 0.3690\n",
      "training accuracy = 0.8985\n",
      "num_test_corrects / test_total_examples = 7995 / 9217\n",
      "testing accuracy = 0.8674\n",
      "--------- epoch: 998 ---------\n",
      "num_corrects / total_examples = 33101 / 36865\n",
      "training loss = 0.3687\n",
      "training accuracy = 0.8979\n",
      "num_test_corrects / test_total_examples = 7885 / 9217\n",
      "testing accuracy = 0.8555\n",
      "--------- epoch: 999 ---------\n",
      "num_corrects / total_examples = 33093 / 36865\n",
      "training loss = 0.3675\n",
      "training accuracy = 0.8977\n",
      "num_test_corrects / test_total_examples = 7959 / 9217\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 1000 ---------\n",
      "num_corrects / total_examples = 33077 / 36865\n",
      "training loss = 0.3690\n",
      "training accuracy = 0.8972\n",
      "num_test_corrects / test_total_examples = 7929 / 9217\n",
      "testing accuracy = 0.8603\n"
     ]
    }
   ],
   "source": [
    "save_weight_path = project_path / 'weights' / 'hypnogram' / 'square_model_scaled_hypnogram_1000epoch.pt'\n",
    "train_losses_2, train_accuracies_2, test_accuracies_2 = train(lc_model, \n",
    "                                                              train_loader_scaled, \n",
    "                                                              test_loader_scaled,\n",
    "                                                              save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ecbed8730>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.8985216617584229\n",
      "max test accuracy = 0.8713247260496908\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies_2)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch Lightning - Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(300, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        self.training_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = train_batch\n",
    "        y_hat = self.sigmoid(self.linear(x))\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.training_step_outputs.append(y_hat.round() == y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_preds = torch.cat(self.training_step_outputs, dim=0)\n",
    "        train_acc = torch.sum(train_preds) / train_preds.shape[0]\n",
    "        self.log(\"train_acc\", train_acc)\n",
    "        print(f\"train acc = {train_acc}\")\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.sigmoid(self.linear(x))\n",
    "        self.val_step_outputs.append(y_hat.round() == y)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        val_preds = torch.cat(self.val_step_outputs, dim=0)\n",
    "        val_acc = torch.sum(val_preds) / val_preds.shape[0]\n",
    "        self.log(\"val_acc\", val_acc)\n",
    "        print(f\"val acc = {val_acc}\")\n",
    "        self.val_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "lr_model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | linear    | Linear  | 301   \n",
      "1 | sigmoid   | Sigmoid | 0     \n",
      "2 | criterion | BCELoss | 0     \n",
      "--------------------------------------\n",
      "301       Trainable params\n",
      "0         Non-trainable params\n",
      "301       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 109.46it/s]val acc = 0.75\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/pockethhe/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:00<00:00, 282.40it/s, v_num=2]val acc = 0.7208419442176819\n",
      "Epoch 0: 100%|██████████| 100/100 [00:06<00:00, 14.66it/s, v_num=2] train acc = 0.6074999570846558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:06<00:00, 14.64it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "# test run on 100 data batches and 1 epoch\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=lr_model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | linear    | Linear  | 301   \n",
      "1 | sigmoid   | Sigmoid | 0     \n",
      "2 | criterion | BCELoss | 0     \n",
      "--------------------------------------\n",
      "301       Trainable params\n",
      "0         Non-trainable params\n",
      "301       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 449.29it/s]val acc = 0.625\n",
      "Epoch 0: 100%|██████████| 9217/9217 [00:31<00:00, 292.78it/s, v_num=4]      val acc = 0.6982749104499817\n",
      "Epoch 0: 100%|██████████| 9217/9217 [00:37<00:00, 244.25it/s, v_num=4]train acc = 0.6211311221122742\n",
      "Epoch 1: 100%|██████████| 9217/9217 [00:30<00:00, 298.31it/s, v_num=4]val acc = 0.6988174319267273\n",
      "Epoch 1: 100%|██████████| 9217/9217 [00:37<00:00, 248.06it/s, v_num=4]train acc = 0.6261494755744934\n",
      "Epoch 2: 100%|██████████| 9217/9217 [00:34<00:00, 263.58it/s, v_num=4]val acc = 0.6992514133453369\n",
      "Epoch 2: 100%|██████████| 9217/9217 [00:41<00:00, 221.85it/s, v_num=4]train acc = 0.6296486854553223\n",
      "Epoch 3: 100%|██████████| 9217/9217 [00:36<00:00, 251.53it/s, v_num=4]val acc = 0.7000108361244202\n",
      "Epoch 3: 100%|██████████| 9217/9217 [00:43<00:00, 212.66it/s, v_num=4]train acc = 0.6354536414146423\n",
      "Epoch 4: 100%|██████████| 9217/9217 [00:36<00:00, 255.47it/s, v_num=4]val acc = 0.7004448175430298\n",
      "Epoch 4: 100%|██████████| 9217/9217 [00:41<00:00, 220.83it/s, v_num=4]train acc = 0.6374338865280151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9217/9217 [00:41<00:00, 220.68it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "trainer.fit(model=lr_model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hesplitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
